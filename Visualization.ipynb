{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Visualization",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khare19yash/Automated-Detection-of-Neuropsychiatric-Disorders/blob/master/Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbcNNOwas0t2",
        "outputId": "587701ee-bd33-4a8a-8618-da95e15ccfc1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZFE-Yyss5RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b5b829-b808-4c5c-9d6f-fa77e13c6c9b"
      },
      "source": [
        "cd gdrive/My Drive/IIITH_Internship/Neuro/Final"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/IIITH_Internship/Neuro/Final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ObQ2ACv8bF"
      },
      "source": [
        "cd gdrive/My Drive/Final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:35.313833Z",
          "iopub.execute_input": "2021-06-18T20:06:35.314183Z",
          "iopub.status.idle": "2021-06-18T20:06:35.320917Z",
          "shell.execute_reply.started": "2021-06-18T20:06:35.31415Z",
          "shell.execute_reply": "2021-06-18T20:06:35.320107Z"
        },
        "trusted": true,
        "id": "Y0UrH07ohkn1"
      },
      "source": [
        "#options: cc200, dosenbach160, aal\n",
        "p_ROI = \"cc200\"\n",
        "p_fold = 10\n",
        "p_center = \"Stanford\"\n",
        "p_mode = \"whole\"\n",
        "p_augmentation = False\n",
        "p_Method = \"ASD-DiagNet\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:35.6938Z",
          "iopub.execute_input": "2021-06-18T20:06:35.694114Z",
          "iopub.status.idle": "2021-06-18T20:06:35.705229Z",
          "shell.execute_reply.started": "2021-06-18T20:06:35.694082Z",
          "shell.execute_reply": "2021-06-18T20:06:35.703223Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XvyvjLghkn2",
        "outputId": "ec862ace-af24-46c9-c6be-06ce093be374"
      },
      "source": [
        "parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\n",
        "print(\"*****List of patameters****\")\n",
        "print(\"ROI atlas: \",p_ROI)\n",
        "print(\"per Center or whole: \",p_mode)\n",
        "if p_mode == 'percenter':\n",
        "    print(\"Center's name: \",p_center)\n",
        "print(\"Method's name: \",p_Method)\n",
        "if p_Method == \"ASD-DiagNet\":\n",
        "    print(\"Augmentation: \",p_augmentation)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****List of patameters****\n",
            "ROI atlas:  cc200\n",
            "per Center or whole:  whole\n",
            "Method's name:  ASD-DiagNet\n",
            "Augmentation:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSeVPBEwtX_C",
        "outputId": "35da698b-11b1-4359-dccf-72a8112a83fb"
      },
      "source": [
        "!pip install pyprind"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0L-wSF6tngd",
        "outputId": "f0c27143-84ad-47fe-ba63-e7e79a0d28f9"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.11.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 38.1 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 81 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 133 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 143 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 163 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 174 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 184 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 194 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 204 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 215 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 225 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 235 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 245 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 256 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 266 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 276 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 286 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 296 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 307 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 317 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 327 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 337 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 348 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 358 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 368 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 378 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 389 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 399 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 409 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 419 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 430 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 440 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 450 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 460 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 471 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 481 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 491 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 501 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 512 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 522 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 532 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 542 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 552 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 563 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 573 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 583 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 593 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 604 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 614 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 624 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 634 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 645 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 655 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 665 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 675 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 686 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 696 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 706 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 716 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 727 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 737 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 747 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 757 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 768 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 778 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 788 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 798 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 808 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 819 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 829 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 839 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 849 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 860 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 870 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 880 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 890 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 901 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 911 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 921 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 931 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 942 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 952 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 962 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 972 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 983 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 993 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.0 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.0 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.0 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.3 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.8 MB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting urllib3>=1.26.5\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 73.9 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Collecting requests<3,>=2.0.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 947 kB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=25d8adc8a375851f916802b64c373d2ad9b31493ee866034792f2e2b1f00c9db\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=06bae2f7c4760f01c69ff9e42aabd051ac6b4a696ca78066aaeb7afab6f0fba4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, urllib3, gitdb, subprocess32, shortuuid, sentry-sdk, requests, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 requests-2.26.0 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 urllib3-1.26.6 wandb-0.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:36.027823Z",
          "iopub.execute_input": "2021-06-18T20:06:36.028151Z",
          "iopub.status.idle": "2021-06-18T20:06:38.856661Z",
          "shell.execute_reply.started": "2021-06-18T20:06:36.028106Z",
          "shell.execute_reply": "2021-06-18T20:06:38.855715Z"
        },
        "trusted": true,
        "id": "5IcOJBWphkn3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from itertools import groupby\n",
        "import sklearn\n",
        "import pyprind\n",
        "import wandb\n",
        "# !wandb login d164742a4a99e4e581f543102aff0992153ad225"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz3YnOOchkn3"
      },
      "source": [
        "## Importing the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.861206Z",
          "iopub.execute_input": "2021-06-18T20:06:38.861461Z",
          "iopub.status.idle": "2021-06-18T20:06:38.868954Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.861434Z",
          "shell.execute_reply": "2021-06-18T20:06:38.868167Z"
        },
        "trusted": true,
        "id": "PmnCD4dphkn4"
      },
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3]) \n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.87189Z",
          "iopub.execute_input": "2021-06-18T20:06:38.872152Z",
          "iopub.status.idle": "2021-06-18T20:06:38.923819Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.872107Z",
          "shell.execute_reply": "2021-06-18T20:06:38.922494Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBaX2AXuhkn4",
        "outputId": "14743520-e5c5-4f6f-a471-b499dab8e4d0"
      },
      "source": [
        "cc200_data_path = './data/filt_global/rois_cc200'#cc200'#path to time series data\n",
        "data_df = pd.read_csv('./data/Phenotypic_V1_0b_preprocessed949.csv',encoding= 'unicode_escape')#path \n",
        "data_df.DX_GROUP = data_df.DX_GROUP.map({1: 1, 2:0})\n",
        "data_df['FILE_PATH'] = data_df['FILE_ID'].apply(lambda x : os.path.join(cc200_data_path,x + '_rois_cc200.1D')) \n",
        "\n",
        "print('Length of data frame : ', len(data_df))\n",
        "print('Sample file path : ', data_df['FILE_PATH'].values[0])\n",
        "print(data_df.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of data frame :  949\n",
            "Sample file path :  ./data/filt_global/rois_cc200/Pitt_0050003_rois_cc200.1D\n",
            "   Unnamed: 0  ...                                          FILE_PATH\n",
            "0           0  ...  ./data/filt_global/rois_cc200/Pitt_0050003_roi...\n",
            "1           1  ...  ./data/filt_global/rois_cc200/Pitt_0050004_roi...\n",
            "2           2  ...  ./data/filt_global/rois_cc200/Pitt_0050006_roi...\n",
            "3           3  ...  ./data/filt_global/rois_cc200/Pitt_0050007_roi...\n",
            "4           4  ...  ./data/filt_global/rois_cc200/Pitt_0050009_roi...\n",
            "\n",
            "[5 rows x 107 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7DgqnL6hkn4"
      },
      "source": [
        "### Helper functions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.925344Z",
          "iopub.execute_input": "2021-06-18T20:06:38.925672Z",
          "iopub.status.idle": "2021-06-18T20:06:38.942734Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.925638Z",
          "shell.execute_reply": "2021-06-18T20:06:38.941792Z"
        },
        "trusted": true,
        "id": "CrCnO-6yhkn5"
      },
      "source": [
        "def get_label(filename):\n",
        "    assert (filename in labels)\n",
        "    return labels[filename]\n",
        "\n",
        "def get_corr_data(df):\n",
        "              \n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        return ma.masked_where(m, corr).compressed()\n",
        "        \n",
        "def get_corr_matrix(filename,data_path):\n",
        "    # returns correlation matrix\n",
        "    for file in os.listdir(data_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_path, file), sep='\\t')\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        return corr\n",
        "\n",
        "def confusion(g_turth,predictions):\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    sensitivity = (tp)/(tp+fn)\n",
        "    specificty = (tn)/(tn+fp)\n",
        "    return accuracy,sensitivity,specificty\n",
        "\n",
        "def get_regs(samplesnames,regnum):\n",
        "    # returns region index array\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        datas.append(all_corr[sn][0])\n",
        "    datas = np.array(datas)     # datas shape len(samplesnames)x19900\n",
        "    avg=[]\n",
        "    for ie in range(datas.shape[1]):\n",
        "        avg.append(np.mean(datas[:,ie]))\n",
        "    avg=np.array(avg)\n",
        "    highs=avg.argsort()[-regnum:][::-1]\n",
        "    lows=avg.argsort()[:regnum][::-1]\n",
        "    regions=np.concatenate((highs,lows),axis=0) # shape (9950,)\n",
        "    return regions"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbKoDX2y4Qzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d62a9b6-5527-40da-8da5-24adf10d0f43"
      },
      "source": [
        "labels = data_df['DX_GROUP'].values\n",
        "fpaths = data_df['FILE_PATH'].values\n",
        "flist = data_df['FILE_ID'].values\n",
        "\n",
        "print(\"Unique values in labels : \", np.unique(labels, return_counts = True))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique values in labels :  (array([0, 1]), array([530, 419]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.945281Z",
          "iopub.execute_input": "2021-06-18T20:06:38.94597Z",
          "iopub.status.idle": "2021-06-18T20:07:07.764978Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.945932Z",
          "shell.execute_reply": "2021-06-18T20:07:07.763126Z"
        },
        "trusted": true,
        "id": "mfjFNCivhkn6"
      },
      "source": [
        "# all_corr = {}\n",
        "\n",
        "# for i,path in enumerate(fpaths):\n",
        "#     key = flist[i]\n",
        "#     x = np.loadtxt(path)\n",
        "#     x = np.array(x, dtype = 'float32')\n",
        "#     x = get_corr_data(x)\n",
        "#     all_corr[key] = (x,labels[i])\n",
        "# print('Length of correlations vector : ', len(all_corr))\n",
        "# pickle.dump(all_corr, open('./data/SFC_CC200.pkl', 'wb'))\n",
        "# # pickle.dump(all_corr, open('./data/Timeseries_CC200.pkl', 'wb'))\n",
        "# print('Length of correlation vector in all_corr : ', len(all_corr[flist[0]][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3zlWZzR1qm6"
      },
      "source": [
        "all_corr = pickle.load(open('./data/SFC_CC200.pkl', 'rb'))\n",
        "# all_corr = pickle.load(open('./data/Timeseries_CC200.pkl', 'rb'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Jt7edf_yzm",
        "outputId": "d7eed059-1895-4c94-e989-9cdcc63b36bd"
      },
      "source": [
        "print(len(flist))\n",
        "print(flist[ : 10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "949\n",
            "['Pitt_0050003' 'Pitt_0050004' 'Pitt_0050006' 'Pitt_0050007'\n",
            " 'Pitt_0050009' 'Pitt_0050010' 'Pitt_0050011' 'Pitt_0050013'\n",
            " 'Pitt_0050014' 'Pitt_0050015']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch18LNuR-lfI",
        "outputId": "bacd6649-25dc-434d-81d3-9bebce65c49e"
      },
      "source": [
        "# sfc = []\n",
        "# labels = []\n",
        "# for i,f in enumerate(flist):\n",
        "#     sfc.append(all_corr[f][0])\n",
        "#     labels.append(all_corr[f][1])\n",
        "\n",
        "# sfc = np.asarray(sfc)\n",
        "# labels = np.asarray(labels)\n",
        "# print('SFC Shape : ',sfc.shape)\n",
        "# print('Labels Shape : ', labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SFC Shape :  (949, 19900)\n",
            "Labels Shape :  (949,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ_26cIy0L2Y",
        "outputId": "7856e40b-b71c-4f88-c73b-1cb469c44271"
      },
      "source": [
        "np.unique(labels, return_counts=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([530, 419]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T7UL0jw0VGO",
        "outputId": "5f9fb734-5d53-4b94-dfd8-5e768703b4c2"
      },
      "source": [
        "print(len(all_corr))\n",
        "print(all_corr[flist[0]][0].shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "949\n",
            "(19900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5daHy9cVnrKp"
      },
      "source": [
        "class ASDDataset(Dataset):\n",
        "    def __init__(self, all_corr, samples):\n",
        "        self.corr = all_corr\n",
        "        self.samples = samples\n",
        "        pass\n",
        "    def __getitem__(self,idx):\n",
        "        return torch.tensor(self.corr[self.samples[idx]][0],dtype=torch.float),torch.tensor(self.corr[self.samples[idx]][1],dtype=torch.float)\n",
        "        pass\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "        pass"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWyj8pQihkn7"
      },
      "source": [
        "# Defining Autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:29:50.96222Z",
          "iopub.execute_input": "2021-06-18T20:29:50.962557Z",
          "iopub.status.idle": "2021-06-18T20:29:51.064936Z",
          "shell.execute_reply.started": "2021-06-18T20:29:50.962527Z",
          "shell.execute_reply": "2021-06-18T20:29:51.06403Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i14YCA9Lhkn8",
        "outputId": "a6dcfe55-b3b1-4d71-b717-f4a45d271c82"
      },
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990, \n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        \n",
        "        self.num_latent = num_latent\n",
        "        self.num_inputs = num_inputs\n",
        "        \n",
        "        self.fc_encoder = nn.Sequential (\n",
        "                nn.Linear(self.num_inputs,4096),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(4096,1024),\n",
        "                nn.Tanh())\n",
        "        \n",
        "        self.fc_decoder = nn.Sequential (\n",
        "                nn.Linear(1024,4096),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(4096,self.num_inputs),\n",
        "                nn.Tanh())\n",
        "         \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Dropout(p=0.25),\n",
        "                nn.Linear(1024, 1),\n",
        "#                 nn.Sigmoid(),\n",
        "#                 nn.Linear(128, 1),\n",
        "\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Linear(1024, 1),\n",
        "#                 nn.Sigmoid(),\n",
        "#                 nn.Linear(128, 1),\n",
        "            )\n",
        "            \n",
        "         \n",
        "    def forward(self, x, eval_classifier=True):\n",
        "\n",
        "        x = self.fc_encoder(x)\n",
        "        if eval_classifier:\n",
        "            x_logit = self.classifier(x)   #   .squeeze(1)\n",
        "            x_logit = self.sigmoid(x_logit)\n",
        "            return x_logit \n",
        "\n",
        "        x = self.fc_decoder(x)        \n",
        "        return x\n",
        "\n",
        "model = MTAutoEncoder()\n",
        "\n",
        "model"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MTAutoEncoder(\n",
              "  (fc_encoder): Sequential(\n",
              "    (0): Linear(in_features=990, out_features=4096, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (3): Tanh()\n",
              "  )\n",
              "  (fc_decoder): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=4096, out_features=990, bias=True)\n",
              "    (3): Tanh()\n",
              "  )\n",
              "  (sigmoid): Sigmoid()\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw6wxQ7uhkn8"
      },
      "source": [
        "# Defining training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:29:54.242475Z",
          "iopub.execute_input": "2021-06-18T20:29:54.242797Z",
          "iopub.status.idle": "2021-06-18T20:29:54.272092Z",
          "shell.execute_reply.started": "2021-06-18T20:29:54.242766Z",
          "shell.execute_reply": "2021-06-18T20:29:54.271229Z"
        },
        "trusted": true,
        "id": "XJGFx-CFhkn8"
      },
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    clf_train_loss = []\n",
        "    ae_train_loss = []\n",
        "    \n",
        "    if mode == 'clf':\n",
        "        final_targets = []\n",
        "        final_predictions = []\n",
        "    else:\n",
        "        final_targets = None\n",
        "        final_predictions = None    \n",
        "    \n",
        "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
        "        if len(batch_x) != batch_size:\n",
        "            continue\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode == 'ae':\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy = model(data*rand_bernoulli, False)\n",
        "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
        "            else:\n",
        "                rec = model(data, False)\n",
        "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
        "                \n",
        "            loss_total = loss_ae\n",
        "            loss_ae_np = loss_ae.detach().cpu().numpy()\n",
        "            \n",
        "            clf_train_loss.append(0.0)\n",
        "            ae_train_loss.append(loss_ae_np)\n",
        "            train_losses.append([loss_ae_np, 0.0])\n",
        "                \n",
        "        if mode == 'clf':\n",
        "            logits = model(data, True)\n",
        "            logits = np.squeeze(logits, 1)\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "\n",
        "            proba = logits.detach().cpu().numpy()\n",
        "            # proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            predictions = np.ones_like(proba, dtype=np.int32)\n",
        "            predictions[proba < 0.5] = 0\n",
        "            \n",
        "            final_targets.append(target.detach().cpu().numpy())\n",
        "            final_predictions.append(predictions)\n",
        "            \n",
        "            loss_total = loss_clf\n",
        "            loss_clf_np = loss_clf.detach().cpu().numpy()\n",
        "            \n",
        "            clf_train_loss.append(loss_clf_np)\n",
        "            ae_train_loss.append(0.0)\n",
        "            train_losses.append([0.0,loss_clf_np])\n",
        "            \n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    if (final_targets is not None) and (final_predictions is not None):\n",
        "        final_targets = np.concatenate(final_targets)\n",
        "        final_predictions = np.concatenate(final_predictions)\n",
        "        train_accuracy = np.mean(final_targets == final_predictions)\n",
        "\n",
        "        return np.mean(clf_train_loss), train_accuracy\n",
        "    else:\n",
        "        return np.mean(ae_train_loss), None\n",
        "\n",
        "def validate(model, epoch, train_loader, mode='ae', lam_factor=1.0):\n",
        "    model.eval()\n",
        "    train_losses = []\n",
        "    clf_train_loss = []\n",
        "    ae_train_loss = []\n",
        "    \n",
        "    if mode == 'clf':\n",
        "        final_targets = []\n",
        "        final_predictions = []\n",
        "    else:\n",
        "        final_targets = None\n",
        "        final_predictions = None    \n",
        "    \n",
        "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
        "        if len(batch_x) != batch_size:\n",
        "            continue\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        if mode == 'ae':\n",
        "            rec = model(data, False)\n",
        "            loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
        "                \n",
        "            loss_total = loss_ae\n",
        "            loss_ae_np = loss_ae.detach().cpu().numpy()\n",
        "            \n",
        "            clf_train_loss.append(0.0)\n",
        "            ae_train_loss.append(loss_ae_np)\n",
        "            train_losses.append([loss_ae_np, 0.0])\n",
        "                \n",
        "        if mode == 'clf':\n",
        "            logits = model(data, True)\n",
        "            logits = np.squeeze(logits, 1)\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "            \n",
        "            proba = logits.detach().cpu().numpy()\n",
        "            # proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            predictions = np.ones_like(proba, dtype=np.int32)\n",
        "            predictions[proba < 0.5] = 0\n",
        "            \n",
        "            final_targets.append(target.detach().cpu().numpy())\n",
        "            final_predictions.append(predictions)\n",
        "            \n",
        "            loss_total = loss_clf\n",
        "            loss_clf_np = loss_clf.detach().cpu().numpy()\n",
        "            \n",
        "            clf_train_loss.append(loss_clf_np)\n",
        "            ae_train_loss.append(0.0)\n",
        "            train_losses.append([0.0,loss_clf_np])\n",
        "    \n",
        "    if (final_targets is not None) and (final_predictions is not None):\n",
        "        final_targets = np.concatenate(final_targets)\n",
        "        final_predictions = np.concatenate(final_predictions)\n",
        "        train_accuracy = np.mean(final_targets == final_predictions)\n",
        "\n",
        "        return np.mean(clf_train_loss), train_accuracy\n",
        "    else:\n",
        "        return np.mean(ae_train_loss), None\n",
        "\n",
        "\n",
        "def test(model, criterion, test_loader, \n",
        "         eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    eval_loss = []\n",
        "    all_predss=[]\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i,(batch_x,batch_y) in enumerate(test_loader):\n",
        "            if num_batch is not None:\n",
        "                if i >= num_batch:\n",
        "                    continue\n",
        "            data, target = batch_x.to(device), batch_y.to(device)\n",
        "            logits = model(data, eval_classifier)\n",
        "            logits = np.squeeze(logits, 1)\n",
        "#             test_loss += criterion(rec, data).detach().cpu().numpy() \n",
        "#             n_test += len(batch_x)\n",
        "            # target = np.squeeze(target, 1)\n",
        "            test_loss = criterion(logits, target).detach().cpu().numpy() \n",
        "            eval_loss.append(test_loss)\n",
        "            if eval_classifier:\n",
        "                proba = logits.detach().cpu().numpy()\n",
        "                # proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)###????\n",
        "                y_arr = np.array(batch_y, dtype=np.int32)\n",
        "\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
        "        metrics_dict = {'accuracy': np.round(mlp_acc, 4), \n",
        "                        'senstivity' : np.round(mlp_sens,4), \n",
        "                        'specificity' : np.round(mlp_spef,4), \n",
        "                        'loss' : np.round(np.mean(eval_loss),4)}\n",
        "        \n",
        "    return  metrics_dict"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:29:55.7098Z",
          "iopub.execute_input": "2021-06-18T20:29:55.710152Z",
          "iopub.status.idle": "2021-06-18T20:29:55.718895Z",
          "shell.execute_reply.started": "2021-06-18T20:29:55.710095Z",
          "shell.execute_reply": "2021-06-18T20:29:55.718161Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTlD8687hkn9",
        "outputId": "971050a9-4a22-4cb1-cd39-1dfbb24c05f7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEYrLdqSCM7D"
      },
      "source": [
        "def attribute_image_features(algorithm, inputs):\n",
        "    model.zero_grad()\n",
        "    model.eval()\n",
        "    tensor_attributions = algorithm.attribute(inputs = inputs, target = 0, return_convergence_delta=True)  \n",
        "    return tensor_attributions"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDhqS1mBHfWc",
        "outputId": "023ce422-40cf-4884-f13f-1c4da53ae93a"
      },
      "source": [
        "!pip install captum"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting captum\n",
            "  Downloading captum-0.4.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 81 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 245 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 256 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 266 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 276 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 286 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 296 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 307 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 317 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 327 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 337 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 348 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 358 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 368 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 378 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 389 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 399 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 409 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 419 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 430 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 440 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 450 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 460 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 471 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 481 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 491 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 501 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 512 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 522 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 532 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 542 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 552 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 563 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 573 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 583 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 593 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 604 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 614 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 624 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 634 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 645 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 655 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 665 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 675 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 686 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 696 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 706 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 716 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 727 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 737 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 747 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 757 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 768 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 778 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 788 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 798 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 808 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 819 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 829 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 839 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 849 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 860 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 870 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 880 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 890 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 901 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 911 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 921 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 931 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 942 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 952 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 962 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 972 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 983 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 993 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.2 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->captum) (1.15.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxl4AEHHHWm0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import Saliency\n",
        "from captum.attr import DeepLift\n",
        "from captum.attr import NoiseTunnel\n",
        "from captum.attr import visualization as viz\n",
        "from captum.attr import Saliency\n",
        "import torchvision"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys7LYh_DzuJv",
        "outputId": "711c2fff-5e4f-4484-da6b-d3758743dadd"
      },
      "source": [
        "len(val_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "214"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEsTbR4mxh5A"
      },
      "source": [
        "# ASD 2 Layer Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9532M-9sEBY",
        "outputId": "21e8b694-02b0-4742-a793-64ba90cb92d5"
      },
      "source": [
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n",
        "    \n",
        "    start = time.time()\n",
        "    batch_size = 16\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    ae_epochs = 50     # 30\n",
        "    clf_epochs = 50      # 50\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = True\n",
        "\n",
        "    start= time.time()\n",
        "\n",
        "#     print('p_bernoulli: ', p_bernoulli)\n",
        "#     print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "#           'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "#     print('use_dropout: ', use_dropout, '\\n')\n",
        "    \n",
        "    # list to store metrics after each fold\n",
        "    repeat_acc=[]\n",
        "    repeat_sen=[]\n",
        "    repeat_spec=[]\n",
        "    repeat_loss=[]\n",
        "    \n",
        "    \n",
        "    for rp in range(1):\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "    # list to store metrics after each fold\n",
        "        crossval_acc=[]\n",
        "        crossval_sen=[]\n",
        "        crossval_spec=[]\n",
        "        crossval_loss=[]\n",
        "        attributions = []\n",
        "        best_fold_val_samples = []\n",
        "        best_fold_weights = None\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, labels)):\n",
        "            \n",
        "            # NAME = f'asd-diagnet-fold-{kk+1}-rp-2'\n",
        "            # ID = f'fold-{kk+1}-rp-2.1'\n",
        "        \n",
        "            train_samples, test_samples = flist[train_index],flist[test_index]\n",
        "            train_labels = labels[train_index]\n",
        "            \n",
        "            train_samples, val_samples, train_labels, val_labels = train_test_split(train_samples, train_labels, test_size = 0.25,\n",
        "                                                              random_state = 42, stratify = train_labels)\n",
        "            \n",
        "            print('Number of train samples : ', len(train_samples))\n",
        "            print('Number of val samples : ', len(val_samples))\n",
        "            print('Number of test samples : ', len(test_samples))\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            num_inpp = 19900\n",
        "            n_lat = 512\n",
        "            \n",
        "            train_dataset = ASDDataset(all_corr, train_samples)\n",
        "            val_dataset = ASDDataset(all_corr, val_samples)\n",
        "            test_dataset = ASDDataset(all_corr, test_samples)\n",
        "            \n",
        "            train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False)\n",
        "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)\n",
        "            \n",
        "               \n",
        "\n",
        "            model = MTAutoEncoder(tied = False, num_inputs = num_inpp, num_latent = n_lat, use_dropout = use_dropout)\n",
        "            model = model.to(device)\n",
        "\n",
        "            criterion_ae = nn.MSELoss(reduction='sum')\n",
        "            criterion_clf = nn.BCELoss()               \n",
        "            optimizer = optim.Adam(model.parameters(),lr = 0.0001, weight_decay = 0.05)          \n",
        "\n",
        "\n",
        "\n",
        "            best_ae_model = None\n",
        "            best_ae_loss = sys.float_info.max\n",
        "            count = 0\n",
        "            \n",
        "            print(\"AE Training Started-----------\")\n",
        "            \n",
        "            for epoch in range(1, ae_epochs+1):\n",
        "                ae_train_loss,_ = train(model, epoch, train_dataloader, p_bernoulli, mode='ae')\n",
        "                print(f'Epoch {epoch}/{ae_epochs}')\n",
        "                train_content = f'AE Train loss: {(ae_train_loss):.4f}'\n",
        "\n",
        "                ae_val_loss,_ = validate(model, epoch, val_dataloader, mode='ae')\n",
        "                val_content = f'AE Val loss: {(ae_val_loss):.4f}'\n",
        "                print(train_content)\n",
        "                print(val_content)\n",
        "\n",
        "                if(count == 5):\n",
        "                    break\n",
        "\n",
        "                if(ae_val_loss < best_ae_loss):\n",
        "                    best_ae_model = model\n",
        "                    best_ae_loss = ae_val_loss\n",
        "                    count = 1\n",
        "                else:\n",
        "                    count = count + 1\n",
        "                    \n",
        "            \n",
        "            print(\"CLF Training Started-----------\")\n",
        "            best_clf_model = None\n",
        "            best_clf_acc = 0.0\n",
        "            count = 0\n",
        "            model = best_ae_model\n",
        "            for epoch in range(1, clf_epochs+1):\n",
        "                clf_train_loss, train_acc = train(model, epoch, train_dataloader, p_bernoulli, mode='clf')\n",
        "                train_content = f'CLF Train loss: {(clf_train_loss):.4f}, Train Accuracy: {(train_acc):.4f}'\n",
        "\n",
        "                clf_val_loss,val_acc = validate(model, epoch, val_dataloader, mode='clf')\n",
        "                val_content = f'CLF Val loss: {(clf_val_loss):.4f}, Validation Accuracy: {(val_acc):.4f}'\n",
        "\n",
        "                print(f'Epoch {epoch}/{clf_epochs}')\n",
        "                print(train_content)\n",
        "                print(val_content)\n",
        "\n",
        "                if(count == 10):\n",
        "                    break\n",
        "\n",
        "                if(val_acc > best_clf_acc):\n",
        "                    best_clf_model = model\n",
        "                    best_clf_acc = val_acc\n",
        "                    count = 1\n",
        "                else:\n",
        "                    count = count + 1\n",
        "        \n",
        "\n",
        "            metrics_dict = test(best_clf_model, criterion_clf, test_dataloader, eval_classifier=True)\n",
        "            print(\"-----------------------------\")\n",
        "            print(f'Fold {kk+1}/{p_fold}')\n",
        "            content = f'{metrics_dict}'\n",
        "            print(content)\n",
        "            print(\"-----------------------------\")\n",
        "            \n",
        "            crossval_acc.append(metrics_dict['accuracy'])\n",
        "            crossval_sen.append(metrics_dict['senstivity'])\n",
        "            crossval_spec.append(metrics_dict['specificity'])\n",
        "            crossval_loss.append(metrics_dict['loss'])\n",
        "\n",
        "            if(max(crossval_acc) == metrics_dict['accuracy']) :\n",
        "                best_fold_val_samples = val_samples\n",
        "                best_fold_weights = best_clf_model.state_dict()\n",
        "                best_fold_val_samples = np.array(best_fold_val_samples)\n",
        "                np.save('./Visualization/Best_Fold_Val_Samples.npy', best_fold_val_samples)\n",
        "                torch.save(best_clf_model.state_dict(), f'./Visualization/Best_Fold_Weights.pth')\n",
        "                      \n",
        "\n",
        "            \n",
        "            #save the model after each fold\n",
        "            \n",
        "            # recorder = {'optimizer': optimizer.state_dict(),\n",
        "            # 'model': model.state_dict(),\n",
        "            # 'fold' : kk+1,\n",
        "            # 'repitition' : rp+1}\n",
        "\n",
        "            # torch.save(recorder, f'{NAME}.pt')\n",
        "            \n",
        "        print(\"*********************************\")    \n",
        "        print(f'Average Value after 10 Folds and repeats one {rp+1}------->')\n",
        "        content = f'Accuracy: {np.round(np.mean(crossval_acc),4)}, Senstivity: {np.round(np.mean(crossval_sen),4)}, Specificity: {np.round(np.mean(crossval_spec),4)}, Loss: {np.round(np.mean(crossval_loss),4)}'\n",
        "        print(content)\n",
        "        print(\"*********************************\") \n",
        "        \n",
        "        repeat_acc.append(np.mean(crossval_acc))\n",
        "        repeat_sen.append(np.mean(crossval_sen))\n",
        "        repeat_spec.append(np.mean(crossval_spec))\n",
        "        repeat_loss.append(np.mean(crossval_loss))\n",
        "\n",
        "        attributions = np.array(attributions)\n",
        "    \n",
        "    print(f\"Average Value after 1 Repeat:\")\n",
        "    content = f'Accuracy: {np.round(np.mean(repeat_acc),4)}, Senstivity: {np.round(np.mean(repeat_sen),4)}, Specificity: {np.round(np.mean(repeat_spec),4)}, Loss: {np.round(np.mean(repeat_loss),4)}'\n",
        "    print(content)\n",
        "        \n",
        "    finish= time.time()\n",
        "    print(finish-start)\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 778.0757\n",
            "AE Val loss: 683.2441\n",
            "Epoch 2/50\n",
            "AE Train loss: 633.3138\n",
            "AE Val loss: 627.9912\n",
            "Epoch 3/50\n",
            "AE Train loss: 558.7278\n",
            "AE Val loss: 593.8788\n",
            "Epoch 4/50\n",
            "AE Train loss: 503.8748\n",
            "AE Val loss: 578.3251\n",
            "Epoch 5/50\n",
            "AE Train loss: 455.6268\n",
            "AE Val loss: 555.6616\n",
            "Epoch 6/50\n",
            "AE Train loss: 412.4742\n",
            "AE Val loss: 541.7264\n",
            "Epoch 7/50\n",
            "AE Train loss: 376.4464\n",
            "AE Val loss: 537.5723\n",
            "Epoch 8/50\n",
            "AE Train loss: 344.7002\n",
            "AE Val loss: 524.0412\n",
            "Epoch 9/50\n",
            "AE Train loss: 314.5940\n",
            "AE Val loss: 520.0790\n",
            "Epoch 10/50\n",
            "AE Train loss: 288.7501\n",
            "AE Val loss: 511.5185\n",
            "Epoch 11/50\n",
            "AE Train loss: 265.2958\n",
            "AE Val loss: 505.8721\n",
            "Epoch 12/50\n",
            "AE Train loss: 242.5514\n",
            "AE Val loss: 502.6372\n",
            "Epoch 13/50\n",
            "AE Train loss: 224.4874\n",
            "AE Val loss: 502.1778\n",
            "Epoch 14/50\n",
            "AE Train loss: 208.1373\n",
            "AE Val loss: 495.2220\n",
            "Epoch 15/50\n",
            "AE Train loss: 194.7257\n",
            "AE Val loss: 492.5378\n",
            "Epoch 16/50\n",
            "AE Train loss: 182.2031\n",
            "AE Val loss: 506.5889\n",
            "Epoch 17/50\n",
            "AE Train loss: 168.8730\n",
            "AE Val loss: 487.0474\n",
            "Epoch 18/50\n",
            "AE Train loss: 155.6577\n",
            "AE Val loss: 486.5289\n",
            "Epoch 19/50\n",
            "AE Train loss: 146.0246\n",
            "AE Val loss: 483.2578\n",
            "Epoch 20/50\n",
            "AE Train loss: 136.0797\n",
            "AE Val loss: 480.4032\n",
            "Epoch 21/50\n",
            "AE Train loss: 126.5687\n",
            "AE Val loss: 479.2373\n",
            "Epoch 22/50\n",
            "AE Train loss: 119.1428\n",
            "AE Val loss: 476.9525\n",
            "Epoch 23/50\n",
            "AE Train loss: 111.2127\n",
            "AE Val loss: 476.2090\n",
            "Epoch 24/50\n",
            "AE Train loss: 103.7378\n",
            "AE Val loss: 475.1577\n",
            "Epoch 25/50\n",
            "AE Train loss: 98.9673\n",
            "AE Val loss: 476.6523\n",
            "Epoch 26/50\n",
            "AE Train loss: 93.6775\n",
            "AE Val loss: 473.6323\n",
            "Epoch 27/50\n",
            "AE Train loss: 88.8378\n",
            "AE Val loss: 473.3986\n",
            "Epoch 28/50\n",
            "AE Train loss: 84.1509\n",
            "AE Val loss: 472.1827\n",
            "Epoch 29/50\n",
            "AE Train loss: 79.4404\n",
            "AE Val loss: 469.8757\n",
            "Epoch 30/50\n",
            "AE Train loss: 75.5390\n",
            "AE Val loss: 469.3360\n",
            "Epoch 31/50\n",
            "AE Train loss: 71.7370\n",
            "AE Val loss: 469.0909\n",
            "Epoch 32/50\n",
            "AE Train loss: 67.9292\n",
            "AE Val loss: 470.7704\n",
            "Epoch 33/50\n",
            "AE Train loss: 64.2005\n",
            "AE Val loss: 467.3911\n",
            "Epoch 34/50\n",
            "AE Train loss: 61.1498\n",
            "AE Val loss: 466.9339\n",
            "Epoch 35/50\n",
            "AE Train loss: 57.8448\n",
            "AE Val loss: 467.1520\n",
            "Epoch 36/50\n",
            "AE Train loss: 54.9964\n",
            "AE Val loss: 471.5096\n",
            "Epoch 37/50\n",
            "AE Train loss: 53.0068\n",
            "AE Val loss: 465.7266\n",
            "Epoch 38/50\n",
            "AE Train loss: 50.6404\n",
            "AE Val loss: 464.8555\n",
            "Epoch 39/50\n",
            "AE Train loss: 48.4178\n",
            "AE Val loss: 464.0856\n",
            "Epoch 40/50\n",
            "AE Train loss: 46.6625\n",
            "AE Val loss: 467.1789\n",
            "Epoch 41/50\n",
            "AE Train loss: 44.3978\n",
            "AE Val loss: 463.9987\n",
            "Epoch 42/50\n",
            "AE Train loss: 43.0653\n",
            "AE Val loss: 462.6784\n",
            "Epoch 43/50\n",
            "AE Train loss: 43.0554\n",
            "AE Val loss: 465.5685\n",
            "Epoch 44/50\n",
            "AE Train loss: 42.0658\n",
            "AE Val loss: 464.5468\n",
            "Epoch 45/50\n",
            "AE Train loss: 40.4609\n",
            "AE Val loss: 465.7370\n",
            "Epoch 46/50\n",
            "AE Train loss: 39.5976\n",
            "AE Val loss: 466.0053\n",
            "Epoch 47/50\n",
            "AE Train loss: 38.6631\n",
            "AE Val loss: 465.1069\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.6906, Train Accuracy: 0.5453\n",
            "CLF Val loss: 0.6623, Validation Accuracy: 0.5817\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6356, Train Accuracy: 0.6391\n",
            "CLF Val loss: 0.6392, Validation Accuracy: 0.6106\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6129, Train Accuracy: 0.6797\n",
            "CLF Val loss: 0.6246, Validation Accuracy: 0.6202\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.5839, Train Accuracy: 0.7203\n",
            "CLF Val loss: 0.6151, Validation Accuracy: 0.6106\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5602, Train Accuracy: 0.7312\n",
            "CLF Val loss: 0.5999, Validation Accuracy: 0.6346\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5422, Train Accuracy: 0.7531\n",
            "CLF Val loss: 0.5933, Validation Accuracy: 0.6490\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5130, Train Accuracy: 0.7781\n",
            "CLF Val loss: 0.5806, Validation Accuracy: 0.6635\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.4987, Train Accuracy: 0.7953\n",
            "CLF Val loss: 0.5725, Validation Accuracy: 0.6635\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.4766, Train Accuracy: 0.8203\n",
            "CLF Val loss: 0.5606, Validation Accuracy: 0.6731\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4623, Train Accuracy: 0.8344\n",
            "CLF Val loss: 0.5525, Validation Accuracy: 0.6683\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4390, Train Accuracy: 0.8531\n",
            "CLF Val loss: 0.5468, Validation Accuracy: 0.6635\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4182, Train Accuracy: 0.8547\n",
            "CLF Val loss: 0.5393, Validation Accuracy: 0.6875\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.3960, Train Accuracy: 0.8828\n",
            "CLF Val loss: 0.5436, Validation Accuracy: 0.6827\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.3749, Train Accuracy: 0.8938\n",
            "CLF Val loss: 0.5274, Validation Accuracy: 0.7019\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3561, Train Accuracy: 0.9062\n",
            "CLF Val loss: 0.5418, Validation Accuracy: 0.6827\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3329, Train Accuracy: 0.9078\n",
            "CLF Val loss: 0.5155, Validation Accuracy: 0.7115\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3189, Train Accuracy: 0.9172\n",
            "CLF Val loss: 0.5200, Validation Accuracy: 0.6971\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.2943, Train Accuracy: 0.9391\n",
            "CLF Val loss: 0.5165, Validation Accuracy: 0.6923\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.2750, Train Accuracy: 0.9437\n",
            "CLF Val loss: 0.5114, Validation Accuracy: 0.7067\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2557, Train Accuracy: 0.9500\n",
            "CLF Val loss: 0.5268, Validation Accuracy: 0.6971\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2361, Train Accuracy: 0.9625\n",
            "CLF Val loss: 0.5288, Validation Accuracy: 0.7019\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2211, Train Accuracy: 0.9688\n",
            "CLF Val loss: 0.5349, Validation Accuracy: 0.7019\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2099, Train Accuracy: 0.9750\n",
            "CLF Val loss: 0.5434, Validation Accuracy: 0.6971\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.1833, Train Accuracy: 0.9828\n",
            "CLF Val loss: 0.5795, Validation Accuracy: 0.6779\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.1707, Train Accuracy: 0.9906\n",
            "CLF Val loss: 0.5241, Validation Accuracy: 0.6827\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1553, Train Accuracy: 0.9906\n",
            "CLF Val loss: 0.5393, Validation Accuracy: 0.6875\n",
            "-----------------------------\n",
            "Fold 1/10\n",
            "{'accuracy': 0.7368, 'senstivity': 0.6444, 'specificity': 0.82, 'loss': 0.5474}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 790.0258\n",
            "AE Val loss: 690.3062\n",
            "Epoch 2/50\n",
            "AE Train loss: 639.3943\n",
            "AE Val loss: 632.8745\n",
            "Epoch 3/50\n",
            "AE Train loss: 566.7640\n",
            "AE Val loss: 599.7310\n",
            "Epoch 4/50\n",
            "AE Train loss: 509.3762\n",
            "AE Val loss: 578.5810\n",
            "Epoch 5/50\n",
            "AE Train loss: 462.6570\n",
            "AE Val loss: 565.0191\n",
            "Epoch 6/50\n",
            "AE Train loss: 420.9200\n",
            "AE Val loss: 551.7546\n",
            "Epoch 7/50\n",
            "AE Train loss: 385.5534\n",
            "AE Val loss: 538.3484\n",
            "Epoch 8/50\n",
            "AE Train loss: 347.4473\n",
            "AE Val loss: 526.5504\n",
            "Epoch 9/50\n",
            "AE Train loss: 316.8590\n",
            "AE Val loss: 523.3118\n",
            "Epoch 10/50\n",
            "AE Train loss: 288.5147\n",
            "AE Val loss: 514.6426\n",
            "Epoch 11/50\n",
            "AE Train loss: 264.4611\n",
            "AE Val loss: 510.9299\n",
            "Epoch 12/50\n",
            "AE Train loss: 243.8704\n",
            "AE Val loss: 505.2348\n",
            "Epoch 13/50\n",
            "AE Train loss: 227.1058\n",
            "AE Val loss: 509.0490\n",
            "Epoch 14/50\n",
            "AE Train loss: 209.8345\n",
            "AE Val loss: 497.7591\n",
            "Epoch 15/50\n",
            "AE Train loss: 193.8446\n",
            "AE Val loss: 495.1185\n",
            "Epoch 16/50\n",
            "AE Train loss: 180.2515\n",
            "AE Val loss: 493.2995\n",
            "Epoch 17/50\n",
            "AE Train loss: 169.6021\n",
            "AE Val loss: 491.3696\n",
            "Epoch 18/50\n",
            "AE Train loss: 158.4781\n",
            "AE Val loss: 490.2642\n",
            "Epoch 19/50\n",
            "AE Train loss: 150.3952\n",
            "AE Val loss: 491.1905\n",
            "Epoch 20/50\n",
            "AE Train loss: 140.6671\n",
            "AE Val loss: 486.2537\n",
            "Epoch 21/50\n",
            "AE Train loss: 130.1382\n",
            "AE Val loss: 483.9547\n",
            "Epoch 22/50\n",
            "AE Train loss: 121.0122\n",
            "AE Val loss: 483.2909\n",
            "Epoch 23/50\n",
            "AE Train loss: 113.3979\n",
            "AE Val loss: 480.9613\n",
            "Epoch 24/50\n",
            "AE Train loss: 105.4271\n",
            "AE Val loss: 479.7749\n",
            "Epoch 25/50\n",
            "AE Train loss: 98.8871\n",
            "AE Val loss: 478.1772\n",
            "Epoch 26/50\n",
            "AE Train loss: 93.0636\n",
            "AE Val loss: 477.7792\n",
            "Epoch 27/50\n",
            "AE Train loss: 87.7951\n",
            "AE Val loss: 475.2077\n",
            "Epoch 28/50\n",
            "AE Train loss: 83.3065\n",
            "AE Val loss: 479.2786\n",
            "Epoch 29/50\n",
            "AE Train loss: 78.9837\n",
            "AE Val loss: 477.8402\n",
            "Epoch 30/50\n",
            "AE Train loss: 74.6633\n",
            "AE Val loss: 472.0218\n",
            "Epoch 31/50\n",
            "AE Train loss: 70.5714\n",
            "AE Val loss: 472.9525\n",
            "Epoch 32/50\n",
            "AE Train loss: 67.0149\n",
            "AE Val loss: 472.9059\n",
            "Epoch 33/50\n",
            "AE Train loss: 63.8146\n",
            "AE Val loss: 472.5451\n",
            "Epoch 34/50\n",
            "AE Train loss: 61.6594\n",
            "AE Val loss: 472.0650\n",
            "Epoch 35/50\n",
            "AE Train loss: 59.0266\n",
            "AE Val loss: 471.3705\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.7033, Train Accuracy: 0.4984\n",
            "CLF Val loss: 0.6757, Validation Accuracy: 0.5577\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6569, Train Accuracy: 0.6094\n",
            "CLF Val loss: 0.6480, Validation Accuracy: 0.5962\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6311, Train Accuracy: 0.6641\n",
            "CLF Val loss: 0.6268, Validation Accuracy: 0.6298\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.6101, Train Accuracy: 0.6609\n",
            "CLF Val loss: 0.6119, Validation Accuracy: 0.6346\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5861, Train Accuracy: 0.7109\n",
            "CLF Val loss: 0.5976, Validation Accuracy: 0.6442\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5634, Train Accuracy: 0.7234\n",
            "CLF Val loss: 0.5856, Validation Accuracy: 0.6683\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5469, Train Accuracy: 0.7500\n",
            "CLF Val loss: 0.5731, Validation Accuracy: 0.6827\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5338, Train Accuracy: 0.7719\n",
            "CLF Val loss: 0.5625, Validation Accuracy: 0.7115\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.5069, Train Accuracy: 0.7797\n",
            "CLF Val loss: 0.5561, Validation Accuracy: 0.7019\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4901, Train Accuracy: 0.8016\n",
            "CLF Val loss: 0.5442, Validation Accuracy: 0.7163\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4700, Train Accuracy: 0.8250\n",
            "CLF Val loss: 0.5409, Validation Accuracy: 0.7260\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4506, Train Accuracy: 0.8375\n",
            "CLF Val loss: 0.5300, Validation Accuracy: 0.7452\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4307, Train Accuracy: 0.8562\n",
            "CLF Val loss: 0.5231, Validation Accuracy: 0.7452\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.4131, Train Accuracy: 0.8734\n",
            "CLF Val loss: 0.5179, Validation Accuracy: 0.7404\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3961, Train Accuracy: 0.8859\n",
            "CLF Val loss: 0.5123, Validation Accuracy: 0.7452\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3750, Train Accuracy: 0.9031\n",
            "CLF Val loss: 0.5141, Validation Accuracy: 0.7404\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3515, Train Accuracy: 0.9031\n",
            "CLF Val loss: 0.5043, Validation Accuracy: 0.7452\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3291, Train Accuracy: 0.9203\n",
            "CLF Val loss: 0.4987, Validation Accuracy: 0.7452\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.3151, Train Accuracy: 0.9187\n",
            "CLF Val loss: 0.4950, Validation Accuracy: 0.7356\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2961, Train Accuracy: 0.9250\n",
            "CLF Val loss: 0.4959, Validation Accuracy: 0.7500\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2718, Train Accuracy: 0.9469\n",
            "CLF Val loss: 0.4920, Validation Accuracy: 0.7500\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2556, Train Accuracy: 0.9563\n",
            "CLF Val loss: 0.4926, Validation Accuracy: 0.7308\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2361, Train Accuracy: 0.9641\n",
            "CLF Val loss: 0.4938, Validation Accuracy: 0.7356\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.2212, Train Accuracy: 0.9672\n",
            "CLF Val loss: 0.5011, Validation Accuracy: 0.7308\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.2008, Train Accuracy: 0.9734\n",
            "CLF Val loss: 0.4925, Validation Accuracy: 0.7356\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1879, Train Accuracy: 0.9719\n",
            "CLF Val loss: 0.5067, Validation Accuracy: 0.7260\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1692, Train Accuracy: 0.9938\n",
            "CLF Val loss: 0.4971, Validation Accuracy: 0.7452\n",
            "Epoch 28/50\n",
            "CLF Train loss: 0.1559, Train Accuracy: 0.9891\n",
            "CLF Val loss: 0.4980, Validation Accuracy: 0.7356\n",
            "Epoch 29/50\n",
            "CLF Train loss: 0.1449, Train Accuracy: 0.9938\n",
            "CLF Val loss: 0.5066, Validation Accuracy: 0.7260\n",
            "Epoch 30/50\n",
            "CLF Train loss: 0.1368, Train Accuracy: 0.9969\n",
            "CLF Val loss: 0.5086, Validation Accuracy: 0.7260\n",
            "-----------------------------\n",
            "Fold 2/10\n",
            "{'accuracy': 0.7895, 'senstivity': 0.6098, 'specificity': 0.9259, 'loss': 0.4537}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 779.3384\n",
            "AE Val loss: 702.6533\n",
            "Epoch 2/50\n",
            "AE Train loss: 637.3856\n",
            "AE Val loss: 646.7421\n",
            "Epoch 3/50\n",
            "AE Train loss: 562.8640\n",
            "AE Val loss: 609.3099\n",
            "Epoch 4/50\n",
            "AE Train loss: 504.3979\n",
            "AE Val loss: 584.6469\n",
            "Epoch 5/50\n",
            "AE Train loss: 457.3251\n",
            "AE Val loss: 577.2900\n",
            "Epoch 6/50\n",
            "AE Train loss: 416.8223\n",
            "AE Val loss: 557.4211\n",
            "Epoch 7/50\n",
            "AE Train loss: 377.5751\n",
            "AE Val loss: 546.2949\n",
            "Epoch 8/50\n",
            "AE Train loss: 346.1227\n",
            "AE Val loss: 537.6112\n",
            "Epoch 9/50\n",
            "AE Train loss: 315.1363\n",
            "AE Val loss: 531.7931\n",
            "Epoch 10/50\n",
            "AE Train loss: 289.8797\n",
            "AE Val loss: 524.8214\n",
            "Epoch 11/50\n",
            "AE Train loss: 266.6233\n",
            "AE Val loss: 520.8365\n",
            "Epoch 12/50\n",
            "AE Train loss: 244.8855\n",
            "AE Val loss: 521.8640\n",
            "Epoch 13/50\n",
            "AE Train loss: 226.6062\n",
            "AE Val loss: 513.2236\n",
            "Epoch 14/50\n",
            "AE Train loss: 210.4730\n",
            "AE Val loss: 508.0831\n",
            "Epoch 15/50\n",
            "AE Train loss: 194.3107\n",
            "AE Val loss: 513.4658\n",
            "Epoch 16/50\n",
            "AE Train loss: 180.8608\n",
            "AE Val loss: 502.0065\n",
            "Epoch 17/50\n",
            "AE Train loss: 168.3879\n",
            "AE Val loss: 499.6964\n",
            "Epoch 18/50\n",
            "AE Train loss: 157.4601\n",
            "AE Val loss: 497.5773\n",
            "Epoch 19/50\n",
            "AE Train loss: 146.0725\n",
            "AE Val loss: 498.4625\n",
            "Epoch 20/50\n",
            "AE Train loss: 135.6276\n",
            "AE Val loss: 494.4663\n",
            "Epoch 21/50\n",
            "AE Train loss: 126.4358\n",
            "AE Val loss: 492.2875\n",
            "Epoch 22/50\n",
            "AE Train loss: 119.1572\n",
            "AE Val loss: 490.3255\n",
            "Epoch 23/50\n",
            "AE Train loss: 114.1826\n",
            "AE Val loss: 490.8150\n",
            "Epoch 24/50\n",
            "AE Train loss: 108.2718\n",
            "AE Val loss: 491.0319\n",
            "Epoch 25/50\n",
            "AE Train loss: 103.1138\n",
            "AE Val loss: 489.3688\n",
            "Epoch 26/50\n",
            "AE Train loss: 96.2178\n",
            "AE Val loss: 487.8600\n",
            "Epoch 27/50\n",
            "AE Train loss: 90.4286\n",
            "AE Val loss: 485.7538\n",
            "Epoch 28/50\n",
            "AE Train loss: 86.1557\n",
            "AE Val loss: 485.1047\n",
            "Epoch 29/50\n",
            "AE Train loss: 81.2570\n",
            "AE Val loss: 483.2368\n",
            "Epoch 30/50\n",
            "AE Train loss: 76.6778\n",
            "AE Val loss: 486.2621\n",
            "Epoch 31/50\n",
            "AE Train loss: 72.0547\n",
            "AE Val loss: 482.3703\n",
            "Epoch 32/50\n",
            "AE Train loss: 67.7939\n",
            "AE Val loss: 482.1484\n",
            "Epoch 33/50\n",
            "AE Train loss: 64.0865\n",
            "AE Val loss: 480.7114\n",
            "Epoch 34/50\n",
            "AE Train loss: 61.1652\n",
            "AE Val loss: 483.6525\n",
            "Epoch 35/50\n",
            "AE Train loss: 58.6095\n",
            "AE Val loss: 488.5654\n",
            "Epoch 36/50\n",
            "AE Train loss: 56.2193\n",
            "AE Val loss: 480.1719\n",
            "Epoch 37/50\n",
            "AE Train loss: 53.4697\n",
            "AE Val loss: 481.5132\n",
            "Epoch 38/50\n",
            "AE Train loss: 50.9679\n",
            "AE Val loss: 479.8314\n",
            "Epoch 39/50\n",
            "AE Train loss: 48.6298\n",
            "AE Val loss: 479.0486\n",
            "Epoch 40/50\n",
            "AE Train loss: 45.8653\n",
            "AE Val loss: 477.3457\n",
            "Epoch 41/50\n",
            "AE Train loss: 44.0183\n",
            "AE Val loss: 478.3664\n",
            "Epoch 42/50\n",
            "AE Train loss: 42.7242\n",
            "AE Val loss: 475.7760\n",
            "Epoch 43/50\n",
            "AE Train loss: 42.2132\n",
            "AE Val loss: 477.2430\n",
            "Epoch 44/50\n",
            "AE Train loss: 41.5227\n",
            "AE Val loss: 476.9849\n",
            "Epoch 45/50\n",
            "AE Train loss: 40.3932\n",
            "AE Val loss: 475.6689\n",
            "Epoch 46/50\n",
            "AE Train loss: 39.4116\n",
            "AE Val loss: 477.8435\n",
            "Epoch 47/50\n",
            "AE Train loss: 38.5537\n",
            "AE Val loss: 480.0586\n",
            "Epoch 48/50\n",
            "AE Train loss: 37.0545\n",
            "AE Val loss: 479.1766\n",
            "Epoch 49/50\n",
            "AE Train loss: 36.1240\n",
            "AE Val loss: 479.4114\n",
            "Epoch 50/50\n",
            "AE Train loss: 34.8001\n",
            "AE Val loss: 476.6743\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.6924, Train Accuracy: 0.5391\n",
            "CLF Val loss: 0.6622, Validation Accuracy: 0.5913\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6500, Train Accuracy: 0.6266\n",
            "CLF Val loss: 0.6330, Validation Accuracy: 0.6442\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6179, Train Accuracy: 0.6672\n",
            "CLF Val loss: 0.6127, Validation Accuracy: 0.6490\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.5923, Train Accuracy: 0.6750\n",
            "CLF Val loss: 0.5953, Validation Accuracy: 0.6827\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5682, Train Accuracy: 0.7250\n",
            "CLF Val loss: 0.5856, Validation Accuracy: 0.6779\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5462, Train Accuracy: 0.7500\n",
            "CLF Val loss: 0.5705, Validation Accuracy: 0.6827\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5269, Train Accuracy: 0.7906\n",
            "CLF Val loss: 0.5619, Validation Accuracy: 0.6875\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5071, Train Accuracy: 0.7922\n",
            "CLF Val loss: 0.5514, Validation Accuracy: 0.7115\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.4866, Train Accuracy: 0.8187\n",
            "CLF Val loss: 0.5427, Validation Accuracy: 0.7115\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4693, Train Accuracy: 0.8266\n",
            "CLF Val loss: 0.5341, Validation Accuracy: 0.7260\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4496, Train Accuracy: 0.8484\n",
            "CLF Val loss: 0.5281, Validation Accuracy: 0.7404\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4242, Train Accuracy: 0.8719\n",
            "CLF Val loss: 0.5233, Validation Accuracy: 0.7500\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4115, Train Accuracy: 0.8703\n",
            "CLF Val loss: 0.5135, Validation Accuracy: 0.7404\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.3912, Train Accuracy: 0.8750\n",
            "CLF Val loss: 0.5101, Validation Accuracy: 0.7356\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3719, Train Accuracy: 0.8875\n",
            "CLF Val loss: 0.5159, Validation Accuracy: 0.7548\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3466, Train Accuracy: 0.9172\n",
            "CLF Val loss: 0.4989, Validation Accuracy: 0.7452\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3274, Train Accuracy: 0.9156\n",
            "CLF Val loss: 0.4977, Validation Accuracy: 0.7692\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3092, Train Accuracy: 0.9359\n",
            "CLF Val loss: 0.4927, Validation Accuracy: 0.7596\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.2902, Train Accuracy: 0.9422\n",
            "CLF Val loss: 0.4883, Validation Accuracy: 0.7356\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2712, Train Accuracy: 0.9563\n",
            "CLF Val loss: 0.4919, Validation Accuracy: 0.7452\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2530, Train Accuracy: 0.9594\n",
            "CLF Val loss: 0.4973, Validation Accuracy: 0.7404\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2319, Train Accuracy: 0.9641\n",
            "CLF Val loss: 0.4868, Validation Accuracy: 0.7260\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2158, Train Accuracy: 0.9672\n",
            "CLF Val loss: 0.4846, Validation Accuracy: 0.7212\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.1980, Train Accuracy: 0.9844\n",
            "CLF Val loss: 0.4924, Validation Accuracy: 0.7404\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.1852, Train Accuracy: 0.9812\n",
            "CLF Val loss: 0.4947, Validation Accuracy: 0.7356\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1705, Train Accuracy: 0.9844\n",
            "CLF Val loss: 0.5014, Validation Accuracy: 0.7356\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1559, Train Accuracy: 0.9922\n",
            "CLF Val loss: 0.5077, Validation Accuracy: 0.7308\n",
            "-----------------------------\n",
            "Fold 3/10\n",
            "{'accuracy': 0.6632, 'senstivity': 0.5682, 'specificity': 0.7451, 'loss': 0.571}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 780.5985\n",
            "AE Val loss: 683.0958\n",
            "Epoch 2/50\n",
            "AE Train loss: 635.3152\n",
            "AE Val loss: 627.3012\n",
            "Epoch 3/50\n",
            "AE Train loss: 561.8988\n",
            "AE Val loss: 591.4812\n",
            "Epoch 4/50\n",
            "AE Train loss: 503.7188\n",
            "AE Val loss: 570.7045\n",
            "Epoch 5/50\n",
            "AE Train loss: 456.2804\n",
            "AE Val loss: 557.4717\n",
            "Epoch 6/50\n",
            "AE Train loss: 414.7556\n",
            "AE Val loss: 547.4787\n",
            "Epoch 7/50\n",
            "AE Train loss: 378.4407\n",
            "AE Val loss: 534.6577\n",
            "Epoch 8/50\n",
            "AE Train loss: 343.3842\n",
            "AE Val loss: 526.5757\n",
            "Epoch 9/50\n",
            "AE Train loss: 314.5954\n",
            "AE Val loss: 520.9769\n",
            "Epoch 10/50\n",
            "AE Train loss: 289.2568\n",
            "AE Val loss: 515.4717\n",
            "Epoch 11/50\n",
            "AE Train loss: 264.1827\n",
            "AE Val loss: 509.2937\n",
            "Epoch 12/50\n",
            "AE Train loss: 241.8412\n",
            "AE Val loss: 503.8618\n",
            "Epoch 13/50\n",
            "AE Train loss: 222.4769\n",
            "AE Val loss: 500.1001\n",
            "Epoch 14/50\n",
            "AE Train loss: 205.7816\n",
            "AE Val loss: 497.5858\n",
            "Epoch 15/50\n",
            "AE Train loss: 191.9521\n",
            "AE Val loss: 497.9009\n",
            "Epoch 16/50\n",
            "AE Train loss: 179.8441\n",
            "AE Val loss: 494.6445\n",
            "Epoch 17/50\n",
            "AE Train loss: 167.0284\n",
            "AE Val loss: 490.5833\n",
            "Epoch 18/50\n",
            "AE Train loss: 155.4431\n",
            "AE Val loss: 490.1651\n",
            "Epoch 19/50\n",
            "AE Train loss: 145.1246\n",
            "AE Val loss: 488.0410\n",
            "Epoch 20/50\n",
            "AE Train loss: 136.0288\n",
            "AE Val loss: 485.3984\n",
            "Epoch 21/50\n",
            "AE Train loss: 127.2820\n",
            "AE Val loss: 486.9193\n",
            "Epoch 22/50\n",
            "AE Train loss: 118.5221\n",
            "AE Val loss: 480.7029\n",
            "Epoch 23/50\n",
            "AE Train loss: 111.9153\n",
            "AE Val loss: 479.2282\n",
            "Epoch 24/50\n",
            "AE Train loss: 104.6123\n",
            "AE Val loss: 482.4874\n",
            "Epoch 25/50\n",
            "AE Train loss: 98.0432\n",
            "AE Val loss: 477.0859\n",
            "Epoch 26/50\n",
            "AE Train loss: 91.9143\n",
            "AE Val loss: 480.2141\n",
            "Epoch 27/50\n",
            "AE Train loss: 87.0450\n",
            "AE Val loss: 481.3648\n",
            "Epoch 28/50\n",
            "AE Train loss: 83.1582\n",
            "AE Val loss: 474.9378\n",
            "Epoch 29/50\n",
            "AE Train loss: 79.7946\n",
            "AE Val loss: 477.2593\n",
            "Epoch 30/50\n",
            "AE Train loss: 76.6447\n",
            "AE Val loss: 473.5021\n",
            "Epoch 31/50\n",
            "AE Train loss: 72.5457\n",
            "AE Val loss: 473.7444\n",
            "Epoch 32/50\n",
            "AE Train loss: 67.9005\n",
            "AE Val loss: 474.6691\n",
            "Epoch 33/50\n",
            "AE Train loss: 63.4761\n",
            "AE Val loss: 470.6555\n",
            "Epoch 34/50\n",
            "AE Train loss: 59.6161\n",
            "AE Val loss: 469.9559\n",
            "Epoch 35/50\n",
            "AE Train loss: 57.1532\n",
            "AE Val loss: 468.8347\n",
            "Epoch 36/50\n",
            "AE Train loss: 54.6960\n",
            "AE Val loss: 470.0529\n",
            "Epoch 37/50\n",
            "AE Train loss: 51.5273\n",
            "AE Val loss: 468.7539\n",
            "Epoch 38/50\n",
            "AE Train loss: 48.8876\n",
            "AE Val loss: 468.0060\n",
            "Epoch 39/50\n",
            "AE Train loss: 47.2229\n",
            "AE Val loss: 467.2110\n",
            "Epoch 40/50\n",
            "AE Train loss: 45.8125\n",
            "AE Val loss: 468.0578\n",
            "Epoch 41/50\n",
            "AE Train loss: 44.5648\n",
            "AE Val loss: 466.6453\n",
            "Epoch 42/50\n",
            "AE Train loss: 42.8722\n",
            "AE Val loss: 466.4425\n",
            "Epoch 43/50\n",
            "AE Train loss: 41.3218\n",
            "AE Val loss: 471.0989\n",
            "Epoch 44/50\n",
            "AE Train loss: 40.7895\n",
            "AE Val loss: 468.3605\n",
            "Epoch 45/50\n",
            "AE Train loss: 40.3193\n",
            "AE Val loss: 467.8387\n",
            "Epoch 46/50\n",
            "AE Train loss: 39.5278\n",
            "AE Val loss: 470.1203\n",
            "Epoch 47/50\n",
            "AE Train loss: 38.4227\n",
            "AE Val loss: 469.9220\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.6845, Train Accuracy: 0.5750\n",
            "CLF Val loss: 0.6643, Validation Accuracy: 0.5721\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6463, Train Accuracy: 0.6219\n",
            "CLF Val loss: 0.6422, Validation Accuracy: 0.6058\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6121, Train Accuracy: 0.6844\n",
            "CLF Val loss: 0.6286, Validation Accuracy: 0.6442\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.5926, Train Accuracy: 0.7219\n",
            "CLF Val loss: 0.6165, Validation Accuracy: 0.6394\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5695, Train Accuracy: 0.7094\n",
            "CLF Val loss: 0.6073, Validation Accuracy: 0.6298\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5504, Train Accuracy: 0.7516\n",
            "CLF Val loss: 0.5974, Validation Accuracy: 0.6490\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5296, Train Accuracy: 0.7641\n",
            "CLF Val loss: 0.5885, Validation Accuracy: 0.6442\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5076, Train Accuracy: 0.8016\n",
            "CLF Val loss: 0.5804, Validation Accuracy: 0.6538\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.4929, Train Accuracy: 0.8094\n",
            "CLF Val loss: 0.5671, Validation Accuracy: 0.6971\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4730, Train Accuracy: 0.8156\n",
            "CLF Val loss: 0.5586, Validation Accuracy: 0.7212\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4553, Train Accuracy: 0.8266\n",
            "CLF Val loss: 0.5555, Validation Accuracy: 0.6923\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4363, Train Accuracy: 0.8578\n",
            "CLF Val loss: 0.5433, Validation Accuracy: 0.7115\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4169, Train Accuracy: 0.8609\n",
            "CLF Val loss: 0.5375, Validation Accuracy: 0.7260\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.3950, Train Accuracy: 0.8766\n",
            "CLF Val loss: 0.5255, Validation Accuracy: 0.7452\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3747, Train Accuracy: 0.9016\n",
            "CLF Val loss: 0.5221, Validation Accuracy: 0.7212\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3514, Train Accuracy: 0.8938\n",
            "CLF Val loss: 0.5147, Validation Accuracy: 0.7308\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3347, Train Accuracy: 0.9187\n",
            "CLF Val loss: 0.5069, Validation Accuracy: 0.7740\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3160, Train Accuracy: 0.9109\n",
            "CLF Val loss: 0.5065, Validation Accuracy: 0.7212\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.2984, Train Accuracy: 0.9313\n",
            "CLF Val loss: 0.4965, Validation Accuracy: 0.7644\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2758, Train Accuracy: 0.9469\n",
            "CLF Val loss: 0.4961, Validation Accuracy: 0.7500\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2559, Train Accuracy: 0.9563\n",
            "CLF Val loss: 0.5109, Validation Accuracy: 0.7163\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2368, Train Accuracy: 0.9625\n",
            "CLF Val loss: 0.4916, Validation Accuracy: 0.7596\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2170, Train Accuracy: 0.9797\n",
            "CLF Val loss: 0.4843, Validation Accuracy: 0.7740\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.1998, Train Accuracy: 0.9828\n",
            "CLF Val loss: 0.4816, Validation Accuracy: 0.7788\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.1851, Train Accuracy: 0.9938\n",
            "CLF Val loss: 0.4855, Validation Accuracy: 0.7644\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1689, Train Accuracy: 0.9922\n",
            "CLF Val loss: 0.4942, Validation Accuracy: 0.7308\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1575, Train Accuracy: 0.9969\n",
            "CLF Val loss: 0.4871, Validation Accuracy: 0.7644\n",
            "Epoch 28/50\n",
            "CLF Train loss: 0.1422, Train Accuracy: 0.9969\n",
            "CLF Val loss: 0.4835, Validation Accuracy: 0.7740\n",
            "Epoch 29/50\n",
            "CLF Train loss: 0.1314, Train Accuracy: 0.9984\n",
            "CLF Val loss: 0.5074, Validation Accuracy: 0.7260\n",
            "Epoch 30/50\n",
            "CLF Train loss: 0.1269, Train Accuracy: 0.9969\n",
            "CLF Val loss: 0.4845, Validation Accuracy: 0.7644\n",
            "Epoch 31/50\n",
            "CLF Train loss: 0.1120, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4865, Validation Accuracy: 0.7788\n",
            "Epoch 32/50\n",
            "CLF Train loss: 0.1021, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4868, Validation Accuracy: 0.7740\n",
            "Epoch 33/50\n",
            "CLF Train loss: 0.0958, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5076, Validation Accuracy: 0.7356\n",
            "Epoch 34/50\n",
            "CLF Train loss: 0.0913, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4909, Validation Accuracy: 0.7788\n",
            "-----------------------------\n",
            "Fold 4/10\n",
            "{'accuracy': 0.7579, 'senstivity': 0.6744, 'specificity': 0.8269, 'loss': 0.5071}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 774.0389\n",
            "AE Val loss: 701.1234\n",
            "Epoch 2/50\n",
            "AE Train loss: 631.9821\n",
            "AE Val loss: 643.9176\n",
            "Epoch 3/50\n",
            "AE Train loss: 557.8148\n",
            "AE Val loss: 610.0947\n",
            "Epoch 4/50\n",
            "AE Train loss: 500.0500\n",
            "AE Val loss: 586.7263\n",
            "Epoch 5/50\n",
            "AE Train loss: 454.0875\n",
            "AE Val loss: 572.8213\n",
            "Epoch 6/50\n",
            "AE Train loss: 415.5550\n",
            "AE Val loss: 564.9949\n",
            "Epoch 7/50\n",
            "AE Train loss: 376.5131\n",
            "AE Val loss: 551.8880\n",
            "Epoch 8/50\n",
            "AE Train loss: 345.3962\n",
            "AE Val loss: 551.9301\n",
            "Epoch 9/50\n",
            "AE Train loss: 316.1642\n",
            "AE Val loss: 540.6598\n",
            "Epoch 10/50\n",
            "AE Train loss: 288.6449\n",
            "AE Val loss: 528.2326\n",
            "Epoch 11/50\n",
            "AE Train loss: 265.0255\n",
            "AE Val loss: 534.4410\n",
            "Epoch 12/50\n",
            "AE Train loss: 243.6675\n",
            "AE Val loss: 524.7803\n",
            "Epoch 13/50\n",
            "AE Train loss: 224.9717\n",
            "AE Val loss: 515.0703\n",
            "Epoch 14/50\n",
            "AE Train loss: 208.1434\n",
            "AE Val loss: 512.4156\n",
            "Epoch 15/50\n",
            "AE Train loss: 191.6792\n",
            "AE Val loss: 512.5762\n",
            "Epoch 16/50\n",
            "AE Train loss: 178.7088\n",
            "AE Val loss: 507.1743\n",
            "Epoch 17/50\n",
            "AE Train loss: 167.3126\n",
            "AE Val loss: 504.7430\n",
            "Epoch 18/50\n",
            "AE Train loss: 156.3604\n",
            "AE Val loss: 508.1245\n",
            "Epoch 19/50\n",
            "AE Train loss: 147.4929\n",
            "AE Val loss: 500.0617\n",
            "Epoch 20/50\n",
            "AE Train loss: 140.5088\n",
            "AE Val loss: 496.8155\n",
            "Epoch 21/50\n",
            "AE Train loss: 131.3589\n",
            "AE Val loss: 496.0090\n",
            "Epoch 22/50\n",
            "AE Train loss: 122.7319\n",
            "AE Val loss: 493.3213\n",
            "Epoch 23/50\n",
            "AE Train loss: 114.0754\n",
            "AE Val loss: 491.4859\n",
            "Epoch 24/50\n",
            "AE Train loss: 106.8580\n",
            "AE Val loss: 491.1240\n",
            "Epoch 25/50\n",
            "AE Train loss: 99.6946\n",
            "AE Val loss: 489.1797\n",
            "Epoch 26/50\n",
            "AE Train loss: 94.0052\n",
            "AE Val loss: 488.9194\n",
            "Epoch 27/50\n",
            "AE Train loss: 88.2801\n",
            "AE Val loss: 491.9157\n",
            "Epoch 28/50\n",
            "AE Train loss: 84.2177\n",
            "AE Val loss: 488.7483\n",
            "Epoch 29/50\n",
            "AE Train loss: 80.4156\n",
            "AE Val loss: 488.1306\n",
            "Epoch 30/50\n",
            "AE Train loss: 76.8889\n",
            "AE Val loss: 484.9581\n",
            "Epoch 31/50\n",
            "AE Train loss: 72.9071\n",
            "AE Val loss: 485.2775\n",
            "Epoch 32/50\n",
            "AE Train loss: 69.3933\n",
            "AE Val loss: 489.0844\n",
            "Epoch 33/50\n",
            "AE Train loss: 65.2296\n",
            "AE Val loss: 484.4977\n",
            "Epoch 34/50\n",
            "AE Train loss: 61.2411\n",
            "AE Val loss: 483.6973\n",
            "Epoch 35/50\n",
            "AE Train loss: 58.7707\n",
            "AE Val loss: 482.0693\n",
            "Epoch 36/50\n",
            "AE Train loss: 56.6398\n",
            "AE Val loss: 484.2625\n",
            "Epoch 37/50\n",
            "AE Train loss: 53.6869\n",
            "AE Val loss: 482.5989\n",
            "Epoch 38/50\n",
            "AE Train loss: 51.7035\n",
            "AE Val loss: 480.6436\n",
            "Epoch 39/50\n",
            "AE Train loss: 49.5293\n",
            "AE Val loss: 480.3887\n",
            "Epoch 40/50\n",
            "AE Train loss: 48.1811\n",
            "AE Val loss: 481.4517\n",
            "Epoch 41/50\n",
            "AE Train loss: 46.2173\n",
            "AE Val loss: 480.0005\n",
            "Epoch 42/50\n",
            "AE Train loss: 43.7874\n",
            "AE Val loss: 480.0672\n",
            "Epoch 43/50\n",
            "AE Train loss: 42.0698\n",
            "AE Val loss: 480.1418\n",
            "Epoch 44/50\n",
            "AE Train loss: 41.2910\n",
            "AE Val loss: 478.1404\n",
            "Epoch 45/50\n",
            "AE Train loss: 40.5990\n",
            "AE Val loss: 481.0490\n",
            "Epoch 46/50\n",
            "AE Train loss: 39.6611\n",
            "AE Val loss: 478.7795\n",
            "Epoch 47/50\n",
            "AE Train loss: 38.1682\n",
            "AE Val loss: 478.7314\n",
            "Epoch 48/50\n",
            "AE Train loss: 36.4112\n",
            "AE Val loss: 479.6126\n",
            "Epoch 49/50\n",
            "AE Train loss: 35.1486\n",
            "AE Val loss: 479.4519\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.6806, Train Accuracy: 0.5641\n",
            "CLF Val loss: 0.6642, Validation Accuracy: 0.6010\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6382, Train Accuracy: 0.6406\n",
            "CLF Val loss: 0.6417, Validation Accuracy: 0.5962\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6002, Train Accuracy: 0.6891\n",
            "CLF Val loss: 0.6281, Validation Accuracy: 0.6106\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.5758, Train Accuracy: 0.7094\n",
            "CLF Val loss: 0.6132, Validation Accuracy: 0.6394\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5441, Train Accuracy: 0.7562\n",
            "CLF Val loss: 0.6037, Validation Accuracy: 0.6538\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5260, Train Accuracy: 0.7641\n",
            "CLF Val loss: 0.5949, Validation Accuracy: 0.6683\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5100, Train Accuracy: 0.7828\n",
            "CLF Val loss: 0.5849, Validation Accuracy: 0.7212\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.4807, Train Accuracy: 0.8156\n",
            "CLF Val loss: 0.5848, Validation Accuracy: 0.6635\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.4614, Train Accuracy: 0.8250\n",
            "CLF Val loss: 0.5751, Validation Accuracy: 0.6779\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4414, Train Accuracy: 0.8500\n",
            "CLF Val loss: 0.5726, Validation Accuracy: 0.6779\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4184, Train Accuracy: 0.8688\n",
            "CLF Val loss: 0.5699, Validation Accuracy: 0.6779\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4040, Train Accuracy: 0.8719\n",
            "CLF Val loss: 0.5596, Validation Accuracy: 0.7163\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.3831, Train Accuracy: 0.8906\n",
            "CLF Val loss: 0.5539, Validation Accuracy: 0.7308\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.3605, Train Accuracy: 0.9109\n",
            "CLF Val loss: 0.5579, Validation Accuracy: 0.7115\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3385, Train Accuracy: 0.9047\n",
            "CLF Val loss: 0.5587, Validation Accuracy: 0.7067\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3168, Train Accuracy: 0.9281\n",
            "CLF Val loss: 0.5529, Validation Accuracy: 0.7260\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3031, Train Accuracy: 0.9437\n",
            "CLF Val loss: 0.5528, Validation Accuracy: 0.7260\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.2789, Train Accuracy: 0.9500\n",
            "CLF Val loss: 0.5619, Validation Accuracy: 0.7019\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.2591, Train Accuracy: 0.9594\n",
            "CLF Val loss: 0.5622, Validation Accuracy: 0.7260\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2402, Train Accuracy: 0.9672\n",
            "CLF Val loss: 0.5851, Validation Accuracy: 0.7115\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2232, Train Accuracy: 0.9656\n",
            "CLF Val loss: 0.5673, Validation Accuracy: 0.7163\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2045, Train Accuracy: 0.9781\n",
            "CLF Val loss: 0.5663, Validation Accuracy: 0.7067\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.1894, Train Accuracy: 0.9828\n",
            "CLF Val loss: 0.5756, Validation Accuracy: 0.7067\n",
            "-----------------------------\n",
            "Fold 5/10\n",
            "{'accuracy': 0.6947, 'senstivity': 0.6889, 'specificity': 0.7, 'loss': 0.592}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 774.0406\n",
            "AE Val loss: 713.2607\n",
            "Epoch 2/50\n",
            "AE Train loss: 630.6404\n",
            "AE Val loss: 652.9176\n",
            "Epoch 3/50\n",
            "AE Train loss: 555.9164\n",
            "AE Val loss: 618.1705\n",
            "Epoch 4/50\n",
            "AE Train loss: 498.4015\n",
            "AE Val loss: 596.7305\n",
            "Epoch 5/50\n",
            "AE Train loss: 450.5412\n",
            "AE Val loss: 581.3144\n",
            "Epoch 6/50\n",
            "AE Train loss: 416.5974\n",
            "AE Val loss: 578.0712\n",
            "Epoch 7/50\n",
            "AE Train loss: 376.5007\n",
            "AE Val loss: 561.5117\n",
            "Epoch 8/50\n",
            "AE Train loss: 339.6017\n",
            "AE Val loss: 550.9893\n",
            "Epoch 9/50\n",
            "AE Train loss: 312.1794\n",
            "AE Val loss: 542.5651\n",
            "Epoch 10/50\n",
            "AE Train loss: 284.1458\n",
            "AE Val loss: 537.9669\n",
            "Epoch 11/50\n",
            "AE Train loss: 261.6826\n",
            "AE Val loss: 537.2186\n",
            "Epoch 12/50\n",
            "AE Train loss: 243.6906\n",
            "AE Val loss: 526.2411\n",
            "Epoch 13/50\n",
            "AE Train loss: 223.6517\n",
            "AE Val loss: 522.3484\n",
            "Epoch 14/50\n",
            "AE Train loss: 206.8784\n",
            "AE Val loss: 519.8504\n",
            "Epoch 15/50\n",
            "AE Train loss: 192.3232\n",
            "AE Val loss: 515.2171\n",
            "Epoch 16/50\n",
            "AE Train loss: 179.5404\n",
            "AE Val loss: 515.8903\n",
            "Epoch 17/50\n",
            "AE Train loss: 168.0755\n",
            "AE Val loss: 513.0607\n",
            "Epoch 18/50\n",
            "AE Train loss: 154.6593\n",
            "AE Val loss: 507.1357\n",
            "Epoch 19/50\n",
            "AE Train loss: 143.2763\n",
            "AE Val loss: 506.2242\n",
            "Epoch 20/50\n",
            "AE Train loss: 133.4340\n",
            "AE Val loss: 503.0944\n",
            "Epoch 21/50\n",
            "AE Train loss: 124.7593\n",
            "AE Val loss: 502.2436\n",
            "Epoch 22/50\n",
            "AE Train loss: 118.0474\n",
            "AE Val loss: 502.3362\n",
            "Epoch 23/50\n",
            "AE Train loss: 112.4652\n",
            "AE Val loss: 503.9911\n",
            "Epoch 24/50\n",
            "AE Train loss: 106.9538\n",
            "AE Val loss: 501.9767\n",
            "Epoch 25/50\n",
            "AE Train loss: 100.5730\n",
            "AE Val loss: 498.6416\n",
            "Epoch 26/50\n",
            "AE Train loss: 94.9731\n",
            "AE Val loss: 497.7031\n",
            "Epoch 27/50\n",
            "AE Train loss: 88.4088\n",
            "AE Val loss: 497.3797\n",
            "Epoch 28/50\n",
            "AE Train loss: 83.1160\n",
            "AE Val loss: 494.7463\n",
            "Epoch 29/50\n",
            "AE Train loss: 78.7236\n",
            "AE Val loss: 494.4415\n",
            "Epoch 30/50\n",
            "AE Train loss: 74.1941\n",
            "AE Val loss: 492.5203\n",
            "Epoch 31/50\n",
            "AE Train loss: 71.0106\n",
            "AE Val loss: 492.8496\n",
            "Epoch 32/50\n",
            "AE Train loss: 67.8983\n",
            "AE Val loss: 495.4778\n",
            "Epoch 33/50\n",
            "AE Train loss: 64.3160\n",
            "AE Val loss: 492.2586\n",
            "Epoch 34/50\n",
            "AE Train loss: 61.1514\n",
            "AE Val loss: 494.4571\n",
            "Epoch 35/50\n",
            "AE Train loss: 57.8246\n",
            "AE Val loss: 489.6631\n",
            "Epoch 36/50\n",
            "AE Train loss: 54.5996\n",
            "AE Val loss: 490.3638\n",
            "Epoch 37/50\n",
            "AE Train loss: 52.1143\n",
            "AE Val loss: 489.1229\n",
            "Epoch 38/50\n",
            "AE Train loss: 50.0758\n",
            "AE Val loss: 488.1119\n",
            "Epoch 39/50\n",
            "AE Train loss: 48.6662\n",
            "AE Val loss: 488.5562\n",
            "Epoch 40/50\n",
            "AE Train loss: 47.1219\n",
            "AE Val loss: 490.9734\n",
            "Epoch 41/50\n",
            "AE Train loss: 45.4380\n",
            "AE Val loss: 489.3486\n",
            "Epoch 42/50\n",
            "AE Train loss: 43.9684\n",
            "AE Val loss: 487.2780\n",
            "Epoch 43/50\n",
            "AE Train loss: 42.1493\n",
            "AE Val loss: 488.9106\n",
            "Epoch 44/50\n",
            "AE Train loss: 40.7737\n",
            "AE Val loss: 490.9928\n",
            "Epoch 45/50\n",
            "AE Train loss: 39.3369\n",
            "AE Val loss: 489.2441\n",
            "Epoch 46/50\n",
            "AE Train loss: 37.6745\n",
            "AE Val loss: 485.7448\n",
            "Epoch 47/50\n",
            "AE Train loss: 36.8103\n",
            "AE Val loss: 489.5520\n",
            "Epoch 48/50\n",
            "AE Train loss: 35.9345\n",
            "AE Val loss: 487.6287\n",
            "Epoch 49/50\n",
            "AE Train loss: 35.3838\n",
            "AE Val loss: 487.1574\n",
            "Epoch 50/50\n",
            "AE Train loss: 35.7219\n",
            "AE Val loss: 487.5534\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.7012, Train Accuracy: 0.5266\n",
            "CLF Val loss: 0.6528, Validation Accuracy: 0.5865\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6621, Train Accuracy: 0.6016\n",
            "CLF Val loss: 0.6257, Validation Accuracy: 0.6538\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6331, Train Accuracy: 0.6641\n",
            "CLF Val loss: 0.6057, Validation Accuracy: 0.6971\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.6119, Train Accuracy: 0.6828\n",
            "CLF Val loss: 0.5897, Validation Accuracy: 0.7067\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5864, Train Accuracy: 0.7031\n",
            "CLF Val loss: 0.5788, Validation Accuracy: 0.7067\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5607, Train Accuracy: 0.7359\n",
            "CLF Val loss: 0.5680, Validation Accuracy: 0.7019\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5410, Train Accuracy: 0.7812\n",
            "CLF Val loss: 0.5586, Validation Accuracy: 0.7404\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5223, Train Accuracy: 0.7641\n",
            "CLF Val loss: 0.5488, Validation Accuracy: 0.7500\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.5080, Train Accuracy: 0.8063\n",
            "CLF Val loss: 0.5393, Validation Accuracy: 0.7356\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4864, Train Accuracy: 0.8203\n",
            "CLF Val loss: 0.5313, Validation Accuracy: 0.7500\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4694, Train Accuracy: 0.8313\n",
            "CLF Val loss: 0.5225, Validation Accuracy: 0.7740\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4524, Train Accuracy: 0.8500\n",
            "CLF Val loss: 0.5247, Validation Accuracy: 0.7644\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4358, Train Accuracy: 0.8500\n",
            "CLF Val loss: 0.5070, Validation Accuracy: 0.7692\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.4109, Train Accuracy: 0.8750\n",
            "CLF Val loss: 0.4996, Validation Accuracy: 0.7837\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3881, Train Accuracy: 0.8875\n",
            "CLF Val loss: 0.4928, Validation Accuracy: 0.7788\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3737, Train Accuracy: 0.8953\n",
            "CLF Val loss: 0.4891, Validation Accuracy: 0.7788\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3509, Train Accuracy: 0.9094\n",
            "CLF Val loss: 0.4842, Validation Accuracy: 0.7740\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3332, Train Accuracy: 0.9141\n",
            "CLF Val loss: 0.4764, Validation Accuracy: 0.7788\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.3131, Train Accuracy: 0.9297\n",
            "CLF Val loss: 0.4764, Validation Accuracy: 0.7692\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2924, Train Accuracy: 0.9297\n",
            "CLF Val loss: 0.4650, Validation Accuracy: 0.8029\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2712, Train Accuracy: 0.9422\n",
            "CLF Val loss: 0.4616, Validation Accuracy: 0.7788\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2509, Train Accuracy: 0.9578\n",
            "CLF Val loss: 0.4602, Validation Accuracy: 0.7981\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2342, Train Accuracy: 0.9578\n",
            "CLF Val loss: 0.4585, Validation Accuracy: 0.7788\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.2191, Train Accuracy: 0.9750\n",
            "CLF Val loss: 0.4631, Validation Accuracy: 0.7596\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.2011, Train Accuracy: 0.9781\n",
            "CLF Val loss: 0.4579, Validation Accuracy: 0.7740\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1818, Train Accuracy: 0.9859\n",
            "CLF Val loss: 0.4512, Validation Accuracy: 0.7885\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1696, Train Accuracy: 0.9891\n",
            "CLF Val loss: 0.4519, Validation Accuracy: 0.7788\n",
            "Epoch 28/50\n",
            "CLF Train loss: 0.1564, Train Accuracy: 0.9938\n",
            "CLF Val loss: 0.4531, Validation Accuracy: 0.7837\n",
            "Epoch 29/50\n",
            "CLF Train loss: 0.1428, Train Accuracy: 0.9953\n",
            "CLF Val loss: 0.4548, Validation Accuracy: 0.7885\n",
            "Epoch 30/50\n",
            "CLF Train loss: 0.1313, Train Accuracy: 0.9969\n",
            "CLF Val loss: 0.4537, Validation Accuracy: 0.7740\n",
            "-----------------------------\n",
            "Fold 6/10\n",
            "{'accuracy': 0.7684, 'senstivity': 0.6829, 'specificity': 0.8333, 'loss': 0.4883}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 774.1143\n",
            "AE Val loss: 710.6205\n",
            "Epoch 2/50\n",
            "AE Train loss: 629.2239\n",
            "AE Val loss: 649.3480\n",
            "Epoch 3/50\n",
            "AE Train loss: 556.3741\n",
            "AE Val loss: 615.5054\n",
            "Epoch 4/50\n",
            "AE Train loss: 498.9537\n",
            "AE Val loss: 593.8350\n",
            "Epoch 5/50\n",
            "AE Train loss: 454.3519\n",
            "AE Val loss: 577.0730\n",
            "Epoch 6/50\n",
            "AE Train loss: 412.8130\n",
            "AE Val loss: 563.9506\n",
            "Epoch 7/50\n",
            "AE Train loss: 375.4015\n",
            "AE Val loss: 554.4268\n",
            "Epoch 8/50\n",
            "AE Train loss: 343.6829\n",
            "AE Val loss: 545.5590\n",
            "Epoch 9/50\n",
            "AE Train loss: 314.0305\n",
            "AE Val loss: 541.6528\n",
            "Epoch 10/50\n",
            "AE Train loss: 289.0392\n",
            "AE Val loss: 532.0728\n",
            "Epoch 11/50\n",
            "AE Train loss: 263.7385\n",
            "AE Val loss: 523.4288\n",
            "Epoch 12/50\n",
            "AE Train loss: 242.5770\n",
            "AE Val loss: 520.9260\n",
            "Epoch 13/50\n",
            "AE Train loss: 223.8747\n",
            "AE Val loss: 519.2308\n",
            "Epoch 14/50\n",
            "AE Train loss: 206.5507\n",
            "AE Val loss: 517.2311\n",
            "Epoch 15/50\n",
            "AE Train loss: 192.2917\n",
            "AE Val loss: 512.6244\n",
            "Epoch 16/50\n",
            "AE Train loss: 179.3091\n",
            "AE Val loss: 507.2766\n",
            "Epoch 17/50\n",
            "AE Train loss: 167.1713\n",
            "AE Val loss: 504.7914\n",
            "Epoch 18/50\n",
            "AE Train loss: 155.1782\n",
            "AE Val loss: 501.6700\n",
            "Epoch 19/50\n",
            "AE Train loss: 144.5795\n",
            "AE Val loss: 501.7864\n",
            "Epoch 20/50\n",
            "AE Train loss: 135.3837\n",
            "AE Val loss: 499.0757\n",
            "Epoch 21/50\n",
            "AE Train loss: 128.2555\n",
            "AE Val loss: 499.2439\n",
            "Epoch 22/50\n",
            "AE Train loss: 121.3367\n",
            "AE Val loss: 495.8167\n",
            "Epoch 23/50\n",
            "AE Train loss: 114.6416\n",
            "AE Val loss: 496.6196\n",
            "Epoch 24/50\n",
            "AE Train loss: 108.3688\n",
            "AE Val loss: 494.0692\n",
            "Epoch 25/50\n",
            "AE Train loss: 101.6994\n",
            "AE Val loss: 492.5846\n",
            "Epoch 26/50\n",
            "AE Train loss: 95.0307\n",
            "AE Val loss: 491.4929\n",
            "Epoch 27/50\n",
            "AE Train loss: 88.8459\n",
            "AE Val loss: 490.0954\n",
            "Epoch 28/50\n",
            "AE Train loss: 83.1308\n",
            "AE Val loss: 489.0561\n",
            "Epoch 29/50\n",
            "AE Train loss: 77.9560\n",
            "AE Val loss: 486.1989\n",
            "Epoch 30/50\n",
            "AE Train loss: 73.9104\n",
            "AE Val loss: 487.4468\n",
            "Epoch 31/50\n",
            "AE Train loss: 70.0680\n",
            "AE Val loss: 487.2628\n",
            "Epoch 32/50\n",
            "AE Train loss: 66.0804\n",
            "AE Val loss: 484.5412\n",
            "Epoch 33/50\n",
            "AE Train loss: 64.1534\n",
            "AE Val loss: 486.8017\n",
            "Epoch 34/50\n",
            "AE Train loss: 62.0329\n",
            "AE Val loss: 485.0480\n",
            "Epoch 35/50\n",
            "AE Train loss: 58.6096\n",
            "AE Val loss: 485.1613\n",
            "Epoch 36/50\n",
            "AE Train loss: 55.5994\n",
            "AE Val loss: 485.5739\n",
            "Epoch 37/50\n",
            "AE Train loss: 53.6051\n",
            "AE Val loss: 484.5773\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.6954, Train Accuracy: 0.5125\n",
            "CLF Val loss: 0.6861, Validation Accuracy: 0.5385\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6545, Train Accuracy: 0.6016\n",
            "CLF Val loss: 0.6620, Validation Accuracy: 0.6202\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6279, Train Accuracy: 0.6531\n",
            "CLF Val loss: 0.6409, Validation Accuracy: 0.6490\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.6002, Train Accuracy: 0.7125\n",
            "CLF Val loss: 0.6270, Validation Accuracy: 0.6635\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5769, Train Accuracy: 0.7297\n",
            "CLF Val loss: 0.6158, Validation Accuracy: 0.6779\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5550, Train Accuracy: 0.7344\n",
            "CLF Val loss: 0.6053, Validation Accuracy: 0.6971\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5398, Train Accuracy: 0.7672\n",
            "CLF Val loss: 0.5953, Validation Accuracy: 0.7067\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5175, Train Accuracy: 0.7953\n",
            "CLF Val loss: 0.5847, Validation Accuracy: 0.7163\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.5016, Train Accuracy: 0.7953\n",
            "CLF Val loss: 0.5753, Validation Accuracy: 0.7019\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4795, Train Accuracy: 0.8109\n",
            "CLF Val loss: 0.5714, Validation Accuracy: 0.7019\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4597, Train Accuracy: 0.8328\n",
            "CLF Val loss: 0.5625, Validation Accuracy: 0.6971\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4415, Train Accuracy: 0.8531\n",
            "CLF Val loss: 0.5631, Validation Accuracy: 0.7019\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4223, Train Accuracy: 0.8672\n",
            "CLF Val loss: 0.5498, Validation Accuracy: 0.7067\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.4030, Train Accuracy: 0.8844\n",
            "CLF Val loss: 0.5428, Validation Accuracy: 0.7163\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3834, Train Accuracy: 0.8922\n",
            "CLF Val loss: 0.5358, Validation Accuracy: 0.7212\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3590, Train Accuracy: 0.9109\n",
            "CLF Val loss: 0.5354, Validation Accuracy: 0.7212\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3420, Train Accuracy: 0.9125\n",
            "CLF Val loss: 0.5245, Validation Accuracy: 0.7212\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3208, Train Accuracy: 0.9281\n",
            "CLF Val loss: 0.5289, Validation Accuracy: 0.7260\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.3018, Train Accuracy: 0.9313\n",
            "CLF Val loss: 0.5470, Validation Accuracy: 0.7115\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2856, Train Accuracy: 0.9437\n",
            "CLF Val loss: 0.5312, Validation Accuracy: 0.7308\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2685, Train Accuracy: 0.9547\n",
            "CLF Val loss: 0.5205, Validation Accuracy: 0.7500\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2528, Train Accuracy: 0.9625\n",
            "CLF Val loss: 0.5260, Validation Accuracy: 0.7452\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2302, Train Accuracy: 0.9672\n",
            "CLF Val loss: 0.5193, Validation Accuracy: 0.7500\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.2166, Train Accuracy: 0.9828\n",
            "CLF Val loss: 0.5676, Validation Accuracy: 0.7212\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.2041, Train Accuracy: 0.9719\n",
            "CLF Val loss: 0.5266, Validation Accuracy: 0.7452\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1876, Train Accuracy: 0.9797\n",
            "CLF Val loss: 0.5198, Validation Accuracy: 0.7452\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1699, Train Accuracy: 0.9891\n",
            "CLF Val loss: 0.5308, Validation Accuracy: 0.7452\n",
            "Epoch 28/50\n",
            "CLF Train loss: 0.1535, Train Accuracy: 0.9953\n",
            "CLF Val loss: 0.5418, Validation Accuracy: 0.7500\n",
            "Epoch 29/50\n",
            "CLF Train loss: 0.1412, Train Accuracy: 0.9984\n",
            "CLF Val loss: 0.5372, Validation Accuracy: 0.7548\n",
            "Epoch 30/50\n",
            "CLF Train loss: 0.1324, Train Accuracy: 0.9984\n",
            "CLF Val loss: 0.5433, Validation Accuracy: 0.7500\n",
            "Epoch 31/50\n",
            "CLF Train loss: 0.1202, Train Accuracy: 0.9969\n",
            "CLF Val loss: 0.5497, Validation Accuracy: 0.7548\n",
            "Epoch 32/50\n",
            "CLF Train loss: 0.1113, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5646, Validation Accuracy: 0.7548\n",
            "Epoch 33/50\n",
            "CLF Train loss: 0.1023, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5620, Validation Accuracy: 0.7500\n",
            "Epoch 34/50\n",
            "CLF Train loss: 0.0957, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5595, Validation Accuracy: 0.7596\n",
            "Epoch 35/50\n",
            "CLF Train loss: 0.0896, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5528, Validation Accuracy: 0.7452\n",
            "Epoch 36/50\n",
            "CLF Train loss: 0.0842, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5504, Validation Accuracy: 0.7500\n",
            "Epoch 37/50\n",
            "CLF Train loss: 0.0802, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5635, Validation Accuracy: 0.7500\n",
            "Epoch 38/50\n",
            "CLF Train loss: 0.0771, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5671, Validation Accuracy: 0.7548\n",
            "Epoch 39/50\n",
            "CLF Train loss: 0.0714, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5745, Validation Accuracy: 0.7500\n",
            "Epoch 40/50\n",
            "CLF Train loss: 0.0675, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5786, Validation Accuracy: 0.7548\n",
            "Epoch 41/50\n",
            "CLF Train loss: 0.0647, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.6161, Validation Accuracy: 0.7548\n",
            "Epoch 42/50\n",
            "CLF Train loss: 0.0634, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5671, Validation Accuracy: 0.7548\n",
            "Epoch 43/50\n",
            "CLF Train loss: 0.0624, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5733, Validation Accuracy: 0.7596\n",
            "Epoch 44/50\n",
            "CLF Train loss: 0.0601, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.5638, Validation Accuracy: 0.7452\n",
            "-----------------------------\n",
            "Fold 7/10\n",
            "{'accuracy': 0.7368, 'senstivity': 0.7838, 'specificity': 0.7069, 'loss': 0.4956}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 769.6414\n",
            "AE Val loss: 704.9133\n",
            "Epoch 2/50\n",
            "AE Train loss: 629.1948\n",
            "AE Val loss: 646.8369\n",
            "Epoch 3/50\n",
            "AE Train loss: 557.5796\n",
            "AE Val loss: 610.9620\n",
            "Epoch 4/50\n",
            "AE Train loss: 499.3588\n",
            "AE Val loss: 584.7881\n",
            "Epoch 5/50\n",
            "AE Train loss: 451.7967\n",
            "AE Val loss: 569.3898\n",
            "Epoch 6/50\n",
            "AE Train loss: 411.6730\n",
            "AE Val loss: 570.6874\n",
            "Epoch 7/50\n",
            "AE Train loss: 376.7675\n",
            "AE Val loss: 548.4715\n",
            "Epoch 8/50\n",
            "AE Train loss: 339.7882\n",
            "AE Val loss: 538.3506\n",
            "Epoch 9/50\n",
            "AE Train loss: 312.5108\n",
            "AE Val loss: 532.0544\n",
            "Epoch 10/50\n",
            "AE Train loss: 285.5678\n",
            "AE Val loss: 530.3980\n",
            "Epoch 11/50\n",
            "AE Train loss: 260.0620\n",
            "AE Val loss: 526.1783\n",
            "Epoch 12/50\n",
            "AE Train loss: 239.7852\n",
            "AE Val loss: 520.1050\n",
            "Epoch 13/50\n",
            "AE Train loss: 220.6800\n",
            "AE Val loss: 512.5427\n",
            "Epoch 14/50\n",
            "AE Train loss: 206.0393\n",
            "AE Val loss: 515.6276\n",
            "Epoch 15/50\n",
            "AE Train loss: 191.3453\n",
            "AE Val loss: 506.8268\n",
            "Epoch 16/50\n",
            "AE Train loss: 177.2252\n",
            "AE Val loss: 503.8146\n",
            "Epoch 17/50\n",
            "AE Train loss: 164.6223\n",
            "AE Val loss: 504.4354\n",
            "Epoch 18/50\n",
            "AE Train loss: 154.5031\n",
            "AE Val loss: 503.1033\n",
            "Epoch 19/50\n",
            "AE Train loss: 145.4735\n",
            "AE Val loss: 498.9760\n",
            "Epoch 20/50\n",
            "AE Train loss: 136.0175\n",
            "AE Val loss: 496.1147\n",
            "Epoch 21/50\n",
            "AE Train loss: 126.6449\n",
            "AE Val loss: 494.1300\n",
            "Epoch 22/50\n",
            "AE Train loss: 118.8467\n",
            "AE Val loss: 491.1389\n",
            "Epoch 23/50\n",
            "AE Train loss: 111.9207\n",
            "AE Val loss: 492.8143\n",
            "Epoch 24/50\n",
            "AE Train loss: 104.1904\n",
            "AE Val loss: 491.4298\n",
            "Epoch 25/50\n",
            "AE Train loss: 97.5028\n",
            "AE Val loss: 490.0777\n",
            "Epoch 26/50\n",
            "AE Train loss: 92.6319\n",
            "AE Val loss: 489.2155\n",
            "Epoch 27/50\n",
            "AE Train loss: 88.1678\n",
            "AE Val loss: 486.8380\n",
            "Epoch 28/50\n",
            "AE Train loss: 83.2838\n",
            "AE Val loss: 486.7047\n",
            "Epoch 29/50\n",
            "AE Train loss: 78.9636\n",
            "AE Val loss: 485.0586\n",
            "Epoch 30/50\n",
            "AE Train loss: 74.0943\n",
            "AE Val loss: 483.0900\n",
            "Epoch 31/50\n",
            "AE Train loss: 69.9035\n",
            "AE Val loss: 485.4936\n",
            "Epoch 32/50\n",
            "AE Train loss: 66.2151\n",
            "AE Val loss: 485.7342\n",
            "Epoch 33/50\n",
            "AE Train loss: 63.7280\n",
            "AE Val loss: 481.9427\n",
            "Epoch 34/50\n",
            "AE Train loss: 61.1495\n",
            "AE Val loss: 482.9837\n",
            "Epoch 35/50\n",
            "AE Train loss: 57.9166\n",
            "AE Val loss: 482.9590\n",
            "Epoch 36/50\n",
            "AE Train loss: 54.4145\n",
            "AE Val loss: 480.0139\n",
            "Epoch 37/50\n",
            "AE Train loss: 52.5046\n",
            "AE Val loss: 479.7920\n",
            "Epoch 38/50\n",
            "AE Train loss: 50.9806\n",
            "AE Val loss: 481.2618\n",
            "Epoch 39/50\n",
            "AE Train loss: 48.9188\n",
            "AE Val loss: 480.2145\n",
            "Epoch 40/50\n",
            "AE Train loss: 46.9622\n",
            "AE Val loss: 479.2092\n",
            "Epoch 41/50\n",
            "AE Train loss: 45.4406\n",
            "AE Val loss: 478.2721\n",
            "Epoch 42/50\n",
            "AE Train loss: 43.4543\n",
            "AE Val loss: 480.2964\n",
            "Epoch 43/50\n",
            "AE Train loss: 42.3424\n",
            "AE Val loss: 478.8745\n",
            "Epoch 44/50\n",
            "AE Train loss: 40.7444\n",
            "AE Val loss: 481.2618\n",
            "Epoch 45/50\n",
            "AE Train loss: 39.9270\n",
            "AE Val loss: 478.8752\n",
            "Epoch 46/50\n",
            "AE Train loss: 38.1303\n",
            "AE Val loss: 478.1945\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.7062, Train Accuracy: 0.4969\n",
            "CLF Val loss: 0.6673, Validation Accuracy: 0.5481\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6588, Train Accuracy: 0.5922\n",
            "CLF Val loss: 0.6363, Validation Accuracy: 0.6058\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6305, Train Accuracy: 0.6438\n",
            "CLF Val loss: 0.6146, Validation Accuracy: 0.6394\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.6097, Train Accuracy: 0.6656\n",
            "CLF Val loss: 0.5958, Validation Accuracy: 0.6538\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5718, Train Accuracy: 0.7344\n",
            "CLF Val loss: 0.5812, Validation Accuracy: 0.6538\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5550, Train Accuracy: 0.7359\n",
            "CLF Val loss: 0.5688, Validation Accuracy: 0.6875\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5321, Train Accuracy: 0.7859\n",
            "CLF Val loss: 0.5571, Validation Accuracy: 0.7115\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5152, Train Accuracy: 0.8000\n",
            "CLF Val loss: 0.5459, Validation Accuracy: 0.7067\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.4933, Train Accuracy: 0.8187\n",
            "CLF Val loss: 0.5357, Validation Accuracy: 0.7356\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4696, Train Accuracy: 0.8438\n",
            "CLF Val loss: 0.5264, Validation Accuracy: 0.7308\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4523, Train Accuracy: 0.8547\n",
            "CLF Val loss: 0.5168, Validation Accuracy: 0.7356\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4326, Train Accuracy: 0.8750\n",
            "CLF Val loss: 0.5089, Validation Accuracy: 0.7404\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4122, Train Accuracy: 0.8797\n",
            "CLF Val loss: 0.5025, Validation Accuracy: 0.7404\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.3952, Train Accuracy: 0.8922\n",
            "CLF Val loss: 0.4949, Validation Accuracy: 0.7452\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3714, Train Accuracy: 0.8922\n",
            "CLF Val loss: 0.4873, Validation Accuracy: 0.7500\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3580, Train Accuracy: 0.9109\n",
            "CLF Val loss: 0.4802, Validation Accuracy: 0.7596\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3382, Train Accuracy: 0.9187\n",
            "CLF Val loss: 0.4746, Validation Accuracy: 0.7644\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3137, Train Accuracy: 0.9234\n",
            "CLF Val loss: 0.4740, Validation Accuracy: 0.7548\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.2947, Train Accuracy: 0.9281\n",
            "CLF Val loss: 0.4768, Validation Accuracy: 0.7548\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2799, Train Accuracy: 0.9328\n",
            "CLF Val loss: 0.4621, Validation Accuracy: 0.7692\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2592, Train Accuracy: 0.9531\n",
            "CLF Val loss: 0.4619, Validation Accuracy: 0.7644\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2386, Train Accuracy: 0.9672\n",
            "CLF Val loss: 0.4591, Validation Accuracy: 0.7596\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2204, Train Accuracy: 0.9703\n",
            "CLF Val loss: 0.4539, Validation Accuracy: 0.7837\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.2049, Train Accuracy: 0.9828\n",
            "CLF Val loss: 0.4563, Validation Accuracy: 0.7596\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.1903, Train Accuracy: 0.9812\n",
            "CLF Val loss: 0.4601, Validation Accuracy: 0.7500\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1757, Train Accuracy: 0.9891\n",
            "CLF Val loss: 0.4521, Validation Accuracy: 0.7837\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1601, Train Accuracy: 0.9922\n",
            "CLF Val loss: 0.4531, Validation Accuracy: 0.7837\n",
            "Epoch 28/50\n",
            "CLF Train loss: 0.1472, Train Accuracy: 0.9969\n",
            "CLF Val loss: 0.4598, Validation Accuracy: 0.7837\n",
            "Epoch 29/50\n",
            "CLF Train loss: 0.1377, Train Accuracy: 0.9984\n",
            "CLF Val loss: 0.4572, Validation Accuracy: 0.7692\n",
            "Epoch 30/50\n",
            "CLF Train loss: 0.1228, Train Accuracy: 0.9984\n",
            "CLF Val loss: 0.4598, Validation Accuracy: 0.7740\n",
            "Epoch 31/50\n",
            "CLF Train loss: 0.1157, Train Accuracy: 0.9984\n",
            "CLF Val loss: 0.4551, Validation Accuracy: 0.7837\n",
            "Epoch 32/50\n",
            "CLF Train loss: 0.1076, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4577, Validation Accuracy: 0.7933\n",
            "Epoch 33/50\n",
            "CLF Train loss: 0.1000, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4629, Validation Accuracy: 0.7644\n",
            "Epoch 34/50\n",
            "CLF Train loss: 0.0934, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4614, Validation Accuracy: 0.7837\n",
            "Epoch 35/50\n",
            "CLF Train loss: 0.0893, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4637, Validation Accuracy: 0.7740\n",
            "Epoch 36/50\n",
            "CLF Train loss: 0.0817, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4655, Validation Accuracy: 0.7740\n",
            "Epoch 37/50\n",
            "CLF Train loss: 0.0783, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4662, Validation Accuracy: 0.7692\n",
            "Epoch 38/50\n",
            "CLF Train loss: 0.0748, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4728, Validation Accuracy: 0.7692\n",
            "Epoch 39/50\n",
            "CLF Train loss: 0.0709, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4731, Validation Accuracy: 0.7548\n",
            "Epoch 40/50\n",
            "CLF Train loss: 0.0665, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4773, Validation Accuracy: 0.7596\n",
            "Epoch 41/50\n",
            "CLF Train loss: 0.0652, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4743, Validation Accuracy: 0.7692\n",
            "Epoch 42/50\n",
            "CLF Train loss: 0.0620, Train Accuracy: 1.0000\n",
            "CLF Val loss: 0.4927, Validation Accuracy: 0.7644\n",
            "-----------------------------\n",
            "Fold 8/10\n",
            "{'accuracy': 0.7158, 'senstivity': 0.5641, 'specificity': 0.8214, 'loss': 0.6216}\n",
            "-----------------------------\n",
            "Number of train samples :  640\n",
            "Number of val samples :  214\n",
            "Number of test samples :  95\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 777.9943\n",
            "AE Val loss: 710.8951\n",
            "Epoch 2/50\n",
            "AE Train loss: 632.6296\n",
            "AE Val loss: 650.0580\n",
            "Epoch 3/50\n",
            "AE Train loss: 557.3698\n",
            "AE Val loss: 618.1339\n",
            "Epoch 4/50\n",
            "AE Train loss: 500.2939\n",
            "AE Val loss: 595.6002\n",
            "Epoch 5/50\n",
            "AE Train loss: 452.4846\n",
            "AE Val loss: 585.0896\n",
            "Epoch 6/50\n",
            "AE Train loss: 412.5991\n",
            "AE Val loss: 568.8536\n",
            "Epoch 7/50\n",
            "AE Train loss: 377.0627\n",
            "AE Val loss: 557.5349\n",
            "Epoch 8/50\n",
            "AE Train loss: 344.2032\n",
            "AE Val loss: 550.3281\n",
            "Epoch 9/50\n",
            "AE Train loss: 316.5903\n",
            "AE Val loss: 543.3582\n",
            "Epoch 10/50\n",
            "AE Train loss: 288.7130\n",
            "AE Val loss: 537.7848\n",
            "Epoch 11/50\n",
            "AE Train loss: 262.9276\n",
            "AE Val loss: 529.5864\n",
            "Epoch 12/50\n",
            "AE Train loss: 241.6106\n",
            "AE Val loss: 528.9685\n",
            "Epoch 13/50\n",
            "AE Train loss: 222.1188\n",
            "AE Val loss: 521.6215\n",
            "Epoch 14/50\n",
            "AE Train loss: 205.0991\n",
            "AE Val loss: 516.9740\n",
            "Epoch 15/50\n",
            "AE Train loss: 193.8148\n",
            "AE Val loss: 519.3120\n",
            "Epoch 16/50\n",
            "AE Train loss: 180.3660\n",
            "AE Val loss: 528.6200\n",
            "Epoch 17/50\n",
            "AE Train loss: 168.7239\n",
            "AE Val loss: 513.2114\n",
            "Epoch 18/50\n",
            "AE Train loss: 156.0198\n",
            "AE Val loss: 511.3895\n",
            "Epoch 19/50\n",
            "AE Train loss: 145.4043\n",
            "AE Val loss: 505.4810\n",
            "Epoch 20/50\n",
            "AE Train loss: 134.4069\n",
            "AE Val loss: 504.1051\n",
            "Epoch 21/50\n",
            "AE Train loss: 124.5143\n",
            "AE Val loss: 499.3392\n",
            "Epoch 22/50\n",
            "AE Train loss: 116.8774\n",
            "AE Val loss: 502.4354\n",
            "Epoch 23/50\n",
            "AE Train loss: 111.5083\n",
            "AE Val loss: 501.9992\n",
            "Epoch 24/50\n",
            "AE Train loss: 105.5167\n",
            "AE Val loss: 497.4417\n",
            "Epoch 25/50\n",
            "AE Train loss: 99.0781\n",
            "AE Val loss: 498.2523\n",
            "Epoch 26/50\n",
            "AE Train loss: 93.3639\n",
            "AE Val loss: 495.7692\n",
            "Epoch 27/50\n",
            "AE Train loss: 88.9860\n",
            "AE Val loss: 497.8299\n",
            "Epoch 28/50\n",
            "AE Train loss: 84.1747\n",
            "AE Val loss: 495.1010\n",
            "Epoch 29/50\n",
            "AE Train loss: 79.0856\n",
            "AE Val loss: 492.6912\n",
            "Epoch 30/50\n",
            "AE Train loss: 74.3725\n",
            "AE Val loss: 492.2684\n",
            "Epoch 31/50\n",
            "AE Train loss: 70.4494\n",
            "AE Val loss: 492.7807\n",
            "Epoch 32/50\n",
            "AE Train loss: 67.2251\n",
            "AE Val loss: 489.2583\n",
            "Epoch 33/50\n",
            "AE Train loss: 63.1391\n",
            "AE Val loss: 489.8726\n",
            "Epoch 34/50\n",
            "AE Train loss: 60.2302\n",
            "AE Val loss: 489.4861\n",
            "Epoch 35/50\n",
            "AE Train loss: 57.3595\n",
            "AE Val loss: 491.8237\n",
            "Epoch 36/50\n",
            "AE Train loss: 55.0404\n",
            "AE Val loss: 489.3745\n",
            "Epoch 37/50\n",
            "AE Train loss: 52.1850\n",
            "AE Val loss: 487.6162\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.6924, Train Accuracy: 0.5437\n",
            "CLF Val loss: 0.6359, Validation Accuracy: 0.6683\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6625, Train Accuracy: 0.6000\n",
            "CLF Val loss: 0.6178, Validation Accuracy: 0.6779\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6248, Train Accuracy: 0.6469\n",
            "CLF Val loss: 0.5994, Validation Accuracy: 0.7019\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.6090, Train Accuracy: 0.6719\n",
            "CLF Val loss: 0.5816, Validation Accuracy: 0.7212\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5887, Train Accuracy: 0.7047\n",
            "CLF Val loss: 0.5651, Validation Accuracy: 0.7356\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5655, Train Accuracy: 0.7344\n",
            "CLF Val loss: 0.5610, Validation Accuracy: 0.7500\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5448, Train Accuracy: 0.7547\n",
            "CLF Val loss: 0.5478, Validation Accuracy: 0.7500\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5287, Train Accuracy: 0.7781\n",
            "CLF Val loss: 0.5375, Validation Accuracy: 0.7500\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.5077, Train Accuracy: 0.8078\n",
            "CLF Val loss: 0.5251, Validation Accuracy: 0.7452\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4914, Train Accuracy: 0.8125\n",
            "CLF Val loss: 0.5125, Validation Accuracy: 0.7452\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4683, Train Accuracy: 0.8438\n",
            "CLF Val loss: 0.5099, Validation Accuracy: 0.7596\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4495, Train Accuracy: 0.8500\n",
            "CLF Val loss: 0.5027, Validation Accuracy: 0.7596\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4293, Train Accuracy: 0.8547\n",
            "CLF Val loss: 0.4946, Validation Accuracy: 0.7644\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.4092, Train Accuracy: 0.8719\n",
            "CLF Val loss: 0.4860, Validation Accuracy: 0.7692\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3912, Train Accuracy: 0.8844\n",
            "CLF Val loss: 0.4775, Validation Accuracy: 0.7644\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3666, Train Accuracy: 0.9031\n",
            "CLF Val loss: 0.4727, Validation Accuracy: 0.7692\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3539, Train Accuracy: 0.8953\n",
            "CLF Val loss: 0.4680, Validation Accuracy: 0.7885\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3322, Train Accuracy: 0.9141\n",
            "CLF Val loss: 0.4711, Validation Accuracy: 0.7644\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.3071, Train Accuracy: 0.9328\n",
            "CLF Val loss: 0.4982, Validation Accuracy: 0.7404\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2918, Train Accuracy: 0.9328\n",
            "CLF Val loss: 0.4606, Validation Accuracy: 0.7788\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2688, Train Accuracy: 0.9531\n",
            "CLF Val loss: 0.4498, Validation Accuracy: 0.7837\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2510, Train Accuracy: 0.9641\n",
            "CLF Val loss: 0.4484, Validation Accuracy: 0.7885\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2326, Train Accuracy: 0.9703\n",
            "CLF Val loss: 0.4481, Validation Accuracy: 0.7788\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.2174, Train Accuracy: 0.9734\n",
            "CLF Val loss: 0.4487, Validation Accuracy: 0.7837\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.2001, Train Accuracy: 0.9734\n",
            "CLF Val loss: 0.4536, Validation Accuracy: 0.7548\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1839, Train Accuracy: 0.9859\n",
            "CLF Val loss: 0.4585, Validation Accuracy: 0.7500\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1658, Train Accuracy: 0.9922\n",
            "CLF Val loss: 0.4503, Validation Accuracy: 0.7644\n",
            "-----------------------------\n",
            "Fold 9/10\n",
            "{'accuracy': 0.7474, 'senstivity': 0.7021, 'specificity': 0.7917, 'loss': 0.5017}\n",
            "-----------------------------\n",
            "Number of train samples :  641\n",
            "Number of val samples :  214\n",
            "Number of test samples :  94\n",
            "AE Training Started-----------\n",
            "Epoch 1/50\n",
            "AE Train loss: 781.3690\n",
            "AE Val loss: 695.1436\n",
            "Epoch 2/50\n",
            "AE Train loss: 636.3771\n",
            "AE Val loss: 636.5565\n",
            "Epoch 3/50\n",
            "AE Train loss: 562.7034\n",
            "AE Val loss: 604.1760\n",
            "Epoch 4/50\n",
            "AE Train loss: 505.7738\n",
            "AE Val loss: 581.5900\n",
            "Epoch 5/50\n",
            "AE Train loss: 456.5980\n",
            "AE Val loss: 563.5700\n",
            "Epoch 6/50\n",
            "AE Train loss: 413.6982\n",
            "AE Val loss: 551.8798\n",
            "Epoch 7/50\n",
            "AE Train loss: 375.9680\n",
            "AE Val loss: 543.5784\n",
            "Epoch 8/50\n",
            "AE Train loss: 345.2828\n",
            "AE Val loss: 542.8622\n",
            "Epoch 9/50\n",
            "AE Train loss: 318.4270\n",
            "AE Val loss: 529.9095\n",
            "Epoch 10/50\n",
            "AE Train loss: 291.2469\n",
            "AE Val loss: 520.7413\n",
            "Epoch 11/50\n",
            "AE Train loss: 266.2280\n",
            "AE Val loss: 530.2748\n",
            "Epoch 12/50\n",
            "AE Train loss: 246.9030\n",
            "AE Val loss: 519.7630\n",
            "Epoch 13/50\n",
            "AE Train loss: 226.6211\n",
            "AE Val loss: 510.7447\n",
            "Epoch 14/50\n",
            "AE Train loss: 207.9798\n",
            "AE Val loss: 504.4253\n",
            "Epoch 15/50\n",
            "AE Train loss: 192.6105\n",
            "AE Val loss: 501.9661\n",
            "Epoch 16/50\n",
            "AE Train loss: 178.4946\n",
            "AE Val loss: 500.4758\n",
            "Epoch 17/50\n",
            "AE Train loss: 166.3961\n",
            "AE Val loss: 498.1704\n",
            "Epoch 18/50\n",
            "AE Train loss: 156.7651\n",
            "AE Val loss: 494.3824\n",
            "Epoch 19/50\n",
            "AE Train loss: 147.1003\n",
            "AE Val loss: 496.6627\n",
            "Epoch 20/50\n",
            "AE Train loss: 137.3347\n",
            "AE Val loss: 489.7361\n",
            "Epoch 21/50\n",
            "AE Train loss: 127.9869\n",
            "AE Val loss: 491.9509\n",
            "Epoch 22/50\n",
            "AE Train loss: 120.8960\n",
            "AE Val loss: 487.1083\n",
            "Epoch 23/50\n",
            "AE Train loss: 112.2866\n",
            "AE Val loss: 485.3082\n",
            "Epoch 24/50\n",
            "AE Train loss: 104.8164\n",
            "AE Val loss: 485.4399\n",
            "Epoch 25/50\n",
            "AE Train loss: 98.5858\n",
            "AE Val loss: 481.7038\n",
            "Epoch 26/50\n",
            "AE Train loss: 93.2086\n",
            "AE Val loss: 480.8037\n",
            "Epoch 27/50\n",
            "AE Train loss: 88.2883\n",
            "AE Val loss: 481.0722\n",
            "Epoch 28/50\n",
            "AE Train loss: 82.8200\n",
            "AE Val loss: 481.8391\n",
            "Epoch 29/50\n",
            "AE Train loss: 78.5989\n",
            "AE Val loss: 479.9873\n",
            "Epoch 30/50\n",
            "AE Train loss: 75.0037\n",
            "AE Val loss: 479.9012\n",
            "Epoch 31/50\n",
            "AE Train loss: 70.6964\n",
            "AE Val loss: 480.4788\n",
            "Epoch 32/50\n",
            "AE Train loss: 66.2014\n",
            "AE Val loss: 477.2160\n",
            "Epoch 33/50\n",
            "AE Train loss: 63.3112\n",
            "AE Val loss: 476.5410\n",
            "Epoch 34/50\n",
            "AE Train loss: 60.9001\n",
            "AE Val loss: 475.8273\n",
            "Epoch 35/50\n",
            "AE Train loss: 57.9495\n",
            "AE Val loss: 477.1210\n",
            "Epoch 36/50\n",
            "AE Train loss: 55.2182\n",
            "AE Val loss: 476.2502\n",
            "Epoch 37/50\n",
            "AE Train loss: 52.5553\n",
            "AE Val loss: 477.4445\n",
            "Epoch 38/50\n",
            "AE Train loss: 50.9466\n",
            "AE Val loss: 478.2134\n",
            "Epoch 39/50\n",
            "AE Train loss: 50.0445\n",
            "AE Val loss: 477.3035\n",
            "CLF Training Started-----------\n",
            "Epoch 1/50\n",
            "CLF Train loss: 0.6905, Train Accuracy: 0.5359\n",
            "CLF Val loss: 0.6584, Validation Accuracy: 0.6058\n",
            "Epoch 2/50\n",
            "CLF Train loss: 0.6573, Train Accuracy: 0.6078\n",
            "CLF Val loss: 0.6385, Validation Accuracy: 0.6538\n",
            "Epoch 3/50\n",
            "CLF Train loss: 0.6235, Train Accuracy: 0.6641\n",
            "CLF Val loss: 0.6246, Validation Accuracy: 0.6731\n",
            "Epoch 4/50\n",
            "CLF Train loss: 0.6066, Train Accuracy: 0.6937\n",
            "CLF Val loss: 0.6137, Validation Accuracy: 0.6731\n",
            "Epoch 5/50\n",
            "CLF Train loss: 0.5776, Train Accuracy: 0.7359\n",
            "CLF Val loss: 0.6026, Validation Accuracy: 0.6779\n",
            "Epoch 6/50\n",
            "CLF Train loss: 0.5639, Train Accuracy: 0.7484\n",
            "CLF Val loss: 0.5947, Validation Accuracy: 0.6923\n",
            "Epoch 7/50\n",
            "CLF Train loss: 0.5414, Train Accuracy: 0.7516\n",
            "CLF Val loss: 0.5854, Validation Accuracy: 0.6923\n",
            "Epoch 8/50\n",
            "CLF Train loss: 0.5219, Train Accuracy: 0.8063\n",
            "CLF Val loss: 0.5780, Validation Accuracy: 0.7019\n",
            "Epoch 9/50\n",
            "CLF Train loss: 0.5037, Train Accuracy: 0.8000\n",
            "CLF Val loss: 0.5701, Validation Accuracy: 0.7019\n",
            "Epoch 10/50\n",
            "CLF Train loss: 0.4777, Train Accuracy: 0.8328\n",
            "CLF Val loss: 0.5669, Validation Accuracy: 0.7019\n",
            "Epoch 11/50\n",
            "CLF Train loss: 0.4670, Train Accuracy: 0.8328\n",
            "CLF Val loss: 0.5643, Validation Accuracy: 0.6971\n",
            "Epoch 12/50\n",
            "CLF Train loss: 0.4433, Train Accuracy: 0.8625\n",
            "CLF Val loss: 0.5536, Validation Accuracy: 0.7115\n",
            "Epoch 13/50\n",
            "CLF Train loss: 0.4269, Train Accuracy: 0.8578\n",
            "CLF Val loss: 0.5484, Validation Accuracy: 0.7212\n",
            "Epoch 14/50\n",
            "CLF Train loss: 0.4057, Train Accuracy: 0.8750\n",
            "CLF Val loss: 0.5468, Validation Accuracy: 0.7115\n",
            "Epoch 15/50\n",
            "CLF Train loss: 0.3824, Train Accuracy: 0.8938\n",
            "CLF Val loss: 0.5409, Validation Accuracy: 0.7308\n",
            "Epoch 16/50\n",
            "CLF Train loss: 0.3591, Train Accuracy: 0.9094\n",
            "CLF Val loss: 0.5358, Validation Accuracy: 0.7260\n",
            "Epoch 17/50\n",
            "CLF Train loss: 0.3432, Train Accuracy: 0.9156\n",
            "CLF Val loss: 0.5343, Validation Accuracy: 0.7308\n",
            "Epoch 18/50\n",
            "CLF Train loss: 0.3231, Train Accuracy: 0.9328\n",
            "CLF Val loss: 0.5335, Validation Accuracy: 0.7163\n",
            "Epoch 19/50\n",
            "CLF Train loss: 0.3029, Train Accuracy: 0.9219\n",
            "CLF Val loss: 0.5383, Validation Accuracy: 0.7356\n",
            "Epoch 20/50\n",
            "CLF Train loss: 0.2811, Train Accuracy: 0.9500\n",
            "CLF Val loss: 0.5305, Validation Accuracy: 0.7067\n",
            "Epoch 21/50\n",
            "CLF Train loss: 0.2627, Train Accuracy: 0.9594\n",
            "CLF Val loss: 0.5342, Validation Accuracy: 0.7115\n",
            "Epoch 22/50\n",
            "CLF Train loss: 0.2442, Train Accuracy: 0.9563\n",
            "CLF Val loss: 0.5370, Validation Accuracy: 0.7163\n",
            "Epoch 23/50\n",
            "CLF Train loss: 0.2289, Train Accuracy: 0.9563\n",
            "CLF Val loss: 0.5480, Validation Accuracy: 0.7308\n",
            "Epoch 24/50\n",
            "CLF Train loss: 0.2171, Train Accuracy: 0.9625\n",
            "CLF Val loss: 0.5410, Validation Accuracy: 0.7163\n",
            "Epoch 25/50\n",
            "CLF Train loss: 0.1926, Train Accuracy: 0.9844\n",
            "CLF Val loss: 0.5414, Validation Accuracy: 0.7212\n",
            "Epoch 26/50\n",
            "CLF Train loss: 0.1799, Train Accuracy: 0.9781\n",
            "CLF Val loss: 0.5421, Validation Accuracy: 0.7260\n",
            "Epoch 27/50\n",
            "CLF Train loss: 0.1675, Train Accuracy: 0.9812\n",
            "CLF Val loss: 0.5470, Validation Accuracy: 0.7308\n",
            "Epoch 28/50\n",
            "CLF Train loss: 0.1533, Train Accuracy: 0.9938\n",
            "CLF Val loss: 0.5478, Validation Accuracy: 0.7260\n",
            "Epoch 29/50\n",
            "CLF Train loss: 0.1411, Train Accuracy: 0.9906\n",
            "CLF Val loss: 0.5514, Validation Accuracy: 0.7212\n",
            "-----------------------------\n",
            "Fold 10/10\n",
            "{'accuracy': 0.7766, 'senstivity': 0.6486, 'specificity': 0.8596, 'loss': 0.4563}\n",
            "-----------------------------\n",
            "*********************************\n",
            "Average Value after 10 Folds and repeats one 1------->\n",
            "Accuracy: 0.7387, Senstivity: 0.6567, Specificity: 0.8031, Loss: 0.5235000252723694\n",
            "*********************************\n",
            "Average Value after 1 Repeat:\n",
            "Accuracy: 0.7387, Senstivity: 0.6567, Specificity: 0.8031, Loss: 0.5235000252723694\n",
            "2524.53489112854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mOd94zEuJ8d"
      },
      "source": [
        "best_fold_model = MTAutoEncoder(tied = False, num_inputs = num_inpp, num_latent = n_lat, use_dropout = use_dropout)\n",
        "best_fold_model.load_state_dict(best_fold_weights)\n",
        "best_fold_model = best_fold_model.to('cpu')\n",
        "# best_fold_model = best_fold_model.to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic90EDwRzRIO"
      },
      "source": [
        "x_val, y_val = [], []\n",
        "for f in best_fold_val_samples :\n",
        "    x_val.append(all_corr[f][0])\n",
        "    y_val.append(all_corr[f][1])\n",
        "\n",
        "x_val = torch.tensor(x_val, dtype=torch.float)\n",
        "y_val = np.array(y_val, dtype='float32')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXEx7E2v4Lk9"
      },
      "source": [
        "probs = best_fold_model(x_val)\n",
        "probs = probs.detach().cpu().numpy()\n",
        "preds = np.round(probs)\n",
        "probs = np.squeeze(probs, 1)\n",
        "preds = np.squeeze(preds, 1)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha-0b8ALASVr",
        "outputId": "8328c263-404c-42b2-d390-02dab5a4586d"
      },
      "source": [
        "preds = np.round(probs)\n",
        "preds"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
              "       1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
              "       1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
              "       0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
              "       1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
              "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 1., 0., 1., 0., 1., 1., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHlt-i2I8Hwa",
        "outputId": "29dde515-fa01-40d9-d3b8-10933bf73141"
      },
      "source": [
        "print(probs.shape)\n",
        "print(preds.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(214,)\n",
            "(214,)\n",
            "(214,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY-mVtNN9AIi",
        "outputId": "596b17f6-ed11-43be-d5a2-642ac7abe6a4"
      },
      "source": [
        "acc = np.mean(y_val == preds)\n",
        "print('Accuracy : ', acc)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  0.719626168224299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "Hxa2_CMS8Fce",
        "outputId": "86a58e5d-f807-473b-b8b7-26034d8d9401"
      },
      "source": [
        "df = {'Caseids' : best_fold_val_samples, 'Targets' : y_val, 'Predictions' : preds, 'Probabilities' : probs}\n",
        "df = pd.DataFrame(df, columns = ['Caseids', 'Targets', 'Predictions', 'Probabilities'])\n",
        "df.to_csv('./Visualization/Vis.csv')\n",
        "df.head(15)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Caseids</th>\n",
              "      <th>Targets</th>\n",
              "      <th>Predictions</th>\n",
              "      <th>Probabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OHSU_0050171</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.306376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CMU_b_0050667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UM_1_0050307</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.400762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NYU_0050981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.127785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>USM_0050439</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.811906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NYU_0051054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.162538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NYU_0051069</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.606833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>UM_1_0050299</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.304586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>UM_1_0050379</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.438429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NYU_0051019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.055355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NYU_0051124</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.238721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>OHSU_0050169</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.360610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Trinity_0050241</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.377266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>UM_1_0050353</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.796338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SBL_0051563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Caseids  Targets  Predictions  Probabilities\n",
              "0      OHSU_0050171      1.0          0.0       0.306376\n",
              "1     CMU_b_0050667      0.0          0.0       0.103380\n",
              "2      UM_1_0050307      1.0          0.0       0.400762\n",
              "3       NYU_0050981      0.0          0.0       0.127785\n",
              "4       USM_0050439      1.0          1.0       0.811906\n",
              "5       NYU_0051054      0.0          0.0       0.162538\n",
              "6       NYU_0051069      0.0          1.0       0.606833\n",
              "7      UM_1_0050299      1.0          0.0       0.304586\n",
              "8      UM_1_0050379      0.0          0.0       0.438429\n",
              "9       NYU_0051019      0.0          0.0       0.055355\n",
              "10      NYU_0051124      0.0          0.0       0.238721\n",
              "11     OHSU_0050169      1.0          0.0       0.360610\n",
              "12  Trinity_0050241      1.0          0.0       0.377266\n",
              "13     UM_1_0050353      1.0          1.0       0.796338\n",
              "14      SBL_0051563      0.0          0.0       0.040370"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJJWrT7G3KtK"
      },
      "source": [
        "# asd_index = np.where(y_val == 1)\n",
        "# hc_index = np.where(y_val == 0)\n",
        "# x_asd, y_asd, prob_asd, pred_asd = x_val[asd_index], y_val[asd_index], probs[asd_index], preds[asd_index]\n",
        "# x_hc, y_hc, prob_hc, pred_hc = x_val[hc_index], y_val[hc_index], probs[hc_index], preds[hc_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7PHBtNY5Osi",
        "outputId": "4e2133b6-0c63-4a95-9bfd-cdb1f7ef516e"
      },
      "source": [
        "df_asd = df[df['Targets'] == 1]\n",
        "df_hc = df[df['Targets'] == 0]\n",
        "\n",
        "print('Number of ASD Samples : ', len(df_asd))\n",
        "print('Number of HC Samples : ', len(df_hc))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of ASD Samples :  97\n",
            "Number of HC Samples :  117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "qm-79db9AKnB",
        "outputId": "9b3238fa-ab72-4a58-c27f-9ccf9796161d"
      },
      "source": [
        "df_hc"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Caseids</th>\n",
              "      <th>Targets</th>\n",
              "      <th>Predictions</th>\n",
              "      <th>Probabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CMU_b_0050667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NYU_0050981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.127785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NYU_0051054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.162538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NYU_0051069</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.606833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>UM_1_0050379</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.438429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>Leuven_1_0050706</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.844226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>KKI_0050775</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.392256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>USM_0050445</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.873393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>MaxMun_b_0051325</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>Yale_0050620</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.464215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>117 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Caseids  Targets  Predictions  Probabilities\n",
              "1       CMU_b_0050667      0.0          0.0       0.103380\n",
              "3         NYU_0050981      0.0          0.0       0.127785\n",
              "5         NYU_0051054      0.0          0.0       0.162538\n",
              "6         NYU_0051069      0.0          1.0       0.606833\n",
              "8        UM_1_0050379      0.0          0.0       0.438429\n",
              "..                ...      ...          ...            ...\n",
              "204  Leuven_1_0050706      0.0          1.0       0.844226\n",
              "208       KKI_0050775      0.0          0.0       0.392256\n",
              "209       USM_0050445      0.0          1.0       0.873393\n",
              "211  MaxMun_b_0051325      0.0          0.0       0.491044\n",
              "212      Yale_0050620      0.0          0.0       0.464215\n",
              "\n",
              "[117 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "yit8ccnU5OvJ",
        "outputId": "0bc09673-5241-4d63-8b42-2c0547ab9bf7"
      },
      "source": [
        "df_hc_right = df_hc[df_hc['Predictions'] == 0]\n",
        "print('Length of correctly prediction HC Samples : ', len(df_hc_right))\n",
        "df_hc_right.head(15)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of correctly prediction HC Samples :  97\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Caseids</th>\n",
              "      <th>Targets</th>\n",
              "      <th>Predictions</th>\n",
              "      <th>Probabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CMU_b_0050667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NYU_0050981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.127785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NYU_0051054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.162538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>UM_1_0050379</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.438429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NYU_0051019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.055355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NYU_0051124</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.238721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SBL_0051563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pitt_0050052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>USM_0050532</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.186708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Caltech_0051459</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.233598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SDSU_0050197</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.177821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>NYU_0050957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>NYU_0051113</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.446438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Trinity_0051138</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.206442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>UCLA_2_0051311</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.396393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Caseids  Targets  Predictions  Probabilities\n",
              "1     CMU_b_0050667      0.0          0.0       0.103380\n",
              "3       NYU_0050981      0.0          0.0       0.127785\n",
              "5       NYU_0051054      0.0          0.0       0.162538\n",
              "8      UM_1_0050379      0.0          0.0       0.438429\n",
              "9       NYU_0051019      0.0          0.0       0.055355\n",
              "10      NYU_0051124      0.0          0.0       0.238721\n",
              "14      SBL_0051563      0.0          0.0       0.040370\n",
              "20     Pitt_0050052      0.0          0.0       0.069996\n",
              "21      USM_0050532      0.0          0.0       0.186708\n",
              "25  Caltech_0051459      0.0          0.0       0.233598\n",
              "27     SDSU_0050197      0.0          0.0       0.177821\n",
              "30      NYU_0050957      0.0          0.0       0.068905\n",
              "31      NYU_0051113      0.0          0.0       0.446438\n",
              "36  Trinity_0051138      0.0          0.0       0.206442\n",
              "39   UCLA_2_0051311      0.0          0.0       0.396393"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "5c9LWGgI5O0t",
        "outputId": "f04d8992-7d23-4ea3-a779-6582864e02c4"
      },
      "source": [
        "df_asd_right = df_asd[df_asd['Predictions'] == 1]\n",
        "print('Length of correctly prediction ASD Samples : ', len(df_asd_right))\n",
        "df_asd_right.head(15)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of correctly prediction ASD Samples :  57\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Caseids</th>\n",
              "      <th>Targets</th>\n",
              "      <th>Predictions</th>\n",
              "      <th>Probabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>USM_0050439</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.811906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>UM_1_0050353</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.796338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>UM_1_0050305</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.994593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>UCLA_1_0051258</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.689850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>CMU_a_0050646</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.788661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>UM_1_0050301</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.597305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>UCLA_1_0051204</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.768830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NYU_0050964</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.911509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Pitt_0050044</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.580280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>UCLA_1_0051253</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.856459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>KKI_0050787</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.849283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>UM_1_0050337</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.801733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Yale_0050571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.764731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>NYU_0051007</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.953106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>NYU_0051024</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.890802</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Caseids  Targets  Predictions  Probabilities\n",
              "4      USM_0050439      1.0          1.0       0.811906\n",
              "13    UM_1_0050353      1.0          1.0       0.796338\n",
              "16    UM_1_0050305      1.0          1.0       0.994593\n",
              "18  UCLA_1_0051258      1.0          1.0       0.689850\n",
              "19   CMU_a_0050646      1.0          1.0       0.788661\n",
              "22    UM_1_0050301      1.0          1.0       0.597305\n",
              "23  UCLA_1_0051204      1.0          1.0       0.768830\n",
              "24     NYU_0050964      1.0          1.0       0.911509\n",
              "34    Pitt_0050044      1.0          1.0       0.580280\n",
              "38  UCLA_1_0051253      1.0          1.0       0.856459\n",
              "48     KKI_0050787      1.0          1.0       0.849283\n",
              "51    UM_1_0050337      1.0          1.0       0.801733\n",
              "53    Yale_0050571      1.0          1.0       0.764731\n",
              "54     NYU_0051007      1.0          1.0       0.953106\n",
              "64     NYU_0051024      1.0          1.0       0.890802"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sTMnJCOCH-o",
        "outputId": "26ddfa19-7e6d-422f-aaf9-fdbb5fd75072"
      },
      "source": [
        "df_asd_right_top = df_asd_right[df_asd_right['Probabilities'] > 0.8]\n",
        "df_hc_right_top = df_hc_right[df_hc_right['Probabilities'] < 0.2]\n",
        "\n",
        "print('Number of top ASD samples : ', len(df_asd_right_top))\n",
        "print('Number of top HC samples : ', len(df_hc_right_top))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of top ASD samples :  30\n",
            "Number of top HC samples :  61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acZ6glb0Chr8"
      },
      "source": [
        "x_asd_samples = df_asd_right_top['Caseids']\n",
        "x_hc_samples = df_hc_right_top['Caseids']"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRxIPRC7C-lC"
      },
      "source": [
        "x_asd_ig, y_asd_ig = [], []\n",
        "for sample in x_asd_samples : \n",
        "    x_asd_ig.append(all_corr[sample][0])\n",
        "    y_asd_ig.append(all_corr[sample][1])\n",
        "\n",
        "x_asd_ig = torch.tensor(x_asd_ig, dtype=torch.float)\n",
        "y_asd_ig = torch.tensor(y_asd_ig, dtype=torch.float)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grpZ4vnZC-sv"
      },
      "source": [
        "x_hc_ig, y_hc_ig = [], []\n",
        "for sample in x_hc_samples : \n",
        "    x_hc_ig.append(all_corr[sample][0])\n",
        "    y_hc_ig.append(all_corr[sample][1])\n",
        "\n",
        "x_hc_ig = torch.tensor(x_hc_ig, dtype=torch.float)\n",
        "y_hc_ig = torch.tensor(y_hc_ig, dtype=torch.float)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adEuyYqLD_Uc",
        "outputId": "70bdbec8-f1f4-4cde-f7b3-10d875ce7cf4"
      },
      "source": [
        "print('Lenghts : ')\n",
        "print(len(x_asd_ig))\n",
        "print(len(x_hc_ig))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lenghts : \n",
            "30\n",
            "61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUsCPS3tzRKj",
        "outputId": "69a83a56-2f9d-4a09-be89-042c60b29bd0"
      },
      "source": [
        "# best_fold_model = best_fold_model.to('cpu')\n",
        "# x_test = torch.tensor(x_test, dtype=torch.float)\n",
        "\n",
        "ig_asd = IntegratedGradients(best_fold_model)        \n",
        "grads_asd, delta_asd = attribute_image_features(ig_asd, inputs = x_asd_ig)         \n",
        "\n",
        "# saliency_asd = Saliency(best_fold_model)\n",
        "# grads_asd = saliency_asd.attribute(inputs = x_asd_ig, target = 0)\n",
        "# grads_asd = grads_asd.squeeze().cpu().detach().numpy()\n",
        "# grads_asd = attr_ig_asd + grads_asd\n",
        "# grads_asd = attr_ig_asd\n",
        "grads_asd = torch.mean(grads_asd, axis = 0)\n",
        "grads_asd = np.array(grads_asd)\n",
        "print(f'Attributions of Fold {kk + 1} : ', grads_asd.shape)\n",
        "# attributions.append(grads_asd) "
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attributions of Fold 10 :  (19900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFo1x3SK3HyI",
        "outputId": "30ffd637-8849-42a7-8c7e-5203b3c3930a"
      },
      "source": [
        "# best_fold_model = best_fold_model.to('cpu')\n",
        "# x_test = torch.tensor(x_test, dtype=torch.float)\n",
        "\n",
        "ig_hc = IntegratedGradients(best_fold_model)        \n",
        "grads_hc, delta_hc = attribute_image_features(ig_hc, inputs = x_hc_ig)         \n",
        "\n",
        "# saliency_hc = Saliency(best_fold_model)\n",
        "# grads_hc = saliency_hc.attribute(inputs = x_hc_ig, target = 0)\n",
        "# grads_hc = grads_hc.squeeze().cpu().detach().numpy()\n",
        "# grads_hc = attr_ig_hc + grads_hc\n",
        "# grads_hc = attr_ig_hc\n",
        "grads_hc= torch.mean(grads_hc, axis = 0)\n",
        "grads_hc = np.array(grads_hc)\n",
        "print(f'Attributions of Best Fold : ', grads_hc.shape)\n",
        "# attributions.append(grads_asd) "
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attributions of Best Fold :  (19900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrQNdFUAzRMo"
      },
      "source": [
        "np.save('./Visualization/Attributions_ASD.npy', grads_asd)\n",
        "np.save('./Visualization/Attributions_HC.npy', grads_hc)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e75UjDvDzRPd",
        "outputId": "c56791a5-1483-40db-cfbf-e8e2f39204fa"
      },
      "source": [
        "print(np.max(grads_asd), np.min(grads_asd))\n",
        "print(np.max(grads_hc), np.min(grads_hc))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0022270672538877417 -0.001607660601168136\n",
            "0.0018908362994820575 -0.0020417672413045903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQTnTfXwBMBM"
      },
      "source": [
        "def imshow(img):\n",
        "    plt.imshow(np.transpose(img))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhIWQ_r2HTHy"
      },
      "source": [
        "### ROIS responsible in ASD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Ap_DruTSoGnc",
        "outputId": "92689cd4-ca47-4cef-ccb1-6716fdc45b26"
      },
      "source": [
        "attr_vals_asd = grads_asd.copy()\n",
        "# attr_vals_asd = np.where(attr_vals_asd > 0, 1  , 0)\n",
        "# attr_vals_asd = (attr_vals_asd - np.min(attr_vals_asd)) / (np.max(attr_vals_asd) - np.min(attr_vals_asd))\n",
        "# attr_vals_asd = attr_vals_asd / np.max(attr_vals_asd)\n",
        "thresh = np.percentile(attr_vals_asd, 99)\n",
        "attr_vals_asd = np.where(attr_vals_asd > thresh,  1 , 0)\n",
        "corr_matrix_asd = np.zeros((200,200))\n",
        "corr_matrix_asd[np.triu_indices(200, 1)] = attr_vals_asd\n",
        "print('Number of unique elements in corr_matrix : ', np.unique(corr_matrix_asd, return_counts=True))\n",
        "max_sum_rows_asd = np.sum(corr_matrix_asd, axis = 1)\n",
        "max_indices_sort_asd = np.argsort(max_sum_rows_asd)\n",
        "max_20_indices_asd = max_indices_sort_asd[-20 : ]\n",
        "print('Most repeated ROIS in ASD : ', max_20_indices_asd)\n",
        "max_val_sort_asd = np.sort(max_sum_rows_asd)\n",
        "max_20_values_asd = max_val_sort_asd[-20 : ]\n",
        "print('Number of times ROIS repeated in ASD : ', max_20_values_asd)\n",
        "plt.matshow(corr_matrix_asd)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique elements in corr_matrix :  (array([0., 1.]), array([39801,   199]))\n",
            "Most repeated ROIS in ASD :  [ 64  12  45 125  14  93  27  54  58 105 106  16  56  70  20  48  31  29\n",
            "  52  63]\n",
            "Number of times ROIS repeated in ASD :  [ 3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.  4.  4.  5.  5.  5.  5.  5.\n",
            "  7. 14.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2cf4c83650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcJ9CagLHNnP"
      },
      "source": [
        "### ROIS responsible in HC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "LA-98M-SGU0_",
        "outputId": "f70e2a81-52ea-464f-cf91-74d755a31ee6"
      },
      "source": [
        "attr_vals_hc = grads_hc.copy()\n",
        "# attr_vals_hc = np.where(attr_vals_hc > 0, 1  , 0)\n",
        "# attr_vals_hc = (attr_vals_hc - np.min(attr_vals_hc)) / (np.max(attr_vals_hc) - np.min(attr_vals_hc))\n",
        "# attr_vals_hc = attr_vals_hc / np.max(attr_vals_hc)\n",
        "thresh = np.percentile(attr_vals_hc, 99)\n",
        "attr_vals_hc = np.where(attr_vals_hc > thresh,  1 , 0)\n",
        "corr_matrix_hc = np.zeros((200,200))\n",
        "corr_matrix_hc[np.triu_indices(200, 1)] = attr_vals_hc\n",
        "print('Number of unique elements in corr_matrix : ', np.unique(corr_matrix_hc, return_counts=True))\n",
        "max_sum_rows_hc = np.sum(corr_matrix_hc, axis = 1)\n",
        "max_indices_sort_hc = np.argsort(max_sum_rows_hc)\n",
        "max_20_indices_hc = max_indices_sort_hc[-20 : ]\n",
        "print('Most repeated ROIS in HC : ', max_20_indices_hc)\n",
        "max_val_sort_hc = np.sort(max_sum_rows_hc)\n",
        "max_20_values_hc = max_val_sort_hc[-20 : ]\n",
        "print('Number of times ROIS repeated in HC : ',max_20_values_hc)\n",
        "plt.matshow(corr_matrix_hc)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique elements in corr_matrix :  (array([0., 1.]), array([39801,   199]))\n",
            "Most repeated ROIS in HC :  [125  85  83  35 153  20  12 105 127  27  58  56   3  31  48 106  54  29\n",
            "  16  63]\n",
            "Number of times ROIS repeated in HC :  [3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 4. 4. 5. 5. 5. 5. 6. 7. 7. 8.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2cf4c5cc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAECCAYAAADzZhIUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOWklEQVR4nO3dX6jc5Z3H8c9n1WaxtDTBEuIfdqXEBQtykIP2QkqW7jbqTdqbonvRsAgpNN7XXrU3BSmUQmErRDaYXlTxRsyFNJXA4s22NYVDjAU1tCnmj2aL0IXKpmq/e3F+B8eTZ87M/P49v9/ze7/gMHN+M2fm+5yZ+czzPL9/jggBwHZ/l7sAAMNEOABIIhwAJBEOAJIIBwBJhAOApOzhYPtB22/YPm/7idz1tMH2Bduv2d6wfaZatsf2y7bfqi53565zWbaP275q+9zMsmR7vOkn1et51va9+Spfzpz2fd/2peo13LD98Mxt363a94btg3mq7l7WcLB9g6T/kPSQpLslPWr77pw1teifI2ItItar35+QdDoi9ks6Xf0+Fs9IenDbsnnteUjS/urniKSneqqxiWd0ffsk6cfVa7gWES9JUvX+fETSF6u/+Wn1Pi5O7p7DfZLOR8TvI+Kvkp6TdChzTV05JOlEdf2EpK9lrGUlEfGKpPe2LZ7XnkOSfhabfiXpc7b39VNpPXPaN88hSc9FxLWI+IOk89p8HxcndzjcJuntmd8vVsvGLiT90vZvbR+plu2NiCvV9Xck7c1TWmvmtaek1/Txamh0fGYYWFL7dpQ7HEr1QETcq80u9lHbX569MTa3WS9mu/XS2lN5StIXJK1JuiLpR3nL6V/ucLgk6Y6Z32+vlo1aRFyqLq9KekGb3c53t7rX1eXVfBW2Yl57inhNI+LdiPgoIv4m6Wl9PHQoon3LyB0Or0rab/tO25/S5kTPycw1NWL707Y/s3Vd0lclndNmuw5Xdzss6cU8FbZmXntOSvpmtdbiS5L+PDP8GI1t8yRf1+ZrKG227xHbu2zfqc2J19/0XV8fbsz55BHxoe3HJZ2SdIOk4xHxes6aWrBX0gu2pc3/788j4he2X5X0vO3HJP1R0jcy1rgS289KOiDpFtsXJX1P0pNKt+clSQ9rc6LufUn/3nvBK5rTvgO217Q5XLog6VuSFBGv235e0u8kfSjpaER8lKPurpldtgGk5B5WABgowgFAEuEAIIlwAJBEOABI6iwcVt3bcmYz4yKV3L6S2yaV3755OgmHmntblv4ClNy+ktsmld++pK56DlPa2xIoUldbSKb2XLt/3p0/5V3x97pZn/WeYrfIWrZ9d93zvt48e3MfJbWG1268/k9/0V/jmlO3Zdt8uhrHHZE2//kPfHygnWl7Tbo/+VJhy6nLGzp461ruMorw6zg997auhhUL91yLiGMRsR4R6zdpV0dlDN+pyxut3Kft58z5eIvUCYZTlzd6r3PsOtm3wvaNkt6U9BVthsKrkv5t3k5Vn/WeuN9fab2OsSjlm7CUdkzJr+O0/jfe629YUejelsCkDGKvzKn3HIBcduo5sIUkgCTCAZO3NVHJpOUnZT0SFDAEW5OoTKZ+Ej0HAEmEAzBgs8Ocvoc8hAOQwbIf9NmhTt/DHsKhhrYSvO9vglWej4m5bo1hfoNwAJDERlAt2vq2HcO3AiBl2Hx6qggFlIRhBYAkwqFFY97CLnftY53kXUXuNq76GjPnoPlzBeyCjNIx57DAvAAgGDBlDCsAJBEOaM2qG1m1MQav8xhDnpcYEsIBQBITksCEcSSoTOi+YswIhw6VsraDkJsmwgELlRJyWA3hACCJcMBcDCemjXDAXMsMJ3IcQKaL0CIIr8eqTGDCWJXZsjF9y7RZ605bNeY4IfCyxvR69e2ue96fexvhACCJYQV6wy7ww8OwokMlTcg1OYjIMgiGcaHnABRo2V4aPYfCMMGGRdropREOI0T3fNy6Hr61hXBoqOkLNtRewFDrKkHdcOd0eCPT9AUbai9giHURWP0iHDAaQwyskhEOAJIIB2COqQ9jCAdgjqkPYwgHAEmEA9CB3Oce3a5OLZwOr0XzzrmJ6Rnae6BOPfQcWnTw1rVPvAjLpvWQvmGALYRDh5ZN66F9y9RV6taiU0U4oDV1Q26IwzGCinAAMAfh0BO+ieYbUo9hyxBr6hvh0BPebBibRuFg+4Lt12xv2D5TLdtj+2Xbb1WXu9sptQyr9iDa6HF0tc59LI+JehodJs72BUnrEfGnmWU/lPReRDxp+wlJuyPiOzs9DoeJA/Lo+zBxhySdqK6fkPS1Dp4DQMeahkNI+qXt39o+Ui3bGxFXquvvSNqb+kPbR2yfsX3mA11rWAaAtjUNhwci4l5JD0k6avvLszfG5pglOW6JiGMRsR4R6zdpV8My0MSYxvljqnXsGoVDRFyqLq9KekHSfZLetb1PkqrLq02LRLfGtCZlTLWOXe1wsP1p25/Zui7pq5LOSTop6XB1t8OSXmxaJIarlG/yJu2Y/dvtj9PH/6er52iyV+ZeSS/Y3nqcn0fEL2y/Kul5249J+qOkbzQvE0DfOOMVMGGc8QpoYFG3va0N1drQ5hCDngMwYfQccJ0cZwfvQlsTibge4TBRq6wSHNrqw9kPdZPahtSuIQYV4YCV5X4jD+lD3ZYhtolwwMqG+EbG8pYNd8IBGJA+NqJaNtwJBwBJhMPIbN9Ut+43S655gz6ft86BddrYpqFJG7d/q+ccwrGdw0icurzBWB+tYzuHApQaDLnXfHRtzO0jHEZozG+47UoNvS1jbh/hMEJjfsMN0VbYDu3kt7kRDsgu9wdyK2y3n+t06ggHZMcHcpgIBzSW+5t/FWOqNTfCAUAS4YDGxjQsGFOtuREOyKqkbn5JbZEIB2S20zf52FYtLmrL2BAOqK3rN3xJqxbH2A7CAbWN8Q2P5REOQANjHC4si3AAGii590Q4YLDGNiFZmianwwM6VfK38hjQcwCQRDgASCIcACQRDigOk5jtIBxQnKYTmXWOWj1ETevi6NPAhHH0aSw0hm0K2qpv6O0cCsIBkoa9k9PWh7mt+rps51CCp406GFYAE8awAsDKCAcASYQDshvKOL1Udf+/hAOyG+pEaCnq/n8JByCB3gzhACTRmyEcgB1NuQdBOKCW3B+avp5/rD0INoIC0AgbQQFYGeEANJR7iLWTJrUtDAfbx21ftX1uZtke2y/bfqu63F0tt+2f2D5v+6zte2tXBozEkOclmtS2TM/hGUkPblv2hKTTEbFf0unqd0l6SNL+6ueIpKdqVwZgaV3scr/w0PQR8Yrtf9y2+JCkA9X1E5L+S9J3quU/i81Zzl/Z/pztfRFxpa2CAVyvi95L3TmHvTMf+Hck7a2u3ybp7Zn7XayWAVjSUOYwGk9IVr2EldeH2j5i+4ztMx/oWtMygGIMZQ6jbji8a3ufJFWXV6vllyTdMXO/26tl14mIYxGxHhHrN2lXzTIAdKVuOJyUdLi6fljSizPLv1mttfiSpD8z3wCM0zKrMp+V9N+S/sn2RduPSXpS0r/afkvSv1S/S9JLkn4v6bykpyV9u5OqgZHpex6hjedbZm3Fo3Nuum5752r+4WjTogDkxxaSyC7H7Hzfz7loknFePXXrbGNSk3BAL3Z6k3d9qPjUcy/7nLN/22WgzKsn55oL9soEJoy9MoEJatrTIRzQu5xbAA5l68M+NB2SEA7oXc5x9NZzTykk6iIcMElD2US5L3XCkHAAJqBOGBIOAJIIB6BwnA4PQBKnwwPQKsIBWMGUVoESDsAKprQKlHAAkEQ4AEgiHAAkEQ4AkggHAEmEA0arpNWKQ2wL4YDRSq1WHOKHbBlDXEVKOKAoQ/yQjRXhACQs2wPJ1VPp43kJB0zOog/WqcsbS/dAcvVU+nhewgGTc/DWtWyHyl+kbo9g3iH4mzwHh6YHGlqlpzGkx5Y4ND0mruvxeZcf3py9GMIBQBLhgOKxerMewgFAEuEAIIlwAJBEOKB4Y93fIjfCAcVjQrIewgFAEuEAIIlwAJBEOABIIhwAJBEOAJIIBwBJhAMwYXfd8/7c2wgHYMLePHvz3NsIBwBJhAOAJMIBQBLhACBpYTjYPm77qu1zM8u+b/uS7Y3q5+GZ275r+7ztN2wf7KpwoGtT39V7mZ7DM5IeTCz/cUSsVT8vSZLtuyU9IumL1d/81PYNbRUL9Gnqu3ovDIeIeEXSe0s+3iFJz0XEtYj4g6Tzku5rUB+ATJrMOTxu+2w17NhdLbtN0tsz97lYLQMwMnXD4SlJX5C0JumKpB+t+gC2j9g+Y/vMB7pWswwAO2kyb1IrHCLi3Yj4KCL+JulpfTx0uCTpjpm73l4tSz3GsYhYj4j1m7SrThkAFmgyb1IrHGzvm/n165K21mSclPSI7V2275S0X9JvalcHjMBO385DWONRt4YbF93B9rOSDki6xfZFSd+TdMD2mqSQdEHStyQpIl63/byk30n6UNLRiPioVmXASOz07Zy6reuT4y5TwzIWhkNEPJpY/J873P8Hkn5QqxoAg8EWkkDPxrL9BOEAIIlwAJBEOABIIhyAFg1h1WVbCAegRWOZbFwG4QAgiXAAkEQ4AEgiHICBWXVSs6tJUMIB6NEyH+RVJzXn7b/RFOEA9KivtRmzzzMvKBYFCOEAFG5eIC0KKsIBKEib8w+EA1CQNocthAOAJMIBQBLhACCJcACQRDgASCIcACQRDgCSCAcASYQDgCTCAUAS4QAgiXAAkEQ4AEgiHICJOHV5Y6VduheeZRtAGVbdnZueA4AkwgFAEuEAIIlwAJBEOAATtMxaC8IBmIDtYbDMmgvCAUAS4QBMQJ1D1hMOQKGanuCGcAAK1fQEN4QDgCTCAUAS4QAgiXAAkEQ4AEgiHAAkEQ4AkggHAEmEA4AkwgFAkiMidw2y/T+S/iLpT7lr6dAtKrd9JbdNKrt9/xARn0/dMIhwkCTbZyJiPXcdXSm5fSW3TSq/ffMwrACQRDgASBpSOBzLXUDHSm5fyW2Tym9f0mDmHAAMy5B6DgAGhHAAkEQ4AEgiHAAkEQ4Akv4fMm9U9O5BfncAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGvNJbGu4whg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlTblvnZ4wk7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bCF87rLZvP"
      },
      "source": [
        "ASD ROIS Importance in incresing order \\\n",
        "64  12  45 125  14  93  27  54  58 105 106  16  56  70  20  48  31  29  52  63"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQC_uTGCLcaY"
      },
      "source": [
        "HC ROIS Importance in incresing order \\\n",
        "125  85  83  35 153  20  12 105 127  27  58  56   3  31  48 106  54  29  16  63"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6rzw64tPTaU",
        "outputId": "fc52b993-bb8a-45fb-c3dc-cdfcc8a54362"
      },
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "FaAW-6NeNao8",
        "outputId": "729ea525-be63-4347-9f25-ecf8dd6bdccc"
      },
      "source": [
        "cc200_labels = pd.read_csv('./Visualization/CC200_ROI_labels.csv')\n",
        "cc200_labels.head(5)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ROI number</th>\n",
              "      <th>volume</th>\n",
              "      <th>center of mass</th>\n",
              "      <th>Dosenbach</th>\n",
              "      <th>AAL</th>\n",
              "      <th>Eickhoff-Zilles</th>\n",
              "      <th>Talairach-Tournoux</th>\n",
              "      <th>Harvard-Oxford</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>162</td>\n",
              "      <td>(-39.6;-85.4;1.4)</td>\n",
              "      <td>[\"None\": 0.91]</td>\n",
              "      <td>[\"Occipital_Mid_L\": 0.81][\"Occipital_Inf_L\": 0.19]</td>\n",
              "      <td>[\"Left Middle Occipital Gyrus\": 0.70][\"Left Inferior Occipital Gyrus\": 0.22]</td>\n",
              "      <td>[\"Left Middle Occipital Gyrus\": 0.69][\"Left Inferior Occipital Gyrus\": 0.31]</td>\n",
              "      <td>[\"Left Lateral Occipital Cortex; inferior division\": 0.87]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>269</td>\n",
              "      <td>(-49.7;-60.8;23.2)</td>\n",
              "      <td>[\"None\": 0.94]</td>\n",
              "      <td>[\"Angular_L\": 0.49][\"Temporal_Mid_L\": 0.45]</td>\n",
              "      <td>[\"Left Middle Temporal Gyrus\": 0.69][\"Left Angular Gyrus\": 0.22]</td>\n",
              "      <td>[\"Left Middle Temporal Gyrus\": 0.57][\"Left Superior Temporal Gyrus\": 0.32]</td>\n",
              "      <td>[\"Left Angular Gyrus\": 0.46][\"Left Lateral Occipital Cortex; superior division\": 0.39]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>284</td>\n",
              "      <td>(13.3;-64.4;24.0)</td>\n",
              "      <td>[\"None\": 0.93]</td>\n",
              "      <td>[\"Precuneus_R\": 0.47][\"Cuneus_R\": 0.31][\"Calcarine_R\": 0.22]</td>\n",
              "      <td>[\"Right Calcarine Gyrus\": 0.38][\"Right Precuneus\": 0.32][\"Right Cuneus\": 0.24]</td>\n",
              "      <td>[\"Right Precuneus\": 0.56][\"Right Posterior Cingulate\": 0.27][\"Right Cuneus\": 0.13]</td>\n",
              "      <td>[\"Right Precuneous Cortex\": 0.65][\"Right Cuneal Cortex\": 0.18]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>243</td>\n",
              "      <td>(-36.6;-13.9;-2.3)</td>\n",
              "      <td>[\"None\": 0.92]</td>\n",
              "      <td>[\"Temporal_Sup_L\": 0.33][\"Putamen_L\": 0.28][\"Insula_L\": 0.25]</td>\n",
              "      <td>[\"Left Superior Temporal Gyrus\": 0.39][\"Left Insula Lobe\": 0.23][\"Left Putamen\": 0.20]</td>\n",
              "      <td>[\"Left Insula\": 0.29][\"Left Lentiform Nucleus\": 0.23][\"Left Superior Temporal Gyrus\": 0.13][\"Left Claustrum\": 0.12][\"None\": 0.11]</td>\n",
              "      <td>[\"Left Putamen\": 0.33][\"Left Insular Cortex\": 0.30][\"Left Planum Polare\": 0.21][\"Left Heschl's Gyrus (includes H1 and H2)\": 0.11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>222</td>\n",
              "      <td>(-6.8;45.7;7.8)</td>\n",
              "      <td>[\"None\": 0.86]</td>\n",
              "      <td>[\"Cingulum_Ant_L\": 0.66][\"Frontal_Sup_Medial_L\": 0.28]</td>\n",
              "      <td>[\"Left Anterior Cingulate Cortex\": 0.59][\"Left Superior Medial Gyrus\": 0.22][\"Left Mid Orbital Gyrus\": 0.18]</td>\n",
              "      <td>[\"Left Medial Frontal Gyrus\": 0.59][\"Left Anterior Cingulate\": 0.41]</td>\n",
              "      <td>[\"Left Paracingulate Gyrus\": 0.62][\"Left Cingulate Gyrus; anterior division\": 0.20]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ROI number   volume       center of mass       Dosenbach  \\\n",
              "0  1           162       (-39.6;-85.4;1.4)   [\"None\": 0.91]   \n",
              "1  2           269       (-49.7;-60.8;23.2)  [\"None\": 0.94]   \n",
              "2  3           284       (13.3;-64.4;24.0)   [\"None\": 0.93]   \n",
              "3  4           243       (-36.6;-13.9;-2.3)  [\"None\": 0.92]   \n",
              "4  5           222       (-6.8;45.7;7.8)     [\"None\": 0.86]   \n",
              "\n",
              "                                                             AAL  \\\n",
              "0  [\"Occipital_Mid_L\": 0.81][\"Occipital_Inf_L\": 0.19]              \n",
              "1  [\"Angular_L\": 0.49][\"Temporal_Mid_L\": 0.45]                     \n",
              "2  [\"Precuneus_R\": 0.47][\"Cuneus_R\": 0.31][\"Calcarine_R\": 0.22]    \n",
              "3  [\"Temporal_Sup_L\": 0.33][\"Putamen_L\": 0.28][\"Insula_L\": 0.25]   \n",
              "4  [\"Cingulum_Ant_L\": 0.66][\"Frontal_Sup_Medial_L\": 0.28]          \n",
              "\n",
              "                                                                                                Eickhoff-Zilles  \\\n",
              "0  [\"Left Middle Occipital Gyrus\": 0.70][\"Left Inferior Occipital Gyrus\": 0.22]                                   \n",
              "1  [\"Left Middle Temporal Gyrus\": 0.69][\"Left Angular Gyrus\": 0.22]                                               \n",
              "2  [\"Right Calcarine Gyrus\": 0.38][\"Right Precuneus\": 0.32][\"Right Cuneus\": 0.24]                                 \n",
              "3  [\"Left Superior Temporal Gyrus\": 0.39][\"Left Insula Lobe\": 0.23][\"Left Putamen\": 0.20]                         \n",
              "4  [\"Left Anterior Cingulate Cortex\": 0.59][\"Left Superior Medial Gyrus\": 0.22][\"Left Mid Orbital Gyrus\": 0.18]   \n",
              "\n",
              "                                                                                                                  Talairach-Tournoux  \\\n",
              "0  [\"Left Middle Occipital Gyrus\": 0.69][\"Left Inferior Occipital Gyrus\": 0.31]                                                        \n",
              "1  [\"Left Middle Temporal Gyrus\": 0.57][\"Left Superior Temporal Gyrus\": 0.32]                                                          \n",
              "2  [\"Right Precuneus\": 0.56][\"Right Posterior Cingulate\": 0.27][\"Right Cuneus\": 0.13]                                                  \n",
              "3  [\"Left Insula\": 0.29][\"Left Lentiform Nucleus\": 0.23][\"Left Superior Temporal Gyrus\": 0.13][\"Left Claustrum\": 0.12][\"None\": 0.11]   \n",
              "4  [\"Left Medial Frontal Gyrus\": 0.59][\"Left Anterior Cingulate\": 0.41]                                                                \n",
              "\n",
              "                                                                                                                      Harvard-Oxford  \n",
              "0  [\"Left Lateral Occipital Cortex; inferior division\": 0.87]                                                                         \n",
              "1  [\"Left Angular Gyrus\": 0.46][\"Left Lateral Occipital Cortex; superior division\": 0.39]                                             \n",
              "2  [\"Right Precuneous Cortex\": 0.65][\"Right Cuneal Cortex\": 0.18]                                                                     \n",
              "3  [\"Left Putamen\": 0.33][\"Left Insular Cortex\": 0.30][\"Left Planum Polare\": 0.21][\"Left Heschl's Gyrus (includes H1 and H2)\": 0.11]  \n",
              "4  [\"Left Paracingulate Gyrus\": 0.62][\"Left Cingulate Gyrus; anterior division\": 0.20]                                                "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkE6jziUZBE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "59d08a00-8504-42b2-cd92-14cdea340bb3"
      },
      "source": [
        "print('ASD Associated Regions : ')\n",
        "asd_rois = cc200_labels[cc200_labels['ROI number'].isin(max_20_indices_asd[-5 : ])]\n",
        "display(asd_rois[['ROI number', 'Harvard-Oxford']])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ASD Associated Regions : \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ROI number</th>\n",
              "      <th>Harvard-Oxford</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>[\"Right Cingulate Gyrus; posterior division\": 0.52][\"Right Cingulate Gyrus; anterior division\": 0.22][\"Left Cingulate Gyrus; posterior division\": 0.17]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>[\"Right Superior Parietal Lobule\": 0.71][\"Right Lateral Occipital Cortex; superior division\": 0.29]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>48</td>\n",
              "      <td>[\"Right Thalamus\": 0.31][\"Right Hippocampus\": 0.29][\"Right Parahippocampal Gyrus; posterior division\": 0.16][\"Right Lingual Gyrus\": 0.13]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>52</td>\n",
              "      <td>[\"None\": 1.00]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>63</td>\n",
              "      <td>[\"Left Inferior Temporal Gyrus; temporooccipital part\": 0.42][\"Left Temporal Occipital Fusiform Cortex\": 0.38][\"Left Temporal Fusiform Cortex; posterior division\": 0.14]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ROI number  \\\n",
              "28  29           \n",
              "30  31           \n",
              "47  48           \n",
              "51  52           \n",
              "62  63           \n",
              "\n",
              "                                                                                                                                                               Harvard-Oxford  \n",
              "28  [\"Right Cingulate Gyrus; posterior division\": 0.52][\"Right Cingulate Gyrus; anterior division\": 0.22][\"Left Cingulate Gyrus; posterior division\": 0.17]                    \n",
              "30  [\"Right Superior Parietal Lobule\": 0.71][\"Right Lateral Occipital Cortex; superior division\": 0.29]                                                                        \n",
              "47  [\"Right Thalamus\": 0.31][\"Right Hippocampus\": 0.29][\"Right Parahippocampal Gyrus; posterior division\": 0.16][\"Right Lingual Gyrus\": 0.13]                                  \n",
              "51  [\"None\": 1.00]                                                                                                                                                             \n",
              "62  [\"Left Inferior Temporal Gyrus; temporooccipital part\": 0.42][\"Left Temporal Occipital Fusiform Cortex\": 0.38][\"Left Temporal Fusiform Cortex; posterior division\": 0.14]  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehvBdfzSZBH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5565fbe-206b-4af4-84cb-c46b88bddc13"
      },
      "source": [
        "print('HC Associated Regions : ')\n",
        "hc_rois = cc200_labels[cc200_labels['ROI number'].isin(max_20_indices_hc[-5 : ])]\n",
        "print(hc_rois[['ROI number', 'Harvard-Oxford']])"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HC Associated Regions : \n",
            "     ROI number  \\\n",
            "15   16           \n",
            "28   29           \n",
            "53   54           \n",
            "62   63           \n",
            "105  106          \n",
            "\n",
            "                                                                                                                                                                Harvard-Oxford  \n",
            "15   [\"Left Lateral Occipital Cortex; inferior division\": 0.72][\"Left Inferior Temporal Gyrus; temporooccipital part\": 0.15]                                                    \n",
            "28   [\"Right Cingulate Gyrus; posterior division\": 0.52][\"Right Cingulate Gyrus; anterior division\": 0.22][\"Left Cingulate Gyrus; posterior division\": 0.17]                    \n",
            "53   [\"Left Cuneal Cortex\": 0.30][\"Right Cuneal Cortex\": 0.25][\"Left Occipital Pole\": 0.22][\"Left Lateral Occipital Cortex; superior division\": 0.11]                           \n",
            "62   [\"Left Inferior Temporal Gyrus; temporooccipital part\": 0.42][\"Left Temporal Occipital Fusiform Cortex\": 0.38][\"Left Temporal Fusiform Cortex; posterior division\": 0.14]  \n",
            "105  [\"Right Frontal Pole\": 0.47][\"Right Middle Frontal Gyrus\": 0.42][\"Right Superior Frontal Gyrus\": 0.11]                                                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Is24rTZBLG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7gia9_mZBOH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO72XnHQUTRG"
      },
      "source": [
        "# Saving the results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqDzo2UDgAWK"
      },
      "source": [
        "asd diagnet data augmentation with 1 neighbor\n",
        "Accuracy: 0.7366, Senstivity: 0.6758, Specificity: 0.7849, Loss: 0.5751000046730042\n",
        "\n",
        "Similarity function : dot product between correlation vectors(not the eigen vectors)  [nearest neighbor]\n",
        "Accuracy: 0.744, Senstivity: 0.6778, Specificity: 0.7962, Loss: 0.557200014591217\n",
        "\n",
        "Similarity function : dot product between correlation vectors(not the eigen vectors)  [random of 5 neighbors]\n",
        "Accuracy: 0.7387, Senstivity: 0.6494, Specificity: 0.8094, Loss: 0.5447999835014343\n",
        "\n",
        "asd diagnet data augmentation\n",
        "Accuracy: 0.7608, Senstivity: 0.6971, Specificity: 0.8113, Loss: 0.5674999952316284\n",
        "\n",
        "shuffling the sequence of timeseries with split number = 2 and aug_factor = 2\n",
        "Accuracy: 0.7461, Senstivity: 0.68, Specificity: 0.7981, Loss: 0.5422999858856201\n",
        "\n",
        "shuffling the sequence of timeseries with split number = 3 and aug_factor = 3\n",
        "Accuracy: 0.7492, Senstivity: 0.6854, Specificity: 0.8, Loss: 0.5454000234603882\n",
        "\n",
        "shuffling the sequence of timeseries with split number = 3 and aug_factor = 4\n",
        "Accuracy: 0.7418, Senstivity: 0.666, Specificity: 0.8019, Loss: 0.5636000037193298"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:29:25.414869Z",
          "iopub.status.idle": "2021-06-15T17:29:25.415635Z"
        },
        "trusted": true,
        "id": "7w7fc7mthkn-"
      },
      "source": [
        "Accuracy: 0.6724, Senstivity: 0.61, Specificity: 0.7321, Loss: 0.6234999895095825\n",
        "589.3430550098419"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:29:25.417276Z",
          "iopub.status.idle": "2021-06-15T17:29:25.417975Z"
        },
        "trusted": true,
        "id": "IBmoEdIVhkn_"
      },
      "source": [
        "adam with weight decay 0.3 \n",
        "\n",
        "Accuracy: 0.657, Senstivity: 0.5431, Specificity: 0.766, Loss: 0.6509000062942505\n",
        "1160.326141834259\n",
        "\n",
        "\n",
        "adam w/o weight decay \n",
        "Accuracy: 0.7043, Senstivity: 0.6636, Specificity: 0.7434, Loss: 0.5964999794960022\n",
        "1056.0533220767975\n",
        "\n",
        "\n",
        "adam with weight decay 0.1 \n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7063, Senstivity: 0.6716, Specificity: 0.7396, Loss: 0.5821999907493591\n",
        "1162.2546133995056\n",
        "\n",
        "adam with weight decay 0.15 \n",
        "Accuracy: 0.686, Senstivity: 0.6598, Specificity: 0.7113, Loss: 0.5911999940872192\n",
        "1161.7236032485962\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25\n",
        "Accuracy: 0.6985, Senstivity: 0.6793, Specificity: 0.717, Loss: 0.5842000246047974\n",
        "1162.4763264656067\n",
        "\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25 n_lat 256\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.6908, Senstivity: 0.6933, Specificity: 0.6887, Loss: 0.5952000021934509\n",
        "1149.6250448226929"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jy8VBLfhkn_"
      },
      "source": [
        "AIMAFE Data\n",
        "\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25 n_lat 256\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7397, Senstivity: 0.6706, Specificity: 0.7944, Loss: 0.5264000296592712\n",
        "1056.9974591732025\n",
        "\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25 n_lat 512\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7471, Senstivity: 0.6923, Specificity: 0.7906, Loss: 0.5105000138282776\n",
        "1067.9337112903595\n",
        "\n",
        "adam with weight decay 0.2  and dropout 0.25 n_lat 512\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7324, Senstivity: 0.6256, Specificity: 0.817, Loss: 0.5450999736785889\n",
        "1068.7675378322601"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}