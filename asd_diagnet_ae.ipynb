{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "asd-diagnet-ae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khare19yash/Automated-Detection-of-Neuropsychiatric-Disorders/blob/master/asd_diagnet_ae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:31.186776Z",
          "iopub.execute_input": "2021-06-18T20:06:31.187107Z",
          "iopub.status.idle": "2021-06-18T20:06:31.831198Z",
          "shell.execute_reply.started": "2021-06-18T20:06:31.187075Z",
          "shell.execute_reply": "2021-06-18T20:06:31.830415Z"
        },
        "trusted": true,
        "id": "r2HULWPUhkny"
      },
      "source": [
        "!ls ../input/abide-cc200/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbcNNOwas0t2",
        "outputId": "708869d0-6d96-472e-9ada-cd480eeb65d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZFE-Yyss5RW",
        "outputId": "e90162bd-55f0-4db8-e3e7-d6a38d93c92c"
      },
      "source": [
        "cd gdrive/My Drive/IIITH_Internship/Neuro/Final"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/IIITH_Internship/Neuro/Final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:35.313833Z",
          "iopub.execute_input": "2021-06-18T20:06:35.314183Z",
          "iopub.status.idle": "2021-06-18T20:06:35.320917Z",
          "shell.execute_reply.started": "2021-06-18T20:06:35.31415Z",
          "shell.execute_reply": "2021-06-18T20:06:35.320107Z"
        },
        "trusted": true,
        "id": "Y0UrH07ohkn1"
      },
      "source": [
        "#options: cc200, dosenbach160, aal\n",
        "p_ROI = \"cc200\"\n",
        "p_fold = 10\n",
        "p_center = \"Stanford\"\n",
        "p_mode = \"whole\"\n",
        "p_augmentation = True\n",
        "p_Method = \"ASD-DiagNet\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:35.6938Z",
          "iopub.execute_input": "2021-06-18T20:06:35.694114Z",
          "iopub.status.idle": "2021-06-18T20:06:35.705229Z",
          "shell.execute_reply.started": "2021-06-18T20:06:35.694082Z",
          "shell.execute_reply": "2021-06-18T20:06:35.703223Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XvyvjLghkn2",
        "outputId": "b1052b04-45c7-4c7f-e0b8-31475e1f907c"
      },
      "source": [
        "parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\n",
        "print(\"*****List of patameters****\")\n",
        "print(\"ROI atlas: \",p_ROI)\n",
        "print(\"per Center or whole: \",p_mode)\n",
        "if p_mode == 'percenter':\n",
        "    print(\"Center's name: \",p_center)\n",
        "print(\"Method's name: \",p_Method)\n",
        "if p_Method == \"ASD-DiagNet\":\n",
        "    print(\"Augmentation: \",p_augmentation)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****List of patameters****\n",
            "ROI atlas:  cc200\n",
            "per Center or whole:  whole\n",
            "Method's name:  ASD-DiagNet\n",
            "Augmentation:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSeVPBEwtX_C",
        "outputId": "30f87983-5212-4676-8ef4-b6da3ad169c9"
      },
      "source": [
        "!pip install pyprind"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/b3/1f12ebc5009c65b607509393ad98240728b4401bc3593868fb161fdd3760/PyPrind-2.11.3-py2.py3-none-any.whl\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0L-wSF6tngd",
        "outputId": "184cae56-0697-4cf9-d6d9-397209de1e86"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b4/9d92953d8cddc8450c859be12e3dbdd4c7754fb8def94c28b3b351c6ee4e/wandb-0.10.32-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 4.3MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 27.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=bccacf31052dd4320013e6474c566bc21c2d4d7beecb48f587c80de4621c95b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=bdb9347df13d56806d61e4e73b69c0d405a8b3e7f8b1dc68fdc38eddceb848c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: sentry-sdk, smmap, gitdb, GitPython, shortuuid, pathtools, subprocess32, configparser, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:36.027823Z",
          "iopub.execute_input": "2021-06-18T20:06:36.028151Z",
          "iopub.status.idle": "2021-06-18T20:06:38.856661Z",
          "shell.execute_reply.started": "2021-06-18T20:06:36.028106Z",
          "shell.execute_reply": "2021-06-18T20:06:38.855715Z"
        },
        "trusted": true,
        "id": "5IcOJBWphkn3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from itertools import groupby\n",
        "import pyprind\n",
        "import wandb\n",
        "# !wandb login d164742a4a99e4e581f543102aff0992153ad225"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz3YnOOchkn3"
      },
      "source": [
        "## Importing the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.861206Z",
          "iopub.execute_input": "2021-06-18T20:06:38.861461Z",
          "iopub.status.idle": "2021-06-18T20:06:38.868954Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.861434Z",
          "shell.execute_reply": "2021-06-18T20:06:38.868167Z"
        },
        "trusted": true,
        "id": "PmnCD4dphkn4"
      },
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3]) \n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.87189Z",
          "iopub.execute_input": "2021-06-18T20:06:38.872152Z",
          "iopub.status.idle": "2021-06-18T20:06:38.923819Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.872107Z",
          "shell.execute_reply": "2021-06-18T20:06:38.922494Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBaX2AXuhkn4",
        "outputId": "ef425222-e159-4266-f553-48632c733c59"
      },
      "source": [
        "cc200_data_path = './data/filt_global/rois_cc200'#cc200'#path to time series data\n",
        "# flist = os.listdir(cc200_data_path)\n",
        "\n",
        "# print(flist[0])\n",
        "\n",
        "# fid = []\n",
        "# for f in flist:\n",
        "#     fid.append(get_key(f))\n",
        "\n",
        "    \n",
        "data_df = pd.read_csv('./data/Phenotypic_V1_0b_preprocessed949.csv',encoding= 'unicode_escape')#path \n",
        "# data_df = data_df[data_df['FILE_ID'].isin(fid)]\n",
        "data_df.DX_GROUP = data_df.DX_GROUP.map({1: 1, 2:0})\n",
        "data_df['FILE_PATH'] = data_df['FILE_ID'].apply(lambda x : os.path.join(cc200_data_path,x + '_rois_cc200.1D')) \n",
        "\n",
        "\n",
        "# print(data_df.head())\n",
        "print(len(data_df))\n",
        "print(data_df['FILE_PATH'].values[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "949\n",
            "./data/filt_global/rois_cc200/Pitt_0050003_rois_cc200.1D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7DgqnL6hkn4"
      },
      "source": [
        "### Helper functions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.925344Z",
          "iopub.execute_input": "2021-06-18T20:06:38.925672Z",
          "iopub.status.idle": "2021-06-18T20:06:38.942734Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.925638Z",
          "shell.execute_reply": "2021-06-18T20:06:38.941792Z"
        },
        "trusted": true,
        "id": "CrCnO-6yhkn5"
      },
      "source": [
        "def get_label(filename):\n",
        "    assert (filename in labels)\n",
        "    return labels[filename]\n",
        "\n",
        "\n",
        "def get_corr_data(df):\n",
        "              \n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        return ma.masked_where(m, corr).compressed()\n",
        "\n",
        "\n",
        "def get_corr_matrix(filename,data_path):\n",
        "    # returns correlation matrix\n",
        "    for file in os.listdir(data_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_path, file), sep='\\t')\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        return corr\n",
        "\n",
        "def confusion(g_turth,predictions):\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    sensitivity = (tp)/(tp+fn)\n",
        "    specificty = (tn)/(tn+fp)\n",
        "    return accuracy,sensitivity,specificty\n",
        "\n",
        "def get_regs(samplesnames,regnum):\n",
        "    # returns region index array\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        datas.append(all_corr[sn][0])\n",
        "    datas = np.array(datas)     # datas shape len(samplesnames)x19900\n",
        "    avg=[]\n",
        "    for ie in range(datas.shape[1]):\n",
        "        avg.append(np.mean(datas[:,ie]))\n",
        "    avg=np.array(avg)\n",
        "    highs=avg.argsort()[-regnum:][::-1]\n",
        "    lows=avg.argsort()[:regnum][::-1]\n",
        "    regions=np.concatenate((highs,lows),axis=0) # shape (9950,)\n",
        "    return regions"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:06:38.945281Z",
          "iopub.execute_input": "2021-06-18T20:06:38.94597Z",
          "iopub.status.idle": "2021-06-18T20:07:07.764978Z",
          "shell.execute_reply.started": "2021-06-18T20:06:38.945932Z",
          "shell.execute_reply": "2021-06-18T20:07:07.763126Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "mfjFNCivhkn6",
        "outputId": "03a02613-13f2-405b-a251-11465f93feb9"
      },
      "source": [
        "labels = data_df['DX_GROUP'].values\n",
        "fpaths = data_df['FILE_PATH']\n",
        "sfc = []\n",
        "# all_corr = {}\n",
        "\n",
        "for i,path in enumerate(fpaths):\n",
        "    corr_data = get_corr_data(path)\n",
        "    sfc.append(corr_data)\n",
        "#     all_corr[path] = (corr_data,labels[i])\n",
        "\n",
        "sfc = np.asarray(sfc)\n",
        "\n",
        "print(sfc.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f8a086c00d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcorr_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corr_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     all_corr[path] = (corr_data,labels[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6aa5fb1775b5>\u001b[0m in \u001b[0;36mget_corr_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_corr_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vHsYKAkhkn6"
      },
      "source": [
        "## Defining dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:07:07.76686Z",
          "iopub.execute_input": "2021-06-18T20:07:07.767238Z",
          "iopub.status.idle": "2021-06-18T20:07:07.774401Z",
          "shell.execute_reply.started": "2021-06-18T20:07:07.767191Z",
          "shell.execute_reply": "2021-06-18T20:07:07.773154Z"
        },
        "trusted": true,
        "id": "TbJYMT6ihkn7"
      },
      "source": [
        "class ASDDataset(Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        pass\n",
        "    def __getitem__(self,idx):\n",
        "        return torch.tensor(self.x[idx],dtype=torch.float),torch.tensor(self.y[idx],dtype=torch.float)\n",
        "        pass\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "        pass\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljRmIeypt09Y"
      },
      "source": [
        "window_size = 40\n",
        "stride = window_size\n",
        "f1 = sfc[:]\n",
        "X = [f1[i : i + window_size] for i in range(window_size, stride)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMskHE-Nqy5E"
      },
      "source": [
        "class CC200Dataset(Dataset):\n",
        "    def __init__(self, pkl_filename=None, data=None, samples_list=None, \n",
        "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False,regs=None):\n",
        "        \n",
        "        self.regs=regs\n",
        "        if pkl_filename is not None:\n",
        "            if verbose:\n",
        "                print ('Loading ..!', end=' ')\n",
        "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
        "        elif data is not None:\n",
        "            self.data = data.copy()\n",
        "            \n",
        "        else:\n",
        "            sys.stderr.write('Either PKL file or data is needed!')\n",
        "            return \n",
        "\n",
        "        #if verbose:\n",
        "        #    print ('Preprocess..!', end='  ')\n",
        "        if samples_list is None:\n",
        "            self.flist = [f for f in self.data]\n",
        "        else:\n",
        "            self.flist = [f for f in samples_list]\n",
        "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
        "        \n",
        "        current_flist = np.array(self.flist.copy())\n",
        "        current_lab0_flist = current_flist[self.labels == 0]\n",
        "        current_lab1_flist = current_flist[self.labels == 1]\n",
        "        #if verbose:\n",
        "        #    print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
        "        #    print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
        "        \n",
        "        \n",
        "        if augmentation:\n",
        "            self.num_data = aug_factor * len(self.flist)\n",
        "            self.window_size = 40\n",
        "            self.stride = self.window_size\n",
        "            self.neighbors = {}\n",
        "            pbar = pyprind.ProgBar(len(self.flist))\n",
        "            weights = norm_weights(samples_list)#??\n",
        "            for f in self.flist:\n",
        "                label = self.data[f][1]\n",
        "                candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
        "                candidates.remove(f)\n",
        "                eig_f = eig_data[f]['eigvecs']\n",
        "                sim_list = []\n",
        "                for cand in candidates:\n",
        "                    eig_cand = eig_data[cand]['eigvecs']\n",
        "                    sim = similarity_fn(eig_f, eig_cand,weights)\n",
        "                    sim_list.append((sim, cand))\n",
        "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
        "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]#list(candidates)#[item[1] for item in sim_list[:num_neighbs]]\n",
        "        \n",
        "        else:\n",
        "            self.num_data = len(self.flist)\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.flist):\n",
        "            fname = self.flist[index]\n",
        "            data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode)    \n",
        "            data = data[self.regs].copy()\n",
        "            label = (self.labels[index],)\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "        else:\n",
        "            f1 = self.flist[index % len(self.flist)]\n",
        "            X = [f1[i : i + self.window_size] for i in range(self.window_size, self.stride)]\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data\n",
        "    \n",
        "    \n",
        "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
        "               batch_size=64, \n",
        "               num_workers=1, mode='train',\n",
        "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False,regions=None):\n",
        "    \"\"\"Build and return data loader.\"\"\"\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "    else:\n",
        "        shuffle = False\n",
        "        augmentation=False\n",
        "\n",
        "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n",
        "                           augmentation=augmentation, aug_factor=aug_factor, \n",
        "                           eig_data=eig_data, similarity_fn=similarity_fn, verbose=verbose,regs=regions)\n",
        "\n",
        "    data_loader = DataLoader(dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "  \n",
        "    return data_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWyj8pQihkn7"
      },
      "source": [
        "## Defining Autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:29:50.96222Z",
          "iopub.execute_input": "2021-06-18T20:29:50.962557Z",
          "iopub.status.idle": "2021-06-18T20:29:51.064936Z",
          "shell.execute_reply.started": "2021-06-18T20:29:50.962527Z",
          "shell.execute_reply": "2021-06-18T20:29:51.06403Z"
        },
        "trusted": true,
        "id": "i14YCA9Lhkn8"
      },
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990, \n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        \n",
        "        self.num_latent = num_latent\n",
        "        self.num_inputs = num_inputs\n",
        "        \n",
        "        self.fc_encoder = nn.Sequential (\n",
        "                nn.Linear(self.num_inputs,4096),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(4096,1024),\n",
        "                nn.Tanh())\n",
        "        \n",
        "        self.fc_decoder = nn.Sequential (\n",
        "                nn.Linear(1024,4096),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(4096,self.num_inputs),\n",
        "                nn.Tanh())\n",
        "         \n",
        "        \n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Dropout(p=0.25),\n",
        "                nn.Linear(1024, 1),\n",
        "#                 nn.Sigmoid(),\n",
        "#                 nn.Linear(128, 1),\n",
        "\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Linear(1024, 1),\n",
        "#                 nn.Sigmoid(),\n",
        "#                 nn.Linear(128, 1),\n",
        "            )\n",
        "            \n",
        "         \n",
        "    def forward(self, x, eval_classifier=False):\n",
        "\n",
        "        if eval_classifier:\n",
        "            x = self.fc_encoder(x)\n",
        "            x_logit = self.classifier(x).squeeze()\n",
        "            \n",
        "        else:\n",
        "            x = self.fc_encoder(x)\n",
        "            x = self.fc_decoder(x)\n",
        "            x_logit = None\n",
        "            \n",
        "        return x, x_logit\n",
        "\n",
        "mtae = MTAutoEncoder()\n",
        "\n",
        "mtae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw6wxQ7uhkn8"
      },
      "source": [
        "## Defining training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:29:54.242475Z",
          "iopub.execute_input": "2021-06-18T20:29:54.242797Z",
          "iopub.status.idle": "2021-06-18T20:29:54.272092Z",
          "shell.execute_reply.started": "2021-06-18T20:29:54.242766Z",
          "shell.execute_reply": "2021-06-18T20:29:54.271229Z"
        },
        "trusted": true,
        "id": "XJGFx-CFhkn8"
      },
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    clf_train_loss = []\n",
        "    ae_train_loss = []\n",
        "    \n",
        "    if mode == 'clf':\n",
        "        final_targets = []\n",
        "        final_predictions = []\n",
        "    else:\n",
        "        final_targets = None\n",
        "        final_predictions = None    \n",
        "    \n",
        "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
        "        if len(batch_x) != batch_size:\n",
        "            continue\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode == 'ae':\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy, _ = model(data*rand_bernoulli, False)\n",
        "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
        "            else:\n",
        "                rec, _ = model(data, False)\n",
        "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
        "                \n",
        "            loss_total = loss_ae\n",
        "            loss_ae_np = loss_ae.detach().cpu().numpy()\n",
        "            \n",
        "            clf_train_loss.append(0.0)\n",
        "            ae_train_loss.append(loss_ae_np)\n",
        "            train_losses.append([loss_ae_np, 0.0])\n",
        "                \n",
        "        if mode == 'clf':\n",
        "            rec_clean, logits = model(data, True)\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "            \n",
        "            proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            predictions = np.ones_like(proba, dtype=np.int32)\n",
        "            predictions[proba < 0.5] = 0\n",
        "            \n",
        "            final_targets.append(target.detach().cpu().numpy())\n",
        "            final_predictions.append(predictions)\n",
        "            \n",
        "            loss_total = loss_clf\n",
        "            loss_clf_np = loss_clf.detach().cpu().numpy()\n",
        "            \n",
        "            clf_train_loss.append(loss_clf_np)\n",
        "            ae_train_loss.append(0.0)\n",
        "            train_losses.append([0.0,loss_clf_np])\n",
        "            \n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    if (final_targets is not None) and (final_predictions is not None):\n",
        "        final_targets = np.concatenate(final_targets)\n",
        "        final_predictions = np.concatenate(final_predictions)\n",
        "        train_accuracy = np.mean(final_targets == final_predictions)\n",
        "\n",
        "        return np.mean(clf_train_loss), train_accuracy\n",
        "    else:\n",
        "        return np.mean(ae_train_loss), None\n",
        "\n",
        "def test(model, criterion, test_loader, \n",
        "         eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    eval_loss = []\n",
        "    all_predss=[]\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i,(batch_x,batch_y) in enumerate(test_loader):\n",
        "            if num_batch is not None:\n",
        "                if i >= num_batch:\n",
        "                    continue\n",
        "            data, target = batch_x.to(device), batch_y.to(device)\n",
        "            rec, logits = model(data, eval_classifier)\n",
        "\n",
        "#             test_loss += criterion(rec, data).detach().cpu().numpy() \n",
        "#             n_test += len(batch_x)\n",
        "            test_loss = criterion(logits, target).detach().cpu().numpy() \n",
        "            eval_loss.append(test_loss)\n",
        "            if eval_classifier:\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)###????\n",
        "                y_arr = np.array(batch_y, dtype=np.int32)\n",
        "\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
        "        metrics_dict = {'accuracy': np.round(mlp_acc, 4), \n",
        "                        'senstivity' : np.round(mlp_sens,4), \n",
        "                        'specificity' : np.round(mlp_spef,4), \n",
        "                        'loss' : np.round(np.mean(eval_loss),4)}\n",
        "        \n",
        "    return  metrics_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-18T20:29:55.7098Z",
          "iopub.execute_input": "2021-06-18T20:29:55.710152Z",
          "iopub.status.idle": "2021-06-18T20:29:55.718895Z",
          "shell.execute_reply.started": "2021-06-18T20:29:55.710095Z",
          "shell.execute_reply": "2021-06-18T20:29:55.718161Z"
        },
        "trusted": true,
        "id": "uTlD8687hkn9"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2021-06-18T21:19:00.231669Z",
          "iopub.execute_input": "2021-06-18T21:19:00.232037Z",
          "iopub.status.idle": "2021-06-18T21:29:06.037273Z",
          "shell.execute_reply.started": "2021-06-18T21:19:00.232002Z",
          "shell.execute_reply": "2021-06-18T21:29:06.035917Z"
        },
        "trusted": true,
        "id": "7EAaCSUMhkn-"
      },
      "source": [
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n",
        "    \n",
        "    start = time.time()\n",
        "    batch_size = 16\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 50\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = True\n",
        "\n",
        "    start= time.time()\n",
        "\n",
        "#     print('p_bernoulli: ', p_bernoulli)\n",
        "#     print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "#           'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "#     print('use_dropout: ', use_dropout, '\\n')\n",
        "    \n",
        "    # list to store metrics after each fold\n",
        "    repeat_acc=[]\n",
        "    repeat_sen=[]\n",
        "    repeat_spec=[]\n",
        "    repeat_loss=[]\n",
        "    \n",
        "    \n",
        "    for rp in range(1):\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "    # list to store metrics after each fold\n",
        "        crossval_acc=[]\n",
        "        crossval_sen=[]\n",
        "        crossval_spec=[]\n",
        "        crossval_loss=[]\n",
        "        \n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(sfc, labels)):\n",
        "            \n",
        "            NAME = f'asd-diagnet-fold-{kk+1}-rp-2'\n",
        "            ID = f'fold-{kk+1}-rp-2.1'\n",
        "        \n",
        "            x_train, y_train = sfc[train_index],labels[train_index]\n",
        "            x_test, y_test = sfc[test_index],labels[test_index]\n",
        "\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "\n",
        "\n",
        "            num_inpp = 19900\n",
        "            n_lat = 512\n",
        "            \n",
        "            train_dataset = ASDDataset(x_train,y_train)\n",
        "            test_dataset = ASDDataset(x_test,y_test)\n",
        "\n",
        "            train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "            test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
        "               \n",
        "\n",
        "            model = MTAutoEncoder(tied=False, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "\n",
        "            criterion_ae = nn.MSELoss(reduction='sum')\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "#             optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "#                                    {'params': model.fc_decoder.parameters(), 'lr': learning_rate_ae},\n",
        "#                                    {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "#                                   momentum=0.9)\n",
        "            \n",
        "            optimizer = optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.05)          \n",
        "\n",
        "\n",
        "\n",
        "            for epoch in range(1, num_epochs+1):\n",
        "                \n",
        "                if epoch <= 30:\n",
        "                    ae_train_loss,_ = train(model, epoch, train_dataloader, p_bernoulli, mode='ae')\n",
        "                    content = f'AE Train loss: {(ae_train_loss):.4f}'\n",
        "\n",
        "                else:\n",
        "                    clf_train_loss, train_acc = train(model, epoch, train_dataloader, p_bernoulli, mode='clf')\n",
        "                    content = f'CLF Train loss: {(clf_train_loss):.4f}, Train Accuracy: {(train_acc):.4f}'\n",
        "                    \n",
        "                print(f'Epoch {epoch}/{num_epochs+1}')\n",
        "                print(content)\n",
        "\n",
        "            metrics_dict = test(model, criterion_clf, test_dataloader, eval_classifier=True)\n",
        "            print(\"-----------------------------\")\n",
        "            print(f'Fold {kk+1}/{p_fold}')\n",
        "            content = f'{metrics_dict}'\n",
        "            print(content)\n",
        "            print(\"-----------------------------\")\n",
        "            \n",
        "            crossval_acc.append(metrics_dict['accuracy'])\n",
        "            crossval_sen.append(metrics_dict['senstivity'])\n",
        "            crossval_spec.append(metrics_dict['specificity'])\n",
        "            crossval_loss.append(metrics_dict['loss'])\n",
        "            \n",
        "            #save the model after each fold\n",
        "            \n",
        "            recorder = {'optimizer': optimizer.state_dict(),\n",
        "            'model': model.state_dict(),\n",
        "            'fold' : kk+1,\n",
        "            'repitition' : rp+1}\n",
        "\n",
        "            torch.save(recorder, f'{NAME}.pt')\n",
        "            \n",
        "        print(\"*********************************\")    \n",
        "        print(f'Average Value after 10 Folds and repeats one {rp+1}------->')\n",
        "        content = f'Accuracy: {np.round(np.mean(crossval_acc),4)}, Senstivity: {np.round(np.mean(crossval_sen),4)}, Specificity: {np.round(np.mean(crossval_spec),4)}, Loss: {np.round(np.mean(crossval_loss),4)}'\n",
        "        print(content)\n",
        "        print(\"*********************************\") \n",
        "        \n",
        "        repeat_acc.append(np.mean(crossval_acc))\n",
        "        repeat_sen.append(np.mean(crossval_sen))\n",
        "        repeat_spec.append(np.mean(crossval_spec))\n",
        "        repeat_loss.append(np.mean(crossval_loss))\n",
        "    \n",
        "    print(f\"Average Value after 1 Repeat:\")\n",
        "    content = f'Accuracy: {np.round(np.mean(repeat_acc),4)}, Senstivity: {np.round(np.mean(repeat_sen),4)}, Specificity: {np.round(np.mean(repeat_spec),4)}, Loss: {np.round(np.mean(repeat_loss),4)}'\n",
        "    print(content)\n",
        "        \n",
        "    finish= time.time()\n",
        "    print(finish-start)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:29:25.414869Z",
          "iopub.status.idle": "2021-06-15T17:29:25.415635Z"
        },
        "trusted": true,
        "id": "7w7fc7mthkn-"
      },
      "source": [
        "Accuracy: 0.6724, Senstivity: 0.61, Specificity: 0.7321, Loss: 0.6234999895095825\n",
        "589.3430550098419"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-15T17:29:25.417276Z",
          "iopub.status.idle": "2021-06-15T17:29:25.417975Z"
        },
        "trusted": true,
        "id": "IBmoEdIVhkn_"
      },
      "source": [
        "adam with weight decay 0.3 \n",
        "\n",
        "Accuracy: 0.657, Senstivity: 0.5431, Specificity: 0.766, Loss: 0.6509000062942505\n",
        "1160.326141834259\n",
        "\n",
        "\n",
        "adam w/o weight decay \n",
        "Accuracy: 0.7043, Senstivity: 0.6636, Specificity: 0.7434, Loss: 0.5964999794960022\n",
        "1056.0533220767975\n",
        "\n",
        "\n",
        "adam with weight decay 0.1 \n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7063, Senstivity: 0.6716, Specificity: 0.7396, Loss: 0.5821999907493591\n",
        "1162.2546133995056\n",
        "\n",
        "adam with weight decay 0.15 \n",
        "Accuracy: 0.686, Senstivity: 0.6598, Specificity: 0.7113, Loss: 0.5911999940872192\n",
        "1161.7236032485962\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25\n",
        "Accuracy: 0.6985, Senstivity: 0.6793, Specificity: 0.717, Loss: 0.5842000246047974\n",
        "1162.4763264656067\n",
        "\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25 n_lat 256\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.6908, Senstivity: 0.6933, Specificity: 0.6887, Loss: 0.5952000021934509\n",
        "1149.6250448226929"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jy8VBLfhkn_"
      },
      "source": [
        "AIMAFE Data\n",
        "\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25 n_lat 256\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7397, Senstivity: 0.6706, Specificity: 0.7944, Loss: 0.5264000296592712\n",
        "1056.9974591732025\n",
        "\n",
        "\n",
        "adam with weight decay 0.1  and dropout 0.25 n_lat 512\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7471, Senstivity: 0.6923, Specificity: 0.7906, Loss: 0.5105000138282776\n",
        "1067.9337112903595\n",
        "\n",
        "adam with weight decay 0.2  and dropout 0.25 n_lat 512\n",
        "Average Value after 1 Repeat:\n",
        "Accuracy: 0.7324, Senstivity: 0.6256, Specificity: 0.817, Loss: 0.5450999736785889\n",
        "1068.7675378322601"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}