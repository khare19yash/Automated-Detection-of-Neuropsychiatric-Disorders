{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls ../input/abide-cc200/cc200/","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:47.731325Z","iopub.execute_input":"2021-06-15T17:28:47.731761Z","iopub.status.idle":"2021-06-15T17:28:48.396681Z","shell.execute_reply.started":"2021-06-15T17:28:47.731715Z","shell.execute_reply":"2021-06-15T17:28:48.395853Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"CC200\n","output_type":"stream"}]},{"cell_type":"code","source":"#options: cc200, dosenbach160, aal\np_ROI = \"cc200\"\np_fold = 10\np_center = \"Stanford\"\np_mode = \"whole\"\np_augmentation = True\np_Method = \"ASD-DiagNet\"","metadata":{"tags":["parameters"],"execution":{"iopub.status.busy":"2021-06-15T17:28:48.400334Z","iopub.execute_input":"2021-06-15T17:28:48.400634Z","iopub.status.idle":"2021-06-15T17:28:48.407917Z","shell.execute_reply.started":"2021-06-15T17:28:48.400602Z","shell.execute_reply":"2021-06-15T17:28:48.406986Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\nprint(\"*****List of patameters****\")\nprint(\"ROI atlas: \",p_ROI)\nprint(\"per Center or whole: \",p_mode)\nif p_mode == 'percenter':\n    print(\"Center's name: \",p_center)\nprint(\"Method's name: \",p_Method)\nif p_Method == \"ASD-DiagNet\":\n    print(\"Augmentation: \",p_augmentation)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:48.409667Z","iopub.execute_input":"2021-06-15T17:28:48.410248Z","iopub.status.idle":"2021-06-15T17:28:48.419495Z","shell.execute_reply.started":"2021-06-15T17:28:48.410075Z","shell.execute_reply":"2021-06-15T17:28:48.417680Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"*****List of patameters****\nROI atlas:  cc200\nper Center or whole:  whole\nMethod's name:  ASD-DiagNet\nAugmentation:  True\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom functools import reduce\nfrom sklearn.impute import SimpleImputer\nimport time\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch\nimport pyprind\nimport sys\nimport pickle\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix\nfrom scipy import stats\nfrom sklearn import tree\nfrom sklearn.cluster import MiniBatchKMeans\nimport functools\nimport numpy.ma as ma # for masked arrays\nimport pyprind\nimport random\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tqdm.notebook import tqdm\nimport wandb\nfrom itertools import groupby\n\n\n# !wandb login d164742a4a99e4e581f543102aff0992153ad225","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:48.421285Z","iopub.execute_input":"2021-06-15T17:28:48.421798Z","iopub.status.idle":"2021-06-15T17:28:51.392040Z","shell.execute_reply.started":"2021-06-15T17:28:48.421731Z","shell.execute_reply":"2021-06-15T17:28:51.391188Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Importing the data ","metadata":{}},{"cell_type":"code","source":"def get_key(filename):\n    f_split = filename.split('_')\n    if f_split[3] == 'rois':\n        key = '_'.join(f_split[0:3]) \n    else:\n        key = '_'.join(f_split[0:2])\n    return key","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:51.397237Z","iopub.execute_input":"2021-06-15T17:28:51.397499Z","iopub.status.idle":"2021-06-15T17:28:51.405727Z","shell.execute_reply.started":"2021-06-15T17:28:51.397471Z","shell.execute_reply":"2021-06-15T17:28:51.404759Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cc200_data_path = '../input/abide-cc200/cc200/CC200'#cc200'#path to time series data\nflist = os.listdir(cc200_data_path)\n\nfid = []\nfor f in flist:\n    fid.append(get_key(f))\n\n    \ndata_df = pd.read_csv('../input/abide-cc200/Phenotypic_V1_0b_preprocessed1.csv')#path \ndata_df = data_df[data_df['FILE_ID'].isin(fid)]\ndata_df.DX_GROUP = data_df.DX_GROUP.map({1: 1, 2:0})\ndata_df['FILE_PATH'] = data_df['FILE_ID'].apply(lambda x : os.path.join(cc200_data_path,x + '_rois_cc200.1D')) \n\n\n# print(data_df.head())\nprint(len(data_df))\nprint(data_df['FILE_PATH'].values[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:51.409452Z","iopub.execute_input":"2021-06-15T17:28:51.409730Z","iopub.status.idle":"2021-06-15T17:28:51.578552Z","shell.execute_reply.started":"2021-06-15T17:28:51.409705Z","shell.execute_reply":"2021-06-15T17:28:51.577494Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1035\n../input/abide-cc200/cc200/CC200/Pitt_0050003_rois_cc200.1D\n","output_type":"stream"}]},{"cell_type":"code","source":"len(flist)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:51.582072Z","iopub.execute_input":"2021-06-15T17:28:51.583385Z","iopub.status.idle":"2021-06-15T17:28:51.601406Z","shell.execute_reply.started":"2021-06-15T17:28:51.583284Z","shell.execute_reply":"2021-06-15T17:28:51.599804Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"1035"},"metadata":{}}]},{"cell_type":"markdown","source":"### Helper functions for computing correlations","metadata":{}},{"cell_type":"code","source":"def get_label(filename):\n    assert (filename in labels)\n    return labels[filename]\n\n\ndef get_corr_data(path):\n    \n    df = pd.read_csv(path, sep='\\t')\n            \n    with np.errstate(invalid=\"ignore\"):\n        corr = np.nan_to_num(np.corrcoef(df.T))\n        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n        m = ma.masked_where(mask == 1, mask)\n        return ma.masked_where(m, corr).compressed()\n\n\ndef get_corr_matrix(filename,data_path):\n    # returns correlation matrix\n    for file in os.listdir(data_path):\n        if file.startswith(filename):\n            df = pd.read_csv(os.path.join(data_path, file), sep='\\t')\n    with np.errstate(invalid=\"ignore\"):\n        corr = np.nan_to_num(np.corrcoef(df.T))\n        return corr\n\ndef confusion(g_turth,predictions):\n    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n    accuracy = (tp+tn)/(tp+fp+tn+fn)\n    sensitivity = (tp)/(tp+fn)\n    specificty = (tn)/(tn+fp)\n    return accuracy,sensitivity,specificty\n\ndef get_regs(samplesnames,regnum):\n    # returns region index array\n    datas = []\n    for sn in samplesnames:\n        datas.append(all_corr[sn][0])\n    datas = np.array(datas)     # datas shape len(samplesnames)x19900\n    avg=[]\n    for ie in range(datas.shape[1]):\n        avg.append(np.mean(datas[:,ie]))\n    avg=np.array(avg)\n    highs=avg.argsort()[-regnum:][::-1]\n    lows=avg.argsort()[:regnum][::-1]\n    regions=np.concatenate((highs,lows),axis=0) # shape (9950,)\n    return regions","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:51.603160Z","iopub.execute_input":"2021-06-15T17:28:51.603668Z","iopub.status.idle":"2021-06-15T17:28:51.633260Z","shell.execute_reply.started":"2021-06-15T17:28:51.603633Z","shell.execute_reply":"2021-06-15T17:28:51.631957Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"labels = data_df['DX_GROUP'].values\nfpaths = data_df['FILE_PATH']\nsfc = []\n\n\nfor path in fpaths:\n    corr_data = get_corr_data(path)\n    sfc.append(corr_data)\n    \nsfc = np.asarray(sfc)\nprint(sfc.shape)\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:28:51.635524Z","iopub.execute_input":"2021-06-15T17:28:51.636308Z","iopub.status.idle":"2021-06-15T17:29:24.971230Z","shell.execute_reply.started":"2021-06-15T17:28:51.636089Z","shell.execute_reply":"2021-06-15T17:29:24.970244Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(1035, 19900)\n(1035,)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(np.min(sfc))\n\nprint(np.max(sfc))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:24.972729Z","iopub.execute_input":"2021-06-15T17:29:24.973356Z","iopub.status.idle":"2021-06-15T17:29:25.011619Z","shell.execute_reply.started":"2021-06-15T17:29:24.973315Z","shell.execute_reply":"2021-06-15T17:29:25.010821Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"-0.9592505245533961\n0.9900251023663668\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Defining dataset class","metadata":{}},{"cell_type":"code","source":"class ASDDataset(Dataset):\n    def __init__(self,x,y):\n        self.x = x\n        self.y = y\n        pass\n    def __getitem__(self,idx):\n        return torch.tensor(self.x[idx],dtype=torch.float),torch.tensor(self.y[idx],dtype=torch.float)\n        pass\n    def __len__(self):\n        return len(self.x)\n        pass\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:25.013052Z","iopub.execute_input":"2021-06-15T17:29:25.014695Z","iopub.status.idle":"2021-06-15T17:29:25.023455Z","shell.execute_reply.started":"2021-06-15T17:29:25.014618Z","shell.execute_reply":"2021-06-15T17:29:25.022113Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"d = ASDDataset(sfc,labels)\n\nd.__getitem__(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:25.025311Z","iopub.execute_input":"2021-06-15T17:29:25.026039Z","iopub.status.idle":"2021-06-15T17:29:25.092544Z","shell.execute_reply.started":"2021-06-15T17:29:25.025998Z","shell.execute_reply":"2021-06-15T17:29:25.091901Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(tensor([ 0.2489,  0.1380,  0.1472,  ...,  0.0305,  0.1095, -0.2725]),\n tensor(1.))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Defining Autoencoder class","metadata":{}},{"cell_type":"code","source":"class MTAutoEncoder(nn.Module):\n    def __init__(self, num_inputs=990, \n                 num_latent=200, tied=True,\n                 num_classes=2, use_dropout=False):\n        super(MTAutoEncoder, self).__init__()\n        \n        self.num_latent = num_latent\n        self.num_inputs = num_inputs\n        \n        self.fc_encoder = nn.Sequential (\n                nn.Linear(self.num_inputs,4096),\n                nn.Tanh(),\n                nn.Linear(4096,256),\n                nn.Tanh())\n        \n        self.fc_decoder = nn.Sequential (\n                nn.Linear(256,4096),\n                nn.Tanh(),\n                nn.Linear(4096,self.num_inputs),\n                nn.Tanh())\n         \n        \n        if use_dropout:\n            self.classifier = nn.Sequential (\n                nn.Dropout(p=0.25),\n                nn.Linear(256, 1),\n#                 nn.Sigmoid(),\n#                 nn.Linear(128, 1),\n\n            )\n        else:\n            self.classifier = nn.Sequential (\n                nn.Linear(256, 1),\n#                 nn.Sigmoid(),\n#                 nn.Linear(128, 1),\n            )\n            \n         \n    def forward(self, x, eval_classifier=False):\n\n        if eval_classifier:\n            x = self.fc_encoder(x)\n            x_logit = self.classifier(x).squeeze()\n            \n        else:\n            x = self.fc_encoder(x)\n            x = self.fc_decoder(x)\n            x_logit = None\n            \n        return x, x_logit\n\nmtae = MTAutoEncoder()\n\nmtae","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:25.095401Z","iopub.execute_input":"2021-06-15T17:29:25.095642Z","iopub.status.idle":"2021-06-15T17:29:25.176626Z","shell.execute_reply.started":"2021-06-15T17:29:25.095617Z","shell.execute_reply":"2021-06-15T17:29:25.176016Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"MTAutoEncoder(\n  (fc_encoder): Sequential(\n    (0): Linear(in_features=990, out_features=4096, bias=True)\n    (1): Tanh()\n    (2): Linear(in_features=4096, out_features=256, bias=True)\n    (3): Tanh()\n  )\n  (fc_decoder): Sequential(\n    (0): Linear(in_features=256, out_features=4096, bias=True)\n    (1): Tanh()\n    (2): Linear(in_features=4096, out_features=990, bias=True)\n    (3): Tanh()\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Defining training and testing functions","metadata":{}},{"cell_type":"code","source":"def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n    model.train()\n    train_losses = []\n    clf_train_loss = []\n    ae_train_loss = []\n    \n    if mode == 'clf':\n        final_targets = []\n        final_predictions = []\n    else:\n        final_targets = None\n        final_predictions = None    \n    \n    for i,(batch_x,batch_y) in enumerate(train_loader):\n        if len(batch_x) != batch_size:\n            continue\n        if p_bernoulli is not None:\n            if i == 0:\n                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n\n        data, target = batch_x.to(device), batch_y.to(device)\n        optimizer.zero_grad()\n\n        if mode == 'ae':\n            if p_bernoulli is not None:\n                rec_noisy, _ = model(data*rand_bernoulli, False)\n                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n            else:\n                rec, _ = model(data, False)\n                loss_ae = criterion_ae(rec, data) / len(batch_x)\n                \n            loss_total = loss_ae\n            loss_ae_np = loss_ae.detach().cpu().numpy()\n            \n            clf_train_loss.append(0.0)\n            ae_train_loss.append(loss_ae_np)\n            train_losses.append([loss_ae_np, 0.0])\n                \n        if mode == 'clf':\n            rec_clean, logits = model(data, True)\n            loss_clf = criterion_clf(logits, target)\n            \n            proba = torch.sigmoid(logits).detach().cpu().numpy()\n            predictions = np.ones_like(proba, dtype=np.int32)\n            predictions[proba < 0.5] = 0\n            \n            final_targets.append(target.detach().cpu().numpy())\n            final_predictions.append(predictions)\n            \n            loss_total = loss_clf\n            loss_clf_np = loss_clf.detach().cpu().numpy()\n            \n            clf_train_loss.append(loss_clf_np)\n            ae_train_loss.append(0.0)\n            train_losses.append([0.0,loss_clf_np])\n            \n\n        loss_total.backward()\n        optimizer.step()\n    \n    if (final_targets is not None) and (final_predictions is not None):\n        final_targets = np.concatenate(final_targets)\n        final_predictions = np.concatenate(final_predictions)\n        train_accuracy = np.mean(final_targets == final_predictions)\n\n        return np.mean(clf_train_loss), train_accuracy\n    else:\n        return np.mean(ae_train_loss), None\n\ndef test(model, criterion, test_loader, \n         eval_classifier=False, num_batch=None):\n    test_loss, n_test, correct = 0.0, 0, 0\n    eval_loss = []\n    all_predss=[]\n    if eval_classifier:\n        y_true, y_pred = [], []\n    with torch.no_grad():\n        model.eval()\n        for i,(batch_x,batch_y) in enumerate(test_loader):\n            if num_batch is not None:\n                if i >= num_batch:\n                    continue\n            data, target = batch_x.to(device), batch_y.to(device)\n            rec, logits = model(data, eval_classifier)\n\n#             test_loss += criterion(rec, data).detach().cpu().numpy() \n#             n_test += len(batch_x)\n            test_loss = criterion(logits, target).detach().cpu().numpy() \n            eval_loss.append(test_loss)\n            if eval_classifier:\n                proba = torch.sigmoid(logits).detach().cpu().numpy()\n                preds = np.ones_like(proba, dtype=np.int32)\n                preds[proba < 0.5] = 0\n                all_predss.extend(preds)###????\n                y_arr = np.array(batch_y, dtype=np.int32)\n\n                correct += np.sum(preds == y_arr)\n                y_true.extend(y_arr.tolist())\n                y_pred.extend(proba.tolist())\n        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n        metrics_dict = {'accuracy': np.round(mlp_acc, 4), \n                        'senstivity' : np.round(mlp_sens,4), \n                        'specificity' : np.round(mlp_spef,4), \n                        'loss' : np.round(np.mean(eval_loss),4)}\n        \n    return  metrics_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:25.178078Z","iopub.execute_input":"2021-06-15T17:29:25.178438Z","iopub.status.idle":"2021-06-15T17:29:25.208268Z","shell.execute_reply.started":"2021-06-15T17:29:25.178402Z","shell.execute_reply":"2021-06-15T17:29:25.207118Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:25.209829Z","iopub.execute_input":"2021-06-15T17:29:25.210216Z","iopub.status.idle":"2021-06-15T17:29:25.267039Z","shell.execute_reply.started":"2021-06-15T17:29:25.210178Z","shell.execute_reply":"2021-06-15T17:29:25.265679Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n    \n    start = time.time()\n    batch_size = 16\n    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n    num_epochs = 50\n\n    p_bernoulli = None\n    augmentation = p_augmentation\n    use_dropout = True\n\n    start= time.time()\n\n#     print('p_bernoulli: ', p_bernoulli)\n#     print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n#           'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n#     print('use_dropout: ', use_dropout, '\\n')\n\n    kk=0\n    \n    # list to store metrics after each fold\n    repeat_acc=[]\n    repeat_sen=[]\n    repeat_spec=[]\n    repeat_loss=[]\n    \n    \n    for rp in range(1):\n        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n    # list to store metrics after each fold\n        crossval_acc=[]\n        crossval_sen=[]\n        crossval_spec=[]\n        crossval_loss=[]\n        \n        for kk,(train_index, test_index) in enumerate(kf.split(sfc, labels)):\n            \n            NAME = f'asd-diagnet-fold-{kk+1}-rp-2'\n            ID = f'fold-{kk+1}-rp-2.1'\n        \n            x_train, y_train = sfc[train_index],labels[train_index]\n            x_test, y_test = sfc[test_index],labels[test_index]\n\n\n            verbose = (True if (kk == 0) else False)\n\n\n\n            num_inpp = 19900\n            n_lat = 512\n            \n            train_dataset = ASDDataset(x_train,y_train)\n            test_dataset = ASDDataset(x_test,y_test)\n\n            train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n            test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n               \n\n            model = MTAutoEncoder(tied=False, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n            model.to(device)\n\n            criterion_ae = nn.MSELoss(reduction='sum')\n            criterion_clf = nn.BCEWithLogitsLoss()\n#             optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n#                                    {'params': model.fc_decoder.parameters(), 'lr': learning_rate_ae},\n#                                    {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n#                                   momentum=0.9)\n            \n            optimizer = optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.1)          \n\n\n\n            for epoch in range(1, num_epochs+1):\n                \n                if epoch <= 30:\n                    ae_train_loss,_ = train(model, epoch, train_dataloader, p_bernoulli, mode='ae')\n                    content = f'AE Train loss: {(ae_train_loss):.4f}'\n\n                else:\n                    clf_train_loss, train_acc = train(model, epoch, train_dataloader, p_bernoulli, mode='clf')\n                    content = f'CLF Train loss: {(clf_train_loss):.4f}, Train Accuracy: {(train_acc):.4f}'\n                    \n                print(f'Epoch {epoch}/{num_epochs+1}')\n                print(content)\n\n            metrics_dict = test(model, criterion_clf, test_dataloader, eval_classifier=True)\n            print(\"-----------------------------\")\n            print(f'Fold {kk+1}/{p_fold}')\n            content = f'{metrics_dict}'\n            print(content)\n            print(\"-----------------------------\")\n            \n            crossval_acc.append(metrics_dict['accuracy'])\n            crossval_sen.append(metrics_dict['senstivity'])\n            crossval_spec.append(metrics_dict['specificity'])\n            crossval_loss.append(metrics_dict['loss'])\n            \n            #save the model after each fold\n            \n            recorder = {'optimizer': optimizer.state_dict(),\n            'model': model.state_dict(),\n            'fold' : kk+1,\n            'repitition' : rp+1}\n\n            torch.save(recorder, f'{NAME}.pt')\n            \n        print(\"*********************************\")    \n        print(f'Average Value after 10 Folds and repeats one {rp+1}------->')\n        content = f'Accuracy: {np.round(np.mean(crossval_acc),4)}, Senstivity: {np.round(np.mean(crossval_sen),4)}, Specificity: {np.round(np.mean(crossval_spec),4)}, Loss: {np.round(np.mean(crossval_loss),4)}'\n        print(content)\n        print(\"*********************************\") \n        \n        repeat_acc.append(np.mean(crossval_acc))\n        repeat_sen.append(np.mean(crossval_sen))\n        repeat_spec.append(np.mean(crossval_spec))\n        repeat_loss.append(np.mean(crossval_loss))\n    \n    print(f\"Average Value after 1 Repeat:\")\n    content = f'Accuracy: {np.round(np.mean(repeat_acc),4)}, Senstivity: {np.round(np.mean(repeat_sen),4)}, Specificity: {np.round(np.mean(repeat_spec),4)}, Loss: {np.round(np.mean(repeat_loss),4)}'\n    print(content)\n        \n    finish= time.time()\n    print(finish-start)\n\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-15T17:30:55.731416Z","iopub.execute_input":"2021-06-15T17:30:55.731818Z","iopub.status.idle":"2021-06-15T17:50:05.383755Z","shell.execute_reply.started":"2021-06-15T17:30:55.731785Z","shell.execute_reply":"2021-06-15T17:50:05.382889Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/51\nAE Train loss: 780.3627\nEpoch 2/51\nAE Train loss: 641.8359\nEpoch 3/51\nAE Train loss: 574.0011\nEpoch 4/51\nAE Train loss: 521.6058\nEpoch 5/51\nAE Train loss: 480.1600\nEpoch 6/51\nAE Train loss: 445.4202\nEpoch 7/51\nAE Train loss: 416.6529\nEpoch 8/51\nAE Train loss: 393.2597\nEpoch 9/51\nAE Train loss: 374.3742\nEpoch 10/51\nAE Train loss: 358.5388\nEpoch 11/51\nAE Train loss: 346.5001\nEpoch 12/51\nAE Train loss: 337.1112\nEpoch 13/51\nAE Train loss: 330.1006\nEpoch 14/51\nAE Train loss: 323.8033\nEpoch 15/51\nAE Train loss: 318.3643\nEpoch 16/51\nAE Train loss: 314.7458\nEpoch 17/51\nAE Train loss: 312.3139\nEpoch 18/51\nAE Train loss: 311.2360\nEpoch 19/51\nAE Train loss: 310.5615\nEpoch 20/51\nAE Train loss: 309.4768\nEpoch 21/51\nAE Train loss: 308.4588\nEpoch 22/51\nAE Train loss: 308.0207\nEpoch 23/51\nAE Train loss: 307.2036\nEpoch 24/51\nAE Train loss: 307.1804\nEpoch 25/51\nAE Train loss: 306.6443\nEpoch 26/51\nAE Train loss: 305.9350\nEpoch 27/51\nAE Train loss: 305.1592\nEpoch 28/51\nAE Train loss: 303.4073\nEpoch 29/51\nAE Train loss: 303.4720\nEpoch 30/51\nAE Train loss: 303.5970\nEpoch 31/51\nCLF Train loss: 0.7073, Train Accuracy: 0.5043\nEpoch 32/51\nCLF Train loss: 0.6865, Train Accuracy: 0.5539\nEpoch 33/51\nCLF Train loss: 0.6679, Train Accuracy: 0.6056\nEpoch 34/51\nCLF Train loss: 0.6499, Train Accuracy: 0.6336\nEpoch 35/51\nCLF Train loss: 0.6370, Train Accuracy: 0.6530\nEpoch 36/51\nCLF Train loss: 0.6232, Train Accuracy: 0.6897\nEpoch 37/51\nCLF Train loss: 0.6145, Train Accuracy: 0.7091\nEpoch 38/51\nCLF Train loss: 0.5975, Train Accuracy: 0.7177\nEpoch 39/51\nCLF Train loss: 0.5854, Train Accuracy: 0.7457\nEpoch 40/51\nCLF Train loss: 0.5753, Train Accuracy: 0.7522\nEpoch 41/51\nCLF Train loss: 0.5682, Train Accuracy: 0.7532\nEpoch 42/51\nCLF Train loss: 0.5543, Train Accuracy: 0.7662\nEpoch 43/51\nCLF Train loss: 0.5445, Train Accuracy: 0.7769\nEpoch 44/51\nCLF Train loss: 0.5324, Train Accuracy: 0.7974\nEpoch 45/51\nCLF Train loss: 0.5186, Train Accuracy: 0.8082\nEpoch 46/51\nCLF Train loss: 0.5072, Train Accuracy: 0.8157\nEpoch 47/51\nCLF Train loss: 0.4964, Train Accuracy: 0.8244\nEpoch 48/51\nCLF Train loss: 0.4844, Train Accuracy: 0.8244\nEpoch 49/51\nCLF Train loss: 0.4725, Train Accuracy: 0.8470\nEpoch 50/51\nCLF Train loss: 0.4574, Train Accuracy: 0.8491\n-----------------------------\nFold 1/10\n{'accuracy': 0.7115, 'senstivity': 0.7451, 'specificity': 0.6792, 'loss': 0.6009}\n-----------------------------\nEpoch 1/51\nAE Train loss: 779.4927\nEpoch 2/51\nAE Train loss: 644.1600\nEpoch 3/51\nAE Train loss: 573.8032\nEpoch 4/51\nAE Train loss: 520.5408\nEpoch 5/51\nAE Train loss: 479.0954\nEpoch 6/51\nAE Train loss: 444.6835\nEpoch 7/51\nAE Train loss: 416.8967\nEpoch 8/51\nAE Train loss: 393.4184\nEpoch 9/51\nAE Train loss: 373.8879\nEpoch 10/51\nAE Train loss: 358.0237\nEpoch 11/51\nAE Train loss: 346.1932\nEpoch 12/51\nAE Train loss: 336.9776\nEpoch 13/51\nAE Train loss: 330.6004\nEpoch 14/51\nAE Train loss: 324.4409\nEpoch 15/51\nAE Train loss: 319.1888\nEpoch 16/51\nAE Train loss: 315.4406\nEpoch 17/51\nAE Train loss: 312.4759\nEpoch 18/51\nAE Train loss: 310.1416\nEpoch 19/51\nAE Train loss: 309.4294\nEpoch 20/51\nAE Train loss: 308.6224\nEpoch 21/51\nAE Train loss: 308.1595\nEpoch 22/51\nAE Train loss: 307.3181\nEpoch 23/51\nAE Train loss: 306.0424\nEpoch 24/51\nAE Train loss: 305.7734\nEpoch 25/51\nAE Train loss: 305.2528\nEpoch 26/51\nAE Train loss: 304.8246\nEpoch 27/51\nAE Train loss: 304.5638\nEpoch 28/51\nAE Train loss: 303.7556\nEpoch 29/51\nAE Train loss: 303.1201\nEpoch 30/51\nAE Train loss: 303.1491\nEpoch 31/51\nCLF Train loss: 0.6890, Train Accuracy: 0.5420\nEpoch 32/51\nCLF Train loss: 0.6717, Train Accuracy: 0.5991\nEpoch 33/51\nCLF Train loss: 0.6549, Train Accuracy: 0.6498\nEpoch 34/51\nCLF Train loss: 0.6356, Train Accuracy: 0.6756\nEpoch 35/51\nCLF Train loss: 0.6254, Train Accuracy: 0.6929\nEpoch 36/51\nCLF Train loss: 0.6162, Train Accuracy: 0.6994\nEpoch 37/51\nCLF Train loss: 0.6052, Train Accuracy: 0.7123\nEpoch 38/51\nCLF Train loss: 0.5977, Train Accuracy: 0.7263\nEpoch 39/51\nCLF Train loss: 0.5846, Train Accuracy: 0.7360\nEpoch 40/51\nCLF Train loss: 0.5730, Train Accuracy: 0.7435\nEpoch 41/51\nCLF Train loss: 0.5621, Train Accuracy: 0.7694\nEpoch 42/51\nCLF Train loss: 0.5524, Train Accuracy: 0.7640\nEpoch 43/51\nCLF Train loss: 0.5418, Train Accuracy: 0.7791\nEpoch 44/51\nCLF Train loss: 0.5302, Train Accuracy: 0.8017\nEpoch 45/51\nCLF Train loss: 0.5193, Train Accuracy: 0.7996\nEpoch 46/51\nCLF Train loss: 0.5035, Train Accuracy: 0.8147\nEpoch 47/51\nCLF Train loss: 0.4956, Train Accuracy: 0.8254\nEpoch 48/51\nCLF Train loss: 0.4845, Train Accuracy: 0.8287\nEpoch 49/51\nCLF Train loss: 0.4704, Train Accuracy: 0.8362\nEpoch 50/51\nCLF Train loss: 0.4575, Train Accuracy: 0.8567\n-----------------------------\nFold 2/10\n{'accuracy': 0.7308, 'senstivity': 0.7255, 'specificity': 0.7358, 'loss': 0.5602}\n-----------------------------\nEpoch 1/51\nAE Train loss: 781.4272\nEpoch 2/51\nAE Train loss: 642.2173\nEpoch 3/51\nAE Train loss: 575.4106\nEpoch 4/51\nAE Train loss: 522.6105\nEpoch 5/51\nAE Train loss: 480.0588\nEpoch 6/51\nAE Train loss: 444.5752\nEpoch 7/51\nAE Train loss: 416.2820\nEpoch 8/51\nAE Train loss: 393.0632\nEpoch 9/51\nAE Train loss: 374.7057\nEpoch 10/51\nAE Train loss: 360.2671\nEpoch 11/51\nAE Train loss: 347.2827\nEpoch 12/51\nAE Train loss: 337.4283\nEpoch 13/51\nAE Train loss: 329.0113\nEpoch 14/51\nAE Train loss: 323.0312\nEpoch 15/51\nAE Train loss: 318.3859\nEpoch 16/51\nAE Train loss: 314.3487\nEpoch 17/51\nAE Train loss: 312.1381\nEpoch 18/51\nAE Train loss: 311.0130\nEpoch 19/51\nAE Train loss: 310.5059\nEpoch 20/51\nAE Train loss: 309.1613\nEpoch 21/51\nAE Train loss: 308.6504\nEpoch 22/51\nAE Train loss: 307.8779\nEpoch 23/51\nAE Train loss: 307.1261\nEpoch 24/51\nAE Train loss: 305.9522\nEpoch 25/51\nAE Train loss: 306.1625\nEpoch 26/51\nAE Train loss: 305.5639\nEpoch 27/51\nAE Train loss: 305.3811\nEpoch 28/51\nAE Train loss: 304.3428\nEpoch 29/51\nAE Train loss: 303.3147\nEpoch 30/51\nAE Train loss: 303.7272\nEpoch 31/51\nCLF Train loss: 0.6854, Train Accuracy: 0.5506\nEpoch 32/51\nCLF Train loss: 0.6721, Train Accuracy: 0.5905\nEpoch 33/51\nCLF Train loss: 0.6523, Train Accuracy: 0.6412\nEpoch 34/51\nCLF Train loss: 0.6425, Train Accuracy: 0.6509\nEpoch 35/51\nCLF Train loss: 0.6279, Train Accuracy: 0.6897\nEpoch 36/51\nCLF Train loss: 0.6171, Train Accuracy: 0.6843\nEpoch 37/51\nCLF Train loss: 0.6091, Train Accuracy: 0.7037\nEpoch 38/51\nCLF Train loss: 0.5969, Train Accuracy: 0.7209\nEpoch 39/51\nCLF Train loss: 0.5873, Train Accuracy: 0.7360\nEpoch 40/51\nCLF Train loss: 0.5777, Train Accuracy: 0.7478\nEpoch 41/51\nCLF Train loss: 0.5664, Train Accuracy: 0.7489\nEpoch 42/51\nCLF Train loss: 0.5569, Train Accuracy: 0.7705\nEpoch 43/51\nCLF Train loss: 0.5483, Train Accuracy: 0.7856\nEpoch 44/51\nCLF Train loss: 0.5342, Train Accuracy: 0.7931\nEpoch 45/51\nCLF Train loss: 0.5224, Train Accuracy: 0.8071\nEpoch 46/51\nCLF Train loss: 0.5157, Train Accuracy: 0.8114\nEpoch 47/51\nCLF Train loss: 0.5051, Train Accuracy: 0.8265\nEpoch 48/51\nCLF Train loss: 0.4881, Train Accuracy: 0.8351\nEpoch 49/51\nCLF Train loss: 0.4788, Train Accuracy: 0.8351\nEpoch 50/51\nCLF Train loss: 0.4645, Train Accuracy: 0.8534\n-----------------------------\nFold 3/10\n{'accuracy': 0.6827, 'senstivity': 0.4902, 'specificity': 0.8679, 'loss': 0.6123}\n-----------------------------\nEpoch 1/51\nAE Train loss: 779.1142\nEpoch 2/51\nAE Train loss: 641.9316\nEpoch 3/51\nAE Train loss: 571.0354\nEpoch 4/51\nAE Train loss: 519.5051\nEpoch 5/51\nAE Train loss: 478.0889\nEpoch 6/51\nAE Train loss: 443.5353\nEpoch 7/51\nAE Train loss: 415.5305\nEpoch 8/51\nAE Train loss: 392.6501\nEpoch 9/51\nAE Train loss: 372.9549\nEpoch 10/51\nAE Train loss: 357.2663\nEpoch 11/51\nAE Train loss: 346.3010\nEpoch 12/51\nAE Train loss: 336.3246\nEpoch 13/51\nAE Train loss: 327.0637\nEpoch 14/51\nAE Train loss: 321.0567\nEpoch 15/51\nAE Train loss: 316.7599\nEpoch 16/51\nAE Train loss: 314.5328\nEpoch 17/51\nAE Train loss: 312.2845\nEpoch 18/51\nAE Train loss: 310.4063\nEpoch 19/51\nAE Train loss: 308.3392\nEpoch 20/51\nAE Train loss: 307.6866\nEpoch 21/51\nAE Train loss: 307.1326\nEpoch 22/51\nAE Train loss: 306.2489\nEpoch 23/51\nAE Train loss: 306.0465\nEpoch 24/51\nAE Train loss: 305.3854\nEpoch 25/51\nAE Train loss: 304.8145\nEpoch 26/51\nAE Train loss: 304.3344\nEpoch 27/51\nAE Train loss: 304.5577\nEpoch 28/51\nAE Train loss: 304.0360\nEpoch 29/51\nAE Train loss: 303.8809\nEpoch 30/51\nAE Train loss: 303.3594\nEpoch 31/51\nCLF Train loss: 0.6891, Train Accuracy: 0.5312\nEpoch 32/51\nCLF Train loss: 0.6689, Train Accuracy: 0.5776\nEpoch 33/51\nCLF Train loss: 0.6487, Train Accuracy: 0.6185\nEpoch 34/51\nCLF Train loss: 0.6355, Train Accuracy: 0.6390\nEpoch 35/51\nCLF Train loss: 0.6239, Train Accuracy: 0.6724\nEpoch 36/51\nCLF Train loss: 0.6131, Train Accuracy: 0.6681\nEpoch 37/51\nCLF Train loss: 0.5959, Train Accuracy: 0.7274\nEpoch 38/51\nCLF Train loss: 0.5854, Train Accuracy: 0.7338\nEpoch 39/51\nCLF Train loss: 0.5720, Train Accuracy: 0.7511\nEpoch 40/51\nCLF Train loss: 0.5640, Train Accuracy: 0.7511\nEpoch 41/51\nCLF Train loss: 0.5488, Train Accuracy: 0.7737\nEpoch 42/51\nCLF Train loss: 0.5414, Train Accuracy: 0.7812\nEpoch 43/51\nCLF Train loss: 0.5335, Train Accuracy: 0.7866\nEpoch 44/51\nCLF Train loss: 0.5160, Train Accuracy: 0.8103\nEpoch 45/51\nCLF Train loss: 0.5063, Train Accuracy: 0.8157\nEpoch 46/51\nCLF Train loss: 0.4932, Train Accuracy: 0.8179\nEpoch 47/51\nCLF Train loss: 0.4781, Train Accuracy: 0.8416\nEpoch 48/51\nCLF Train loss: 0.4699, Train Accuracy: 0.8384\nEpoch 49/51\nCLF Train loss: 0.4554, Train Accuracy: 0.8534\nEpoch 50/51\nCLF Train loss: 0.4393, Train Accuracy: 0.8610\n-----------------------------\nFold 4/10\n{'accuracy': 0.6442, 'senstivity': 0.5294, 'specificity': 0.7547, 'loss': 0.5842}\n-----------------------------\nEpoch 1/51\nAE Train loss: 786.1308\nEpoch 2/51\nAE Train loss: 643.7274\nEpoch 3/51\nAE Train loss: 575.8994\nEpoch 4/51\nAE Train loss: 523.0932\nEpoch 5/51\nAE Train loss: 481.9797\nEpoch 6/51\nAE Train loss: 447.2403\nEpoch 7/51\nAE Train loss: 418.6073\nEpoch 8/51\nAE Train loss: 393.5051\nEpoch 9/51\nAE Train loss: 374.8244\nEpoch 10/51\nAE Train loss: 359.4268\nEpoch 11/51\nAE Train loss: 347.9906\nEpoch 12/51\nAE Train loss: 338.1019\nEpoch 13/51\nAE Train loss: 330.0911\nEpoch 14/51\nAE Train loss: 323.6434\nEpoch 15/51\nAE Train loss: 319.0946\nEpoch 16/51\nAE Train loss: 315.3349\nEpoch 17/51\nAE Train loss: 313.1721\nEpoch 18/51\nAE Train loss: 310.9542\nEpoch 19/51\nAE Train loss: 309.7352\nEpoch 20/51\nAE Train loss: 308.8818\nEpoch 21/51\nAE Train loss: 307.8242\nEpoch 22/51\nAE Train loss: 306.3488\nEpoch 23/51\nAE Train loss: 305.3471\nEpoch 24/51\nAE Train loss: 305.3500\nEpoch 25/51\nAE Train loss: 304.9154\nEpoch 26/51\nAE Train loss: 304.2485\nEpoch 27/51\nAE Train loss: 303.9466\nEpoch 28/51\nAE Train loss: 304.2198\nEpoch 29/51\nAE Train loss: 304.1927\nEpoch 30/51\nAE Train loss: 303.6493\nEpoch 31/51\nCLF Train loss: 0.6952, Train Accuracy: 0.5248\nEpoch 32/51\nCLF Train loss: 0.6709, Train Accuracy: 0.5819\nEpoch 33/51\nCLF Train loss: 0.6528, Train Accuracy: 0.6250\nEpoch 34/51\nCLF Train loss: 0.6336, Train Accuracy: 0.6724\nEpoch 35/51\nCLF Train loss: 0.6221, Train Accuracy: 0.6864\nEpoch 36/51\nCLF Train loss: 0.6131, Train Accuracy: 0.6886\nEpoch 37/51\nCLF Train loss: 0.6004, Train Accuracy: 0.7047\nEpoch 38/51\nCLF Train loss: 0.5885, Train Accuracy: 0.7252\nEpoch 39/51\nCLF Train loss: 0.5792, Train Accuracy: 0.7435\nEpoch 40/51\nCLF Train loss: 0.5675, Train Accuracy: 0.7478\nEpoch 41/51\nCLF Train loss: 0.5534, Train Accuracy: 0.7640\nEpoch 42/51\nCLF Train loss: 0.5458, Train Accuracy: 0.7802\nEpoch 43/51\nCLF Train loss: 0.5315, Train Accuracy: 0.7909\nEpoch 44/51\nCLF Train loss: 0.5199, Train Accuracy: 0.8136\nEpoch 45/51\nCLF Train loss: 0.5106, Train Accuracy: 0.8060\nEpoch 46/51\nCLF Train loss: 0.4967, Train Accuracy: 0.8200\nEpoch 47/51\nCLF Train loss: 0.4883, Train Accuracy: 0.8351\nEpoch 48/51\nCLF Train loss: 0.4794, Train Accuracy: 0.8373\nEpoch 49/51\nCLF Train loss: 0.4640, Train Accuracy: 0.8308\nEpoch 50/51\nCLF Train loss: 0.4545, Train Accuracy: 0.8502\n-----------------------------\nFold 5/10\n{'accuracy': 0.75, 'senstivity': 0.8431, 'specificity': 0.6604, 'loss': 0.5803}\n-----------------------------\nEpoch 1/51\nAE Train loss: 779.1948\nEpoch 2/51\nAE Train loss: 642.2032\nEpoch 3/51\nAE Train loss: 574.6450\nEpoch 4/51\nAE Train loss: 521.7431\nEpoch 5/51\nAE Train loss: 479.4956\nEpoch 6/51\nAE Train loss: 444.0725\nEpoch 7/51\nAE Train loss: 416.4170\nEpoch 8/51\nAE Train loss: 394.4748\nEpoch 9/51\nAE Train loss: 375.3602\nEpoch 10/51\nAE Train loss: 358.4602\nEpoch 11/51\nAE Train loss: 346.3215\nEpoch 12/51\nAE Train loss: 336.7974\nEpoch 13/51\nAE Train loss: 329.4737\nEpoch 14/51\nAE Train loss: 322.7493\nEpoch 15/51\nAE Train loss: 319.8668\nEpoch 16/51\nAE Train loss: 317.0674\nEpoch 17/51\nAE Train loss: 314.4511\nEpoch 18/51\nAE Train loss: 312.2297\nEpoch 19/51\nAE Train loss: 310.9037\nEpoch 20/51\nAE Train loss: 308.8405\nEpoch 21/51\nAE Train loss: 307.9830\nEpoch 22/51\nAE Train loss: 308.0881\nEpoch 23/51\nAE Train loss: 306.7138\nEpoch 24/51\nAE Train loss: 307.2302\nEpoch 25/51\nAE Train loss: 306.6538\nEpoch 26/51\nAE Train loss: 306.7623\nEpoch 27/51\nAE Train loss: 306.1485\nEpoch 28/51\nAE Train loss: 305.3324\nEpoch 29/51\nAE Train loss: 303.7671\nEpoch 30/51\nAE Train loss: 304.5471\nEpoch 31/51\nCLF Train loss: 0.7192, Train Accuracy: 0.4623\nEpoch 32/51\nCLF Train loss: 0.6785, Train Accuracy: 0.5938\nEpoch 33/51\nCLF Train loss: 0.6661, Train Accuracy: 0.5938\nEpoch 34/51\nCLF Train loss: 0.6499, Train Accuracy: 0.6325\nEpoch 35/51\nCLF Train loss: 0.6365, Train Accuracy: 0.6541\nEpoch 36/51\nCLF Train loss: 0.6196, Train Accuracy: 0.7015\nEpoch 37/51\nCLF Train loss: 0.6081, Train Accuracy: 0.7134\nEpoch 38/51\nCLF Train loss: 0.5912, Train Accuracy: 0.7360\nEpoch 39/51\nCLF Train loss: 0.5822, Train Accuracy: 0.7403\nEpoch 40/51\nCLF Train loss: 0.5684, Train Accuracy: 0.7586\nEpoch 41/51\nCLF Train loss: 0.5604, Train Accuracy: 0.7737\nEpoch 42/51\nCLF Train loss: 0.5475, Train Accuracy: 0.7694\nEpoch 43/51\nCLF Train loss: 0.5376, Train Accuracy: 0.8006\nEpoch 44/51\nCLF Train loss: 0.5234, Train Accuracy: 0.7985\nEpoch 45/51\nCLF Train loss: 0.5152, Train Accuracy: 0.8168\nEpoch 46/51\nCLF Train loss: 0.4979, Train Accuracy: 0.8211\nEpoch 47/51\nCLF Train loss: 0.4868, Train Accuracy: 0.8416\nEpoch 48/51\nCLF Train loss: 0.4728, Train Accuracy: 0.8384\nEpoch 49/51\nCLF Train loss: 0.4624, Train Accuracy: 0.8384\nEpoch 50/51\nCLF Train loss: 0.4467, Train Accuracy: 0.8772\n-----------------------------\nFold 6/10\n{'accuracy': 0.6602, 'senstivity': 0.68, 'specificity': 0.6415, 'loss': 0.6199}\n-----------------------------\nEpoch 1/51\nAE Train loss: 781.2765\nEpoch 2/51\nAE Train loss: 644.5245\nEpoch 3/51\nAE Train loss: 574.9885\nEpoch 4/51\nAE Train loss: 521.6376\nEpoch 5/51\nAE Train loss: 478.8889\nEpoch 6/51\nAE Train loss: 443.7584\nEpoch 7/51\nAE Train loss: 415.6620\nEpoch 8/51\nAE Train loss: 393.8423\nEpoch 9/51\nAE Train loss: 375.2364\nEpoch 10/51\nAE Train loss: 359.0522\nEpoch 11/51\nAE Train loss: 345.8329\nEpoch 12/51\nAE Train loss: 336.4437\nEpoch 13/51\nAE Train loss: 329.4899\nEpoch 14/51\nAE Train loss: 323.2209\nEpoch 15/51\nAE Train loss: 318.6479\nEpoch 16/51\nAE Train loss: 315.5652\nEpoch 17/51\nAE Train loss: 313.2063\nEpoch 18/51\nAE Train loss: 311.2491\nEpoch 19/51\nAE Train loss: 310.3454\nEpoch 20/51\nAE Train loss: 309.2786\nEpoch 21/51\nAE Train loss: 308.5689\nEpoch 22/51\nAE Train loss: 307.2123\nEpoch 23/51\nAE Train loss: 306.4679\nEpoch 24/51\nAE Train loss: 305.3849\nEpoch 25/51\nAE Train loss: 305.6491\nEpoch 26/51\nAE Train loss: 305.7320\nEpoch 27/51\nAE Train loss: 304.9709\nEpoch 28/51\nAE Train loss: 304.1175\nEpoch 29/51\nAE Train loss: 304.1807\nEpoch 30/51\nAE Train loss: 304.3315\nEpoch 31/51\nCLF Train loss: 0.7133, Train Accuracy: 0.5022\nEpoch 32/51\nCLF Train loss: 0.6845, Train Accuracy: 0.5927\nEpoch 33/51\nCLF Train loss: 0.6648, Train Accuracy: 0.6099\nEpoch 34/51\nCLF Train loss: 0.6529, Train Accuracy: 0.6185\nEpoch 35/51\nCLF Train loss: 0.6335, Train Accuracy: 0.6735\nEpoch 36/51\nCLF Train loss: 0.6245, Train Accuracy: 0.6864\nEpoch 37/51\nCLF Train loss: 0.6103, Train Accuracy: 0.6983\nEpoch 38/51\nCLF Train loss: 0.5997, Train Accuracy: 0.7263\nEpoch 39/51\nCLF Train loss: 0.5878, Train Accuracy: 0.7338\nEpoch 40/51\nCLF Train loss: 0.5800, Train Accuracy: 0.7371\nEpoch 41/51\nCLF Train loss: 0.5683, Train Accuracy: 0.7425\nEpoch 42/51\nCLF Train loss: 0.5568, Train Accuracy: 0.7640\nEpoch 43/51\nCLF Train loss: 0.5466, Train Accuracy: 0.7694\nEpoch 44/51\nCLF Train loss: 0.5366, Train Accuracy: 0.7909\nEpoch 45/51\nCLF Train loss: 0.5258, Train Accuracy: 0.8006\nEpoch 46/51\nCLF Train loss: 0.5143, Train Accuracy: 0.8233\nEpoch 47/51\nCLF Train loss: 0.5023, Train Accuracy: 0.8179\nEpoch 48/51\nCLF Train loss: 0.4897, Train Accuracy: 0.8373\nEpoch 49/51\nCLF Train loss: 0.4795, Train Accuracy: 0.8319\nEpoch 50/51\nCLF Train loss: 0.4690, Train Accuracy: 0.8491\n-----------------------------\nFold 7/10\n{'accuracy': 0.6796, 'senstivity': 0.82, 'specificity': 0.5472, 'loss': 0.5946}\n-----------------------------\nEpoch 1/51\nAE Train loss: 779.6664\nEpoch 2/51\nAE Train loss: 642.4254\nEpoch 3/51\nAE Train loss: 572.0867\nEpoch 4/51\nAE Train loss: 520.6626\nEpoch 5/51\nAE Train loss: 478.8504\nEpoch 6/51\nAE Train loss: 444.7000\nEpoch 7/51\nAE Train loss: 417.4990\nEpoch 8/51\nAE Train loss: 394.1724\nEpoch 9/51\nAE Train loss: 374.6217\nEpoch 10/51\nAE Train loss: 357.9859\nEpoch 11/51\nAE Train loss: 345.5205\nEpoch 12/51\nAE Train loss: 335.2845\nEpoch 13/51\nAE Train loss: 327.5476\nEpoch 14/51\nAE Train loss: 321.5027\nEpoch 15/51\nAE Train loss: 316.7489\nEpoch 16/51\nAE Train loss: 313.7056\nEpoch 17/51\nAE Train loss: 311.2370\nEpoch 18/51\nAE Train loss: 309.4267\nEpoch 19/51\nAE Train loss: 308.8745\nEpoch 20/51\nAE Train loss: 308.1596\nEpoch 21/51\nAE Train loss: 307.3771\nEpoch 22/51\nAE Train loss: 305.8881\nEpoch 23/51\nAE Train loss: 305.0255\nEpoch 24/51\nAE Train loss: 304.8936\nEpoch 25/51\nAE Train loss: 305.3553\nEpoch 26/51\nAE Train loss: 304.8435\nEpoch 27/51\nAE Train loss: 304.3235\nEpoch 28/51\nAE Train loss: 303.8830\nEpoch 29/51\nAE Train loss: 303.6262\nEpoch 30/51\nAE Train loss: 302.9731\nEpoch 31/51\nCLF Train loss: 0.6993, Train Accuracy: 0.5226\nEpoch 32/51\nCLF Train loss: 0.6746, Train Accuracy: 0.5593\nEpoch 33/51\nCLF Train loss: 0.6563, Train Accuracy: 0.6131\nEpoch 34/51\nCLF Train loss: 0.6390, Train Accuracy: 0.6616\nEpoch 35/51\nCLF Train loss: 0.6303, Train Accuracy: 0.6800\nEpoch 36/51\nCLF Train loss: 0.6175, Train Accuracy: 0.6756\nEpoch 37/51\nCLF Train loss: 0.6025, Train Accuracy: 0.7177\nEpoch 38/51\nCLF Train loss: 0.5917, Train Accuracy: 0.7198\nEpoch 39/51\nCLF Train loss: 0.5763, Train Accuracy: 0.7381\nEpoch 40/51\nCLF Train loss: 0.5697, Train Accuracy: 0.7457\nEpoch 41/51\nCLF Train loss: 0.5596, Train Accuracy: 0.7403\nEpoch 42/51\nCLF Train loss: 0.5481, Train Accuracy: 0.7812\nEpoch 43/51\nCLF Train loss: 0.5349, Train Accuracy: 0.7769\nEpoch 44/51\nCLF Train loss: 0.5220, Train Accuracy: 0.7974\nEpoch 45/51\nCLF Train loss: 0.5111, Train Accuracy: 0.8060\nEpoch 46/51\nCLF Train loss: 0.5022, Train Accuracy: 0.8341\nEpoch 47/51\nCLF Train loss: 0.4868, Train Accuracy: 0.8276\nEpoch 48/51\nCLF Train loss: 0.4733, Train Accuracy: 0.8373\nEpoch 49/51\nCLF Train loss: 0.4650, Train Accuracy: 0.8416\nEpoch 50/51\nCLF Train loss: 0.4482, Train Accuracy: 0.8534\n-----------------------------\nFold 8/10\n{'accuracy': 0.6408, 'senstivity': 0.68, 'specificity': 0.6038, 'loss': 0.603}\n-----------------------------\nEpoch 1/51\nAE Train loss: 788.1851\nEpoch 2/51\nAE Train loss: 646.2599\nEpoch 3/51\nAE Train loss: 579.2782\nEpoch 4/51\nAE Train loss: 526.4258\nEpoch 5/51\nAE Train loss: 483.1852\nEpoch 6/51\nAE Train loss: 449.0561\nEpoch 7/51\nAE Train loss: 421.8130\nEpoch 8/51\nAE Train loss: 398.2927\nEpoch 9/51\nAE Train loss: 377.7991\nEpoch 10/51\nAE Train loss: 360.3106\nEpoch 11/51\nAE Train loss: 347.1469\nEpoch 12/51\nAE Train loss: 337.4934\nEpoch 13/51\nAE Train loss: 330.4156\nEpoch 14/51\nAE Train loss: 324.2559\nEpoch 15/51\nAE Train loss: 320.1710\nEpoch 16/51\nAE Train loss: 316.4696\nEpoch 17/51\nAE Train loss: 314.1174\nEpoch 18/51\nAE Train loss: 312.3248\nEpoch 19/51\nAE Train loss: 310.1761\nEpoch 20/51\nAE Train loss: 309.8988\nEpoch 21/51\nAE Train loss: 309.9736\nEpoch 22/51\nAE Train loss: 309.1108\nEpoch 23/51\nAE Train loss: 308.0851\nEpoch 24/51\nAE Train loss: 307.5945\nEpoch 25/51\nAE Train loss: 306.9135\nEpoch 26/51\nAE Train loss: 306.0215\nEpoch 27/51\nAE Train loss: 305.5997\nEpoch 28/51\nAE Train loss: 305.7224\nEpoch 29/51\nAE Train loss: 305.8749\nEpoch 30/51\nAE Train loss: 305.6985\nEpoch 31/51\nCLF Train loss: 0.6979, Train Accuracy: 0.5259\nEpoch 32/51\nCLF Train loss: 0.6745, Train Accuracy: 0.5819\nEpoch 33/51\nCLF Train loss: 0.6502, Train Accuracy: 0.6530\nEpoch 34/51\nCLF Train loss: 0.6400, Train Accuracy: 0.6519\nEpoch 35/51\nCLF Train loss: 0.6271, Train Accuracy: 0.6692\nEpoch 36/51\nCLF Train loss: 0.6094, Train Accuracy: 0.7112\nEpoch 37/51\nCLF Train loss: 0.6001, Train Accuracy: 0.7101\nEpoch 38/51\nCLF Train loss: 0.5822, Train Accuracy: 0.7446\nEpoch 39/51\nCLF Train loss: 0.5746, Train Accuracy: 0.7381\nEpoch 40/51\nCLF Train loss: 0.5633, Train Accuracy: 0.7683\nEpoch 41/51\nCLF Train loss: 0.5526, Train Accuracy: 0.7716\nEpoch 42/51\nCLF Train loss: 0.5411, Train Accuracy: 0.7802\nEpoch 43/51\nCLF Train loss: 0.5295, Train Accuracy: 0.7812\nEpoch 44/51\nCLF Train loss: 0.5184, Train Accuracy: 0.8082\nEpoch 45/51\nCLF Train loss: 0.5085, Train Accuracy: 0.8147\nEpoch 46/51\nCLF Train loss: 0.4919, Train Accuracy: 0.8438\nEpoch 47/51\nCLF Train loss: 0.4826, Train Accuracy: 0.8276\nEpoch 48/51\nCLF Train loss: 0.4682, Train Accuracy: 0.8384\nEpoch 49/51\nCLF Train loss: 0.4572, Train Accuracy: 0.8696\nEpoch 50/51\nCLF Train loss: 0.4383, Train Accuracy: 0.8793\n-----------------------------\nFold 9/10\n{'accuracy': 0.6893, 'senstivity': 0.64, 'specificity': 0.7358, 'loss': 0.5944}\n-----------------------------\nEpoch 1/51\nAE Train loss: 783.6187\nEpoch 2/51\nAE Train loss: 642.2469\nEpoch 3/51\nAE Train loss: 574.3564\nEpoch 4/51\nAE Train loss: 522.1498\nEpoch 5/51\nAE Train loss: 479.3440\nEpoch 6/51\nAE Train loss: 444.1578\nEpoch 7/51\nAE Train loss: 416.8201\nEpoch 8/51\nAE Train loss: 393.3910\nEpoch 9/51\nAE Train loss: 374.7986\nEpoch 10/51\nAE Train loss: 358.4287\nEpoch 11/51\nAE Train loss: 345.7596\nEpoch 12/51\nAE Train loss: 335.7971\nEpoch 13/51\nAE Train loss: 328.6539\nEpoch 14/51\nAE Train loss: 323.5891\nEpoch 15/51\nAE Train loss: 319.6298\nEpoch 16/51\nAE Train loss: 316.5508\nEpoch 17/51\nAE Train loss: 313.5605\nEpoch 18/51\nAE Train loss: 311.3422\nEpoch 19/51\nAE Train loss: 309.3135\nEpoch 20/51\nAE Train loss: 308.1579\nEpoch 21/51\nAE Train loss: 307.8559\nEpoch 22/51\nAE Train loss: 306.6343\nEpoch 23/51\nAE Train loss: 305.5037\nEpoch 24/51\nAE Train loss: 305.2131\nEpoch 25/51\nAE Train loss: 305.2942\nEpoch 26/51\nAE Train loss: 304.7590\nEpoch 27/51\nAE Train loss: 304.6121\nEpoch 28/51\nAE Train loss: 304.0536\nEpoch 29/51\nAE Train loss: 303.3455\nEpoch 30/51\nAE Train loss: 302.7534\nEpoch 31/51\nCLF Train loss: 0.6918, Train Accuracy: 0.5571\nEpoch 32/51\nCLF Train loss: 0.6723, Train Accuracy: 0.5862\nEpoch 33/51\nCLF Train loss: 0.6522, Train Accuracy: 0.6444\nEpoch 34/51\nCLF Train loss: 0.6347, Train Accuracy: 0.6659\nEpoch 35/51\nCLF Train loss: 0.6271, Train Accuracy: 0.6649\nEpoch 36/51\nCLF Train loss: 0.6118, Train Accuracy: 0.6940\nEpoch 37/51\nCLF Train loss: 0.6033, Train Accuracy: 0.6800\nEpoch 38/51\nCLF Train loss: 0.5856, Train Accuracy: 0.7317\nEpoch 39/51\nCLF Train loss: 0.5741, Train Accuracy: 0.7543\nEpoch 40/51\nCLF Train loss: 0.5651, Train Accuracy: 0.7478\nEpoch 41/51\nCLF Train loss: 0.5602, Train Accuracy: 0.7532\nEpoch 42/51\nCLF Train loss: 0.5460, Train Accuracy: 0.7672\nEpoch 43/51\nCLF Train loss: 0.5330, Train Accuracy: 0.8028\nEpoch 44/51\nCLF Train loss: 0.5270, Train Accuracy: 0.7834\nEpoch 45/51\nCLF Train loss: 0.5168, Train Accuracy: 0.7996\nEpoch 46/51\nCLF Train loss: 0.5044, Train Accuracy: 0.8190\nEpoch 47/51\nCLF Train loss: 0.4908, Train Accuracy: 0.8297\nEpoch 48/51\nCLF Train loss: 0.4794, Train Accuracy: 0.8405\nEpoch 49/51\nCLF Train loss: 0.4672, Train Accuracy: 0.8448\nEpoch 50/51\nCLF Train loss: 0.4546, Train Accuracy: 0.8556\n-----------------------------\nFold 10/10\n{'accuracy': 0.7184, 'senstivity': 0.78, 'specificity': 0.6604, 'loss': 0.6018}\n-----------------------------\n*********************************\nAverage Value after 10 Folds and repeats one 1------->\nAccuracy: 0.6908, Senstivity: 0.6933, Specificity: 0.6887, Loss: 0.5952000021934509\n*********************************\nAverage Value after 1 Repeat:\nAccuracy: 0.6908, Senstivity: 0.6933, Specificity: 0.6887, Loss: 0.5952000021934509\n1149.6250448226929\n","output_type":"stream"}]},{"cell_type":"code","source":"Accuracy: 0.6724, Senstivity: 0.61, Specificity: 0.7321, Loss: 0.6234999895095825\n589.3430550098419","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:25.414869Z","iopub.status.idle":"2021-06-15T17:29:25.415635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam with weight decay 0.3 \n\nAccuracy: 0.657, Senstivity: 0.5431, Specificity: 0.766, Loss: 0.6509000062942505\n1160.326141834259\n\n\nadam w/o weight decay \nAccuracy: 0.7043, Senstivity: 0.6636, Specificity: 0.7434, Loss: 0.5964999794960022\n1056.0533220767975\n\n\nadam with weight decay 0.1 \nAverage Value after 1 Repeat:\nAccuracy: 0.7063, Senstivity: 0.6716, Specificity: 0.7396, Loss: 0.5821999907493591\n1162.2546133995056\n\nadam with weight decay 0.15 \nAccuracy: 0.686, Senstivity: 0.6598, Specificity: 0.7113, Loss: 0.5911999940872192\n1161.7236032485962\n\nadam with weight decay 0.1  and dropout 0.25\nAccuracy: 0.6985, Senstivity: 0.6793, Specificity: 0.717, Loss: 0.5842000246047974\n1162.4763264656067","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:29:25.417276Z","iopub.status.idle":"2021-06-15T17:29:25.417975Z"},"trusted":true},"execution_count":null,"outputs":[]}]}